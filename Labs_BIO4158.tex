% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  12pt,
]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={BIO4158 Applied biostats with R},
  pdfauthor={Julien Martin},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
%\usepackage{booktabs}
\usepackage{ctable}
\usepackage{fancyhdr}
\usepackage{float}
\usepackage[margin=2cm]{geometry}

\floatplacement{figure}{H}

%\usepackage[sf,bf]{titlesec}

\hypersetup{colorlinks=true, urlcolor=blue}

\renewcommand{\chaptername}{Chapitre}
\renewcommand{\contentsname}{Table des Mati√®res}
\renewcommand{\partname}{Partie}

\usepackage{framed,color}
\definecolor{incolor}{RGB}{240,240,240}
\definecolor{outcolor}{RGB}{248,248,248}

\renewcommand{\textfraction}{0.05}
\renewcommand{\topfraction}{0.8}
\renewcommand{\bottomfraction}{0.8}
\renewcommand{\floatpagefraction}{0.75}

%\renewenvironment{quote}{\begin{VF}}{\end{VF}}

\ifxetex
 \usepackage{letltxmacro}
 \setlength{\XeTeXLinkMargin}{1pt}
 \LetLtxMacro\SavedIncludeGraphics\includegraphics
 \def\includegraphics#1#{% #1 catches optional stuff (star/opt. arg.)
   \IncludeGraphicsAux{#1}%
 }%
 \newcommand*{\IncludeGraphicsAux}[2]{%
   \XeTeXLinkBox{%
     \SavedIncludeGraphics#1{#2}%
   }%
 }%
\fi

\makeatletter
\newenvironment{kframe}{%
\medskip{}
\setlength{\fboxsep}{.8em}
\def\at@end@of@kframe{}%
\ifinner\ifhmode%
 \def\at@end@of@kframe{\end{minipage}}%
 \begin{minipage}{\columnwidth}%
\fi\fi%
\def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
\colorbox{incolor}{##1}\hskip-\fboxsep
    % There is no \\@totalrightmargin, so:
    \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
\MakeFramed {\advance\hsize-\width
  \@totalleftmargin\z@ \linewidth\hsize
  \@setminipage}}%
{\par\unskip\endMakeFramed%
\at@end@of@kframe}
\makeatother

\makeatletter
\@ifundefined{Shaded}{
}{\renewenvironment{Shaded}{\begin{kframe}}{\end{kframe}}}
\makeatother

% \let\oldverbatim\verbatim
% \renewenvironment{Shaded}{\vspace{0.2cm}\begin{kframe}}{\end{kframe}}
% \renewenvironment{verbatim}{\begin{shaded}\begin{oldverbatim}}{\end{oldverbatim}\end{shaded}}

\newenvironment{rmdblock}[1]
 {
 \begin{itemize}
 \renewcommand{\labelitemi}{
   \raisebox{-.7\height}[0pt][0pt]{
     {\setkeys{Gin}{width=3em,keepaspectratio}\includegraphics{images/#1}}
   }
 }
 \begin{kframe}
 \setlength{\fboxsep}{1em}
 \item
 }
 {
 \end{kframe}
 \end{itemize}
 }
\newenvironment{rmdnote}
  {\begin{rmdblock}{note}}
  {\end{rmdblock}}
\newenvironment{rmdcaution}
  {\begin{rmdblock}{caution}}
  {\end{rmdblock}}
\newenvironment{rmdimportant}
  {\begin{rmdblock}{important}}
  {\end{rmdblock}}
\newenvironment{rmdtip}
  {\begin{rmdblock}{tip}}
  {\end{rmdblock}}
\newenvironment{rmdwarning}
  {\begin{rmdblock}{warning}}
  {\end{rmdblock}}
\newenvironment{rmdcode}
  {\begin{rmdblock}{screen}}
  {\end{rmdblock}}

\usepackage{makeidx}
\makeindex

\urlstyle{tt}

\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother

% \frontmatter
\usepackage[]{natbib}
\bibliographystyle{plainnat}

\title{BIO4158 Applied biostats with R}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{Laboratory manual}
\author{Julien Martin}
\date{19-10-2021}

\begin{document}
\maketitle

%\cleardoublepage\newpage\thispagestyle{empty}\null
%\cleardoublepage\newpage\thispagestyle{empty}\null
%\cleardoublepage\newpage
%\thispagestyle{empty}
%\begin{center}
%\includegraphics{images/missing.png}
%\end{center}

%\setlength{\abovedisplayskip}{-5pt}
%\setlength{\abovedisplayshortskip}{-5pt}

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{note}{%
\chapter*{Note}\label{note}}
\addcontentsline{toc}{chapter}{Note}

Development version. Lab material will appear slowly during the Fall 2021 term.

\hypertarget{preface}{%
\chapter*{Preface}\label{preface}}
\addcontentsline{toc}{chapter}{Preface}

The laboratory exercises outlined in the following pages are designed to allow you to develop some expertise in using statistical software (R)
to analyze data. R is powerful statistical software but, like all software, it has its limitations. In particular, it is dumb: it cannot think for you,
it cannot tell you whether the analysis you are attempting to do is appropriate or even makes any sense, and it cannot interpret your
results.

\hypertarget{general-points-to-keep-in-mind}{%
\section*{General points to keep in mind}\label{general-points-to-keep-in-mind}}
\addcontentsline{toc}{section}{General points to keep in mind}

\begin{itemize}
\item
  Before attempting any statistical procedure, you must familiarize yourself with what the procedure is actually doing. This does not mean you actually have to know the underlying mathematics (although this certainly helps!), but you should at least understand the principles involved in the analysis. Therefore, before doing a laboratory exercise, read the appropriate section(s) in the lecture notes. Otherwise, the output from your analyses - even if done correctly - will seem like drivel.
\item
  The laboratories are designed to complement the lectures, and vice versa. Owing to scheduling constraints, it may not be possible to synchronize the two perfectly. But feel free to bring questions about the laboratories to class, or questions about the lectures to the labs.
\item
  Work on the laboratories at your own speed: some can be donemuch more quickly than others, and one laboratory need not correspond to one laboratory session. In fact, for some laboratorieswe have allotted two laboratory sessions. Although you will notbe ``graded'' on the laboratories per se, be aware that completingthe labs is essential. If you do not complete the labs, it is veryunlikely that you will be able to complete the assignments and thefinal exam/term paper. So take these laboratories seriously!
\item
  The objective of the first lab is to allow you to acquire or reviewthe minimum knowledge required to complete the following laboratory exercises with R. There are always several methods toaccomplish something in R, but you will only find simple ways inthis manual. Those amongst you that want to go further will easilyfind many examples of more detailed and sophisticated methods.In particular, I point you to the following resources:

  \begin{itemize}
  \tightlist
  \item
    R for beginners
    \url{http://cran.r-project.org/doc/contrib/Paradis-rdebuts_en.pdf}
  \item
    An introduction to R
    \url{http://cran.r-project.org/doc/manuals/R-intro.html}
  \item
    If you prefer paper books, the CRAN web site has a commented list at :
    \url{http://www.r-project.org/doc/bib/R-books.html}
  \item
    Excellent list of R books
    \url{https://www.bigbookofr.com/}
  \item
    R reference card by Tom Short
    \url{http://cran.r-project.org/doc/contrib/Short-refcard.pdf}
  \end{itemize}
\end{itemize}

\hypertarget{what-is-r-and-why-use-it-in-this-course}{%
\section*{What is R and why use it in this course?}\label{what-is-r-and-why-use-it-in-this-course}}
\addcontentsline{toc}{section}{What is R and why use it in this course?}

R is multiplatform free software forming a system for statistical computation and graphics. R is also a programming language specially designed for statistical data analysis. It is a dialect of the S language. S- Plus is another dialect of the S language, very similar to R, incorporated into a commercial package. S-Plus has a built-in graphical design intreface that some find convivial.

R has 2 major advantages for this course. Initially, you will find that it also has one inconvenience. However, this ``inconvenience'' will rapidly force you to acquire very good working habits. So, I see it as a third advantage.

The first advantage is that you can install it freely on you personal computer(s). This is important because it is by doing analyses that you will learn and eventually master biostatistics. This implies that you have easy and unlimited access to a statistical software package. The second advantage is that R can do everything in statistics. R was conceived to be extensible and has become the preferred tool for statisticians around the world. The question is not ``Can R do this?'' but rather ``How can I do this in R?''. And search engine are your friends.

No other software package offers you these two advantages.

The inconvenience of R is that one has to type commands (or copy and paste code) rather than use a menu and select options. If you do not know what command to use, nothing will happen. It is therefore not that easy when you start. However, it is possible to rapidly learn to make basic operations (open a data file, plot data, and run a simple analysis). And once you understand the operating principle, you can easily find examples on the Web for more complex analyses and graphs for which you can adapt the code.

This is exactly what you will do in the first lab to familiarize yourself with R.

Why is this inconvenience really an advantage in my mind? Because this way of doing things is more efficient and will save you time on the long run. I guarantee it. Believe me, you will never do an analysis only once. As you'll proceed through analyses, you will find data entry errors, discover that the analysis must be run separately for subgroups, find extra data, have to rerun the analysis on transformed data, or you will make some analytical error along the way. If you use a graphical interface with menus, redoing an analysis implies that you reclick here, enter values there, select some options, etc. Each of these steps is a potential source of error. If, instead, you use lines of codes, you only have to fix the code and submit to repeat instantaneously the entire analysis. And you can perfectly document what you did, leaving an audit trail for the future. This is how pros work and can document the quality of the results of their analyses.

\hypertarget{software-installation}{%
\section*{Software installation}\label{software-installation}}
\addcontentsline{toc}{section}{Software installation}

\hypertarget{r}{%
\subsection*{R}\label{r}}
\addcontentsline{toc}{subsection}{R}

To install R on a computer, go to \url{http://cran.r-project.org/}. You will find compiled versions (binairies) for your preferred operating system (Windows, MacOS, Linux).

Note : R has already been installed on the lab computers (the version may be slightly different, but this should not matter).

\hypertarget{rstudio-or-vs-code}{%
\subsection*{Rstudio or VS code}\label{rstudio-or-vs-code}}
\addcontentsline{toc}{subsection}{Rstudio or VS code}

RStudio and VS code are integrated development environment software or IDE. RStudio was develop specifically to work with R. VScode is more generela but work extremely well with R. Both are available on Windows, OS X and Linux

\begin{itemize}
\tightlist
\item
  RStudio: \url{https://www.rstudio.com/products/rstudio/download/}
\item
  VScode: \url{https://code.visualstudio.com/download}
\end{itemize}

\hypertarget{r-libraries}{%
\subsection*{R libraries}\label{r-libraries}}
\addcontentsline{toc}{subsection}{R libraries}

R is essentially unlimited in terms of functions that can be used, because is relies on functions packages that can be added as extra components to use in R.

\begin{itemize}
\tightlist
\item
  Rmarkdown
\item
  tinytex
\end{itemize}

Those 2 packages should be installed automatically with RStudio but I recommend to install them manually in case they are not. To do so, just copy-paste the text below in R terminal.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{install.packages}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"rmarkdown"}\NormalTok{, }\StringTok{"tinytex"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\hypertarget{gpower}{%
\subsection*{G*Power}\label{gpower}}
\addcontentsline{toc}{subsection}{G*Power}

G*Power est un programme gratuit, d√©velopp√© par des psychologues de l'Universit√© de Dusseldorf en Allemagne.
Le programme existe en version Mac et Windows.
Il peut cependant √™tre utilis√© sous linux via Wine.
G*Power vous permettra d'effectuer une analyse de puissance pour la majorit√© des tests que nous verrons au cours de la session sans avoir √† effectuer des calculs complexes ou farfouiller dans des tableaux ou des figures d√©crivant des distributions ou des courbes de puissance.

T√©l√©chargez le programme sur le site \url{https://www.psychologie.hhu.de/arbeitsgruppen/allgemeine-psychologie-und-arbeitspsychologie/gpower.html}

\hypertarget{general-laboratory-instructions}{%
\section*{General laboratory instructions}\label{general-laboratory-instructions}}
\addcontentsline{toc}{section}{General laboratory instructions}

\begin{itemize}
\tightlist
\item
  Bring a USB key or equivalent so you can save your work. Alternatively, email your results to yourself.
\item
  Read the lab exercise before coming to the lab. Read the R code and come with questions about the code.
\item
  During pre-labs, listen to the special instructions
\item
  Do the laboratory exercises at your own rhythm, in teams. Then, I
  recommend that you start (complete?) the lab assignment so that you can benefit from the presence of the TA or prof.
\item
  During your analyses, copy and paste results in a separate document, for example in your preferred word processing program. Annotate abundantly
\item
  Each time you shut down R, save the history of your commands (ex: labo1.1 rHistory, labo1.2.rHistory, etc). You will be able to redo the lab rapidly, get code fragments, or more easily identify errors.
\item
  Create your own ``library'' of code fragments (snippets). Annotate
  it abundantly. You will thank yourself later.
\end{itemize}

\hypertarget{notes-about-the-manual}{%
\section*{Notes about the manual}\label{notes-about-the-manual}}
\addcontentsline{toc}{section}{Notes about the manual}

You will find explanations on the theory, R code and functions, IDE best practice and exercises with R.

The manual tries to highlight some part of the text using the following boxes and icons.

\begin{rmdcode}
Exercises,
\end{rmdcode}

\begin{rmdcaution}
warnings,
\end{rmdcaution}

\begin{rmdwarning}
warnings,
\end{rmdwarning}

\begin{rmdimportant}
important points
\end{rmdimportant}

\begin{rmdnote}
notes
\end{rmdnote}

\begin{rmdtip}
and tips
\end{rmdtip}
\#\#\# Resources \{-\}

This document was developped using the excellent \href{https://bookdown.org/}{bookdown} üì¶ de \href{https://yihui.name/}{Yihui Xie}. The manual is based on the previous lab manual \emph{Findlay, Morin and Rundle, BIO4158 Laboratory manual for BIO4158}.

\hypertarget{license}{%
\subsection*{License}\label{license}}
\addcontentsline{toc}{subsection}{License}

The document is available follwoing the license \href{http://creativecommons.org/licenses/by-nc-sa/4.0/}{License Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International}.

\begin{figure}
\centering
\includegraphics{images/icons/license_cc.png}
\caption{License Creative Commons}
\end{figure}

\hypertarget{introduction-to-r}{%
\chapter{Introduction to R}\label{introduction-to-r}}

After completing this laboratory exercise, you should be able to:

\begin{itemize}
\tightlist
\item
  Open R data files
\item
  Import rectangular data sets
\item
  Export R data to text files
\item
  Verify that data were imported correctly
\item
  Examine the distribution of a variable
\item
  Examine visually and test for normality of a variable
\item
  Calculate descriptive statistics for a variable
\item
  Transform data
\end{itemize}

\hypertarget{set-intro}{%
\section{Packages and data needed for the lab}\label{set-intro}}

This labs needs the following:

\begin{itemize}
\tightlist
\item
  R packages:

  \begin{itemize}
  \tightlist
  \item
    ggplot2
  \end{itemize}
\item
  data files

  \begin{itemize}
  \tightlist
  \item
    ErablesGatineau.csv
  \item
    sturgeon.csv
  \end{itemize}
\end{itemize}

\hypertarget{importing-and-exporting-data}{%
\section{Importing and exporting data}\label{importing-and-exporting-data}}

There are multiple format to save data. The 2 most used formats with R are \texttt{.csv} and \texttt{.Rdata}.

\begin{itemize}
\tightlist
\item
  \texttt{.csv} files are used to store data in a simple format and are editable using any text editor (e.g.~Word, Writer, atom, \ldots) and spreadsheets (e.g.~MS Excel, LO Calc).
  They can be read using the function \texttt{read.csv()} and created in R with \texttt{write.csv()}.
\item
  \texttt{.Rdata} files are used to store not only data but any R object, however, those files can only be used in R. They are created using the \texttt{save()} function and read using the \texttt{load()} function.
\end{itemize}

Data for exercises and labs are provides in \texttt{.csv}.

\hypertarget{working-directory}{%
\subsection{Working directory}\label{working-directory}}

\begin{rmdwarning}
Potentially the most frequent error when starting with R is link to loading data or reading data from an external file in R.
\end{rmdwarning}

A typical error message is:

\begin{verbatim}
Error in file(file, "rt") : cannot open the connection
In addition: Warning message:
In file(file, "rt") :
  cannot open file 'ou_est_mon_fichier.csv': No such file or directory
\end{verbatim}

This type of error simply means that R cannot find the file you specified. By default, when R starts, a folder is defined as the base folder for R. This is the working directory. R by default will save any files in this folder and will start looking for files in this folder. So you need to specify to R where to look for files and where to save your files. This can be done in 3 different ways:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{file.choose()}. (not recommended, because not reproducible). This function will open a dialog box allowing you to click on the file you want. This is not recommended and can be long because you will have to do it absolutely every time you use R.
\item
  specify the complete path in the function. For example \texttt{read.csv("/home/julien/Documents/cours/BIO4558/labo/data/monfichier.csv")}. This is longer to type the first time and a bit tricky to get the correct path but after you can run the line of code and it works every time without trying to remember were you saved that damned file. However, this is specific to your own computer and would not work elsewhere.
\item
  specify a working directory with \texttt{setwd()}. This simplify tells R where to look for files and where to save files. (This is automatically done when using .Rmd files). Just set the working directory to where you want and after that all path will be relative to this working directory. The big advantage is that if you keep a similar folder structure for you R project it will be compatible and reproducible across all computer and OS
\end{enumerate}

To know which folder is the workind directory simply type \texttt{getwd()}

\begin{rmdtip}
When opening Rstudio by double-clicking on a file, it will automatically set the working irectory to the folder where this file is located. This can be super handy.
\end{rmdtip}

\begin{rmdimportant}
For all labs, I strongly recommend you to make a folder where you will save all your R scripts and data and use it as your working directory in R. For better organisation I suggest to save your data in a subfolder named \texttt{data\ All\ R\ code\ for\ data\ loading\ in\ the\ manual\ is\ based\ on\ that\ structure.\ This\ is\ why\ dat\ loading\ or\ saving\ code\ look\ like}data/my\_file.xxx`. If you follow it also all code for data loading can be simply copy-pasted and should work.
\end{rmdimportant}

\hypertarget{opening-a-.rdata-file}{%
\subsection{\texorpdfstring{Opening a \texttt{.Rdata} file}{Opening a .Rdata file}}\label{opening-a-.rdata-file}}

You can double-click on the file and R/Rstudio should open. Alternatively, you can use \texttt{load()} function and specify the names (and path) of the file. For example to load the data \texttt{ErablesGatineau.Rdata} in R which is located in the folder \texttt{data} in the working directory you can use:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{load}\NormalTok{(}\StringTok{"data/ErablesGatineau.Rdata"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{open-a-.csv-file}{%
\subsection{\texorpdfstring{Open a \texttt{.csv} file}{Open a .csv file}}\label{open-a-.csv-file}}

To import data saved in a \texttt{.csv} file, you need to use the \texttt{read.csv()} function.
For example, to create a R object named \texttt{erables} which contain the data from the file \texttt{ErablesGatineau.csv}, you need to use:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{erables \textless{}{-}}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"data/ErablesGatineau.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{rmdwarning}
Beware of the coma. If you are working in adifferent language (other than english), be careful because the decimal symbol might ot be the same.
By default R use the point for the decimal sign. If the dat use the coma for the decimal then R would not be able to read the file correctly. In this case you can use \texttt{read.csv2()} or \texttt{read.data()} which should solve the problem.
\end{rmdwarning}

To verify that the data were read and loaded properly, you can list all objects in memory with the \texttt{ls()} function, or get a more detailed description with \texttt{ls.str()}:

\begin{rmdtip}
I do not recommend to use \texttt{ls.str()} since it can produce really long R ouputs when you have multiple R object loaded.
I suggest instead to use the combination of \texttt{ls()} to get the list of all R objects and then \texttt{str()} only for the objects you want to look at.
\end{rmdtip}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ls}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "erables" "params"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(erables)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 'data.frame':    100 obs. of  3 variables:
##  $ station: chr  "A" "A" "A" "A" ...
##  $ diam   : num  22.4 36.1 44.4 24.6 17.7 ...
##  $ biom   : num  732 1171 673 1552 504 ...
\end{verbatim}

R confirms that the object \texttt{erables}.
\texttt{erables} is a data.frame that contains 100 observations (lines) of 3 variables (columns) : \texttt{station} , a variable of type Factor with 2
levels, and \texttt{diam} and \texttt{biom} that are 2 numeric variables.

\hypertarget{entering-data-in-r}{%
\subsection{Entering data in R}\label{entering-data-in-r}}

R is not the ideal environment to input data. It is possible, but the syntax is heavy and makes most people upset. Use your preferred worksheet program instead. It will be more efficient and less frustrating.

\hypertarget{cleaning-up-correcting-data}{%
\subsection{Cleaning up / correcting data}\label{cleaning-up-correcting-data}}

Another operation that can be frustrating in R. Our advice: unless you want to keep track of all corrections made (so that you can go back to the original data), do not change data in R. Return to the original data file (in a worksheet or database), correct the data there, and then reimport into R. It is simple to resubmit the few lines of code to reimport data. Doing things this way will leave you with a single version of your data file that has all corrections, and the code that allows you to repeat the analysis exactly.

\hypertarget{exporting-data-from-r}{%
\subsection{Exporting data from R}\label{exporting-data-from-r}}

You have 2 options: export data in \texttt{.csv} or in \texttt{.Rdata}

To export in \texttt{.Rdata} use the function \texttt{save()} to export in \texttt{.csv} use \texttt{write.csv()}

For example, to save teh object \texttt{mydata} in a file \texttt{wonderful\_data.csv}that will be saved in your working directory you can type:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{write.csv}\NormalTok{(mydata, }\DataTypeTok{file =} \StringTok{"wonderful\_data.csv"}\NormalTok{, }\DataTypeTok{row.names =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{preliminary-examination-of-data}{%
\section{Preliminary examination of data}\label{preliminary-examination-of-data}}

The first step of data analysis is to examine the data at hand. This examination will tell you if the data were correctly imported, whether the numbers are credible, whether all data came in, etc. This initial data examination often will allow you to detect unlikely observations, possibly due to errors at the data entry stage. Finally, the initial plotting of the data will allow you to visualize the major trends that will be confirmed later by your statistical analysis.

The file \texttt{sturgeon.csv} contains data on sturgeons from the Saskatchewan River. These data were collected to examine how sturgeon size varies among sexes ( \texttt{sex} ), sites ( \texttt{location} ), and years( \texttt{year} ).

\begin{itemize}
\tightlist
\item
  Load the data from \texttt{sturgeon.csv} in a R object named \texttt{sturgeon}.
\item
  use the function \texttt{str()} to check that the data was loaded and read correctly.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sturgeon \textless{}{-}}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"data/sturgeon.csv"}\NormalTok{)}
\KeywordTok{str}\NormalTok{(sturgeon)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 'data.frame':    186 obs. of  9 variables:
##  $ fklngth : num  37 50.2 28.9 50.2 45.6 ...
##  $ totlngth: num  40.7 54.1 31.3 53.1 49.5 ...
##  $ drlngth : num  23.6 31.5 17.3 32.3 32.1 ...
##  $ rdwght  : num  15.95 NA 6.49 NA 29.92 ...
##  $ age     : int  11 24 7 23 20 23 20 7 23 19 ...
##  $ girth   : num  40.5 53.5 31 52.5 50 54.2 48 28.5 44 39 ...
##  $ sex     : chr  "MALE" "FEMALE" "MALE" "FEMALE" ...
##  $ location: chr  "THE_PAS" "THE_PAS" "THE_PAS" "THE_PAS" ...
##  $ year    : int  1978 1978 1978 1978 1978 1978 1978 1978 1978 1978 ...
\end{verbatim}

\hypertarget{summary-statistics}{%
\subsection{Summary statistics}\label{summary-statistics}}

To get summary statistics on the contents of the data frame sturgeon, type the command:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(sturgeon)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     fklngth         totlngth        drlngth          rdwght     
##  Min.   :24.96   Min.   :28.15   Min.   :14.33   Min.   : 4.73  
##  1st Qu.:41.00   1st Qu.:43.66   1st Qu.:25.00   1st Qu.:18.09  
##  Median :44.06   Median :47.32   Median :27.00   Median :23.10  
##  Mean   :44.15   Mean   :47.45   Mean   :27.29   Mean   :24.87  
##  3rd Qu.:48.00   3rd Qu.:51.97   3rd Qu.:29.72   3rd Qu.:30.27  
##  Max.   :66.85   Max.   :72.05   Max.   :41.93   Max.   :93.72  
##                  NA's   :85      NA's   :13      NA's   :4      
##       age            girth           sex              location        
##  Min.   : 7.00   Min.   :11.50   Length:186         Length:186        
##  1st Qu.:17.00   1st Qu.:40.00   Class :character   Class :character  
##  Median :20.00   Median :44.00   Mode  :character   Mode  :character  
##  Mean   :20.24   Mean   :44.33                                        
##  3rd Qu.:23.50   3rd Qu.:48.80                                        
##  Max.   :55.00   Max.   :73.70                                        
##  NA's   :11      NA's   :85                                           
##       year     
##  Min.   :1978  
##  1st Qu.:1979  
##  Median :1979  
##  Mean   :1979  
##  3rd Qu.:1980  
##  Max.   :1980  
## 
\end{verbatim}

For each variable, R lists:

\begin{itemize}
\tightlist
\item
  the minimum
\item
  the maximum
\item
  the median that is the \(50^{th}\) percentile, here the \(93^{rd}\) value of the 186 observations ordered in ascending order
\item
  values at the first (25\%) and third quartile (75\%)
\item
  the number of missing values in the column.
\end{itemize}

Note that several variables have missing values (NA). Only the variables \texttt{fklngth} (fork length), \texttt{sex} , \texttt{location} , and \texttt{year} have 186 observations.

\begin{rmdwarning}
\textbf{Beware of missing values}

Several R functions are sensitive to missing values and you will frequently have to do your analyses on data subsets without missing data, or by using optional parameters in various commands. We will get back to this, but you should always pay attention and take note of missing data when you do analyses.
\end{rmdwarning}

\hypertarget{histogram-empirical-probability-density-boxplot-and-visual-assessment-of-normality}{%
\subsection{Histogram, empirical probability density, boxplot, and visual assessment of normality}\label{histogram-empirical-probability-density-boxplot-and-visual-assessment-of-normality}}

Let's look more closely at the distribution of \texttt{fklngth}.
The command \texttt{hist()} will create a histogram. For the histogram of \texttt{fklngth} in the \texttt{sturgeon} data frame, type the command:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{hist}\NormalTok{(sturgeon}\OperatorTok{$}\NormalTok{fklngth)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{Labs_BIO4158_files/figure-latex/hist-stur-1.pdf}
\caption{\label{fig:hist-stur}Histogram of fluke length of sturgeons}
\end{figure}

The data appear to be approximately normal. This is good to know.

\begin{rmdnote}
Note that this syntax is a bit heavy as you need to prefix variable names by the data frame name \texttt{sturgeon\$}. You can lighten the syntax by making the variables directly accessible by commands by typing the command \texttt{attach()}.
However, I \textbf{strongly recommend not to use} it because it can lead to many problems hard to detect compare to the little benfit is provides
\end{rmdnote}

This histogram (Fig. \ref{fig:hist-stur}) is a very classical representation of the distribution. Histograms are not perfect however because their shape partly depends on the number of bins used, more so for small samples. One can do better, especially if you want to visually compare the observed distribution to a normal distribution. But you need to come up with a bit of extra R code based on the \texttt{ggplot2} üì¶.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\# load ggplot2 if needed}
\KeywordTok{library}\NormalTok{(ggplot2)}

\CommentTok{\#\# use "sturgeon" dataframe to make plot called mygraph}
\CommentTok{\# and define x axis as representing fklngth}
\NormalTok{mygraph \textless{}{-}}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ sturgeon, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ fklngth))}

\CommentTok{\#\# add data to the mygraph ggplot}
\NormalTok{mygraph \textless{}{-}}\StringTok{ }\NormalTok{mygraph }\OperatorTok{+}
\StringTok{  }\CommentTok{\#\# add semitransparent histogram}
\StringTok{  }\KeywordTok{geom\_histogram}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ ..density..),}
    \DataTypeTok{bins =} \DecValTok{30}\NormalTok{, }\DataTypeTok{color =} \StringTok{"black"}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.3}
\NormalTok{  ) }\OperatorTok{+}
\StringTok{  }\CommentTok{\#\#  add density smooth}
\StringTok{  }\KeywordTok{geom\_density}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\CommentTok{\#\# add observations positions or rug bars}
\StringTok{  }\KeywordTok{geom\_rug}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\CommentTok{\#\# add Gaussian curve adjusted to the data with mean and sd from fklngth}
\StringTok{  }\KeywordTok{stat\_function}\NormalTok{(}
    \DataTypeTok{fun =}\NormalTok{ dnorm,}
    \DataTypeTok{args =} \KeywordTok{list}\NormalTok{(}
      \DataTypeTok{mean =} \KeywordTok{mean}\NormalTok{(sturgeon}\OperatorTok{$}\NormalTok{fklngth),}
      \DataTypeTok{sd =} \KeywordTok{sd}\NormalTok{(sturgeon}\OperatorTok{$}\NormalTok{fklngth)}
\NormalTok{    ),}
    \DataTypeTok{color =} \StringTok{"red"}
\NormalTok{  )}

\CommentTok{\#\# display graph}
\NormalTok{mygraph}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{Labs_BIO4158_files/figure-latex/stur-g1-1.pdf}
\caption{\label{fig:stur-g1}Distribution of fluke length in sturgeon plotted with ggplot}
\end{figure}

Each observation is represented by a short vertical bar below the x- axis (rug). The red line is the normal distribution with the same mean and standard deviation as the data. The other line is the empirical distribution, smoothed from the observations.

The ggplot object you just created (\texttt{mygraph}) can be further manipulated. For example, you can plot the distribution of \texttt{fklngth} per \texttt{sex} and \texttt{year} groups simply by adding a \texttt{facet\_grid()} statement:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mygraph }\OperatorTok{+}\StringTok{ }\KeywordTok{facet\_grid}\NormalTok{(year }\OperatorTok{\textasciitilde{}}\StringTok{ }\NormalTok{sex)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Labs_BIO4158_files/figure-latex/aventure-1.pdf}

Each panel contains the data distribution for one sex that year, and the recurring red curve is the normal distribution for the entire data set. It can serve as a reference to help visually evaluate differences among panels.

Another way to visually assess normality of data is the QQ plot that is
obtained by the pair of commands \texttt{qqnorm()} and \texttt{qqline()}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{qqnorm}\NormalTok{(sturgeon}\OperatorTok{$}\NormalTok{fklngth)}
\KeywordTok{qqline}\NormalTok{(sturgeon}\OperatorTok{$}\NormalTok{fklngth)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Labs_BIO4158_files/figure-latex/stur-norm-1.pdf}
Perfectly normal data would follow the straight diagonal line. Here there are deviations in the tails of the distribution and a bit to the right of the center.
Compare this representation to the two preceding graphs.
You will probably agree that it is easier to visualize how data deviate from normality by looking at a histogram of an empirical probability density than by looking at the QQ plots.
However, QQ plots are often automatically produced by various statistical routines and you should be able to interpret them.
In addition, one can easily run a formal test of normality in R with the command \texttt{shapiro.test()} that computes a statistic (\texttt{W}) that measures how tightly data fall around the straight diagonal line of the QQ plot. If data fall perfectly on the line, then \texttt{W\ =\ 1}. If \texttt{W} is much less than 1, then data are not normal.

For the \texttt{fklngth} data:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{shapiro.test}\NormalTok{(sturgeon}\OperatorTok{$}\NormalTok{fklngth)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Shapiro-Wilk normality test
## 
## data:  sturgeon$fklngth
## W = 0.97225, p-value = 0.0009285
\end{verbatim}

W is close to 1, but far enough to indicate a statistically significant deviation from normality.

Visual examination of very large data sets is often made difficult by the superposition of data points. Boxplots are an interesting alternative.
The command \texttt{boxplot(fklngth\textasciitilde{}sex,\ notch=TRUE)} produces a boxplot of \texttt{fklngth} for each \texttt{sex} , and adds whiskers.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{boxplot}\NormalTok{(fklngth }\OperatorTok{\textasciitilde{}}\StringTok{ }\NormalTok{sex, }\DataTypeTok{data =}\NormalTok{ sturgeon, }\DataTypeTok{notch =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{Labs_BIO4158_files/figure-latex/boxplot-stur-1.pdf}
\caption{\label{fig:boxplot-stur}Boxplot of fluke length in strugeon by sex}
\end{figure}

The slightly thicker line inside the box of figure \ref{fig:boxplot-stur} indicates the median.
The width of the notch is proportional to the uncertainty around the median estimate.
One can visually assess the approximate statistical significance of differences among medians by looking at the overlap of the notches (here there is no overlap and one could tentatively conclude that the median female size is larger than the median male size).
Boxes extend from the first to third quartile (the 25\textsuperscript{th} to 75\textsuperscript{th} percentile if you prefer).
Bars (whiskers) extend above and below the boxes from the minimum to the maximum observed value or, if there are extreme values, from the smallest to the largest observed value within 1.5x the interquartile range from the median. Observations exceeding the limits of the whiskers (hence further away from the median than 1.5x the interquartile range, the range between the 25\textsuperscript{th} and 75\textsuperscript{th} percentile) are plotted as circles. These are outliers, possibly aberrant data.

\hypertarget{scatterplots}{%
\subsection{Scatterplots}\label{scatterplots}}

In addition to histograms and other univariate plots, it is often informative to examine scatter plots.
The command \texttt{plot(y\textasciitilde{}x)} produces a scatter plot of y on the vertical axis (the ordinate) vs x on the horizontal axis (abscissa).

\begin{rmdcode}
Create a scatterplot of fklngth vs age using the plot() command.
\end{rmdcode}

You should obtain:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(fklngth }\OperatorTok{\textasciitilde{}}\StringTok{ }\NormalTok{age, }\DataTypeTok{data =}\NormalTok{ sturgeon)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Labs_BIO4158_files/figure-latex/stur-biv-plot-1.pdf}

R has a function to create all pairwise scatterplots rapidly called
\texttt{pairs()} . One of \texttt{pairs()} options is the addition of a lowess trace on
each plot to that is a smoothed trend in the data.
To get the plot matrix with the lowess smooth for all variables in the
sturgeon data frame, execute the command
\texttt{pairs(sturgeon,\ panel=panel.smooth)}. Howeber given the large number of variable in \texttt{sturgeon} we can limit the plot to the first 6 columns in the data.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pairs}\NormalTok{(sturgeon[, }\DecValTok{1}\OperatorTok{:}\DecValTok{6}\NormalTok{], }\DataTypeTok{panel =}\NormalTok{ panel.smooth)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Labs_BIO4158_files/figure-latex/pairs-stur-1.pdf}

\hypertarget{creating-data-subsets}{%
\section{Creating data subsets}\label{creating-data-subsets}}

You will frequently want to do analyses on some subset of your data.
The command \texttt{subset()} is what you need to isolate cases meeting some criteria.
For example, to create a subset of the sturgeon data frame that contains only females caught in 1978, you could write:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sturgeon\_female\_}\DecValTok{1978}\NormalTok{ \textless{}{-}}\StringTok{ }\KeywordTok{subset}\NormalTok{(sturgeon, sex }\OperatorTok{==}\StringTok{ "FEMALE"} \OperatorTok{\&}\StringTok{ }\NormalTok{year }\OperatorTok{==}\StringTok{ "1978"}\NormalTok{)}
\NormalTok{sturgeon\_female\_}\DecValTok{1978}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      fklngth totlngth  drlngth rdwght age girth    sex   location year
## 2   50.19685 54.13386 31.49606     NA  24  53.5 FEMALE    THE_PAS 1978
## 4   50.19685 53.14961 32.28346     NA  23  52.5 FEMALE    THE_PAS 1978
## 6   49.60630 53.93701 31.10236  35.86  23  54.2 FEMALE    THE_PAS 1978
## 7   47.71654 51.37795 33.97638  33.88  20  48.0 FEMALE    THE_PAS 1978
## 15  48.89764 53.93701 29.92126  35.86  23  52.5 FEMALE    THE_PAS 1978
## 105 46.85039       NA 28.34646  23.90  24    NA FEMALE CUMBERLAND 1978
## 106 40.74803       NA 24.80315  17.50  18    NA FEMALE CUMBERLAND 1978
## 107 40.35433       NA 25.59055  20.90  21    NA FEMALE CUMBERLAND 1978
## 109 43.30709       NA 27.95276  24.10  19    NA FEMALE CUMBERLAND 1978
## 113 53.54331       NA 33.85827  48.90  20    NA FEMALE CUMBERLAND 1978
## 114 51.77165       NA 31.49606  35.30  26    NA FEMALE CUMBERLAND 1978
## 116 45.27559       NA 26.57480  23.70  24    NA FEMALE CUMBERLAND 1978
## 118 53.14961       NA 32.67717  45.30  25    NA FEMALE CUMBERLAND 1978
## 119 50.19685       NA 32.08661  33.90  26    NA FEMALE CUMBERLAND 1978
## 123 49.01575       NA 29.13386  37.50  22    NA FEMALE CUMBERLAND 1978
\end{verbatim}

\begin{rmdcaution}
When using criteria to select cases, be careful of the \texttt{==} syntax to mean equal to.
In this context, if you use a single \texttt{=}, you will not get what you want.
The following table lists the most common criteria to create expressions and their R syntax.
\end{rmdcaution}

\begin{longtable}[]{@{}llll@{}}
\toprule
Operator & Explanation & Operator & Explanation\tabularnewline
\midrule
\endhead
== & Equal to & != & Not equal to\tabularnewline
\textgreater{} & Larger than & \textless{} & Lower than\tabularnewline
\textgreater= & Larger than or equal to & \textless= & Lower than or equal to\tabularnewline
\& & And (vectorized) & \textbar{} & Or (vectorized)\tabularnewline
\&\& & And (control) & \textbar\textbar{} & Or (control)\tabularnewline
! & Not & &\tabularnewline
\bottomrule
\end{longtable}

\begin{rmdcode}
Using the commands \texttt{subset()} and \texttt{hist()} , create a histogram for females caught in 1979 and 1980 (hint: \texttt{sex=="FEMALE"\ \&\ (year\ =="1979"\ \textbar{}\ year=="1980")})
\end{rmdcode}

\begin{figure}
\centering
\includegraphics{Labs_BIO4158_files/figure-latex/intror-subex-1.pdf}
\caption{\label{fig:intror-subex}Distibution of fluke length of female sturgeons in 1979 and 1980}
\end{figure}

\hypertarget{data-transformation}{%
\section{Data transformation}\label{data-transformation}}

You will frequently transform raw data to better satisfy assumptions of statistical tests. R will allow you to do that easily.
The most used functions are probably:

\begin{itemize}
\tightlist
\item
  \texttt{log()}
\item
  \texttt{sqrt()}
\item
  \texttt{ifelse()}
\end{itemize}

You can use these functions directly within commands, create vector variables, or add columns in data frames.
To do a plot of the decimal log of fklngth vs age, you can simply use the \texttt{log10()} function within the plot command:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(}\KeywordTok{log10}\NormalTok{(fklngth)}\OperatorTok{\textasciitilde{}}\NormalTok{age, }\DataTypeTok{data =}\NormalTok{ sturgeon)}
\end{Highlighting}
\end{Shaded}

To create a vector variable, an orphan variable if you wish, one that is not part of a data frame, called \texttt{lfklngth} and corresponding too the decimal log of \texttt{fklngth}, simply enter:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{logfklngth \textless{}{-}}\StringTok{ }\KeywordTok{log10}\NormalTok{(sturgeon}\OperatorTok{$}\NormalTok{fklngth)}
\end{Highlighting}
\end{Shaded}

If you want this new variable to be added to a data frame, then you must prefix the variable name by the data frame name and the \texttt{\$} symbol.
For example to add the variable \texttt{lfkl} containing the decimal log of \texttt{fklngth} to the \texttt{sturgeon} data frame, enter:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sturgeon}\OperatorTok{$}\NormalTok{lfkl \textless{}{-}}\StringTok{ }\KeywordTok{log10}\NormalTok{(sturgeon}\OperatorTok{$}\NormalTok{fklngth)}
\end{Highlighting}
\end{Shaded}

\texttt{lfkl} will be added to the data frame \texttt{sturgeon} for the R session.
Do not forget to save the modified data frame if you want to keep the modified version. Or better, save you Rscript and do not forget to run the line of code again next time you need it.

For conditional transformations, you can use the function \texttt{ifelse()}.
For example, to create a new variable called dummy with a value of 1 for males and 0 for females, you can use:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sturgeon}\OperatorTok{$}\NormalTok{dummy \textless{}{-}}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(sturgeon}\OperatorTok{$}\NormalTok{sex }\OperatorTok{==}\StringTok{ "MALE"}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{exercice}{%
\section{Exercice}\label{exercice}}

The file \texttt{salmonella.csv} contains numerical values for the variable
called ratio for two environments (\texttt{milieu}: \texttt{IN\ VITRO} or \texttt{IN\ VIVO})
and for 3 strains (\texttt{souche}).
Examine the ratio variable and make a graph to visually assess normality for the wild (SAUVAGE) strain.

\begin{figure}
\centering
\includegraphics{Labs_BIO4158_files/figure-latex/intror-exer-1.pdf}
\caption{\label{fig:intror-exer}Distibution of infection ratios by the wild (SAUVAGE) strain of salmonella}
\end{figure}

\hypertarget{power-analysis-with-r-and-gpower}{%
\chapter{Power Analysis with R and G*Power}\label{power-analysis-with-r-and-gpower}}

After completing this laboratory, you should :

\begin{itemize}
\tightlist
\item
  be able to compute the power of a t-test with G*Power and R
\item
  be able to calculate the required sample size to achieve a desired power level with a t-test
\item
  be able to calculate the detectable effect size by a t-test given the sample size, the power and \(\alpha\)
\item
  understand how power changes when sample size increases, the effect size changes, or when \(\alpha\) decreases
\item
  understand how power is affected when you change from a two-tailed to a one-tailed test.
\end{itemize}

\hypertarget{the-theory}{%
\section{The theory}\label{the-theory}}

\hypertarget{what-is-power}{%
\subsection{What is power?}\label{what-is-power}}

\emph{Power is the probability of rejecting the null hypothesis when it is false}

\hypertarget{why-do-a-power-analysis}{%
\subsection{Why do a power analysis?}\label{why-do-a-power-analysis}}

\hypertarget{assess-the-strength-of-evidence}{%
\subsubsection*{Assess the strength of evidence}\label{assess-the-strength-of-evidence}}
\addcontentsline{toc}{subsubsection}{Assess the strength of evidence}

Power analysis, performed after accepting a null hypothesis, can help assess the probability of rejecting the null if it were false, and if the magnitude of the effect was equal to that observed (or to any other given magnitude). This type of \emph{a posteriori} analysis is very common.

\hypertarget{design-better-experiments}{%
\subsubsection*{Design better experiments}\label{design-better-experiments}}
\addcontentsline{toc}{subsubsection}{Design better experiments}

Power analysis, performed prior to conducting an experiment (but most often after a preliminary experiment), can be used to determine the number of observations required to detect an effect of a given magnitude with some probability (the power). This type of \emph{a priori} experiment should be more common.

\hypertarget{estimate-minimum-detectable-effect}{%
\subsubsection*{Estimate minimum detectable effect}\label{estimate-minimum-detectable-effect}}
\addcontentsline{toc}{subsubsection}{Estimate minimum detectable effect}

Sampling effort is often predetermined (when you are handed data of an experiment already completed), or extremely constrained (when logistics dictates what can be done). Whether it is \emph{a priori} or \emph{a posteriori}, power analysis can help you estimate, for a fixed sample size and a given power, what is the minimum effect size that can be detected.

\hypertarget{factors-affecting-power}{%
\subsection{Factors affecting power}\label{factors-affecting-power}}

For a given statistical test, there are 3 factors that affect power.

\hypertarget{decision-criteria}{%
\subsubsection*{Decision criteria}\label{decision-criteria}}
\addcontentsline{toc}{subsubsection}{Decision criteria}

Power is related to \(\alpha\), the probability level at which one rejects the null hypothesis. If this decision criteria is made very strict (i.e.~if critical \(\alpha\) is set to a very low value, like 0.1\% or \(p = 0.001\)), then power will be lower than if the critical \(\alpha\) was less strict.

\hypertarget{sample-size}{%
\subsubsection*{Sample size}\label{sample-size}}
\addcontentsline{toc}{subsubsection}{Sample size}

The larger the sample size, the larger the power. As sample size increases, one's ability to detect small effect sizes as being statistically significant gets better.

\hypertarget{effect-size}{%
\subsubsection*{Effect size}\label{effect-size}}
\addcontentsline{toc}{subsubsection}{Effect size}

The larger the effect size, the larger the power. For a given sample size, the ability to detect an effect as being significant is higher for large effects than for small ones. Effect size measures how false the null hypothesis is.

\hypertarget{what-is-gpower}{%
\section{What is G*Power?}\label{what-is-gpower}}

G*Power is free software developed by quantitative psychologists from the University of Dusseldorf in Germany.
It is available in MacOS and Windows versions.
It can be run under Linux using Wine or a virtual machine.

G*Power will allow you to do power analyses for the majority of statistical tests we will cover during the term without making lengthy calculations and looking up long tables and figures of power curves. It is a really useful tool that you need to master.

It is possible to perform all analysis made by G*Power in R, but it requires a bit more code, and a better understanding of the process since everything should be coded by hand. In simple cases, R code is also provided.

\begin{rmdcode}
Download the software \textbf{\href{https://www.psychologie.hhu.de/arbeitsgruppen/allgemeine-psychologie-und-arbeitspsychologie/gpower.html}{here}} and install it on your computer and your workstation (if it is not there already).
\end{rmdcode}

\hypertarget{how-to-use-gpower}{%
\section{How to use G*Power}\label{how-to-use-gpower}}

\hypertarget{general-principle}{%
\subsection{General Principle}\label{general-principle}}

Using G*Power generally involves 3 steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Choosing the appropriate test
\item
  Choosing one of the 5 types of available power analyses
\item
  Enter parameter values and press the \textbf{Calculate} button
\end{enumerate}

\hypertarget{types-of-power-analyses}{%
\subsection{Types of power analyses}\label{types-of-power-analyses}}

First, \(\alpha\) is define as the probability level at which
one rejects the null hypothesis, and \(\beta\) is \(1 - power\).

\hypertarget{a-priori}{%
\subsubsection*{A priori}\label{a-priori}}
\addcontentsline{toc}{subsubsection}{A priori}

Computes the sample size required given \(\beta\), \(\alpha\), and the effect size. This type of analysis is useful when planning experiments.

\hypertarget{compromise}{%
\subsubsection*{Compromise}\label{compromise}}
\addcontentsline{toc}{subsubsection}{Compromise}

Computes \(\alpha\) and \(\beta\) for a given \(\alpha\)/\(\beta\) ratio, sample size, and effect size. Less commonly used (I have never used it myself) although it can be useful when the \(\alpha\)/\(\beta\) ratio has meaning, for example when the cost of type I and type II errors can be quantified.

\hypertarget{criterion}{%
\subsubsection*{Criterion}\label{criterion}}
\addcontentsline{toc}{subsubsection}{Criterion}

Computes \(\alpha\) for a given \(\beta\), sample size, and effect size. In practice, I see little interest in this. Let me know if you see something I don't!

\hypertarget{post-hoc}{%
\subsubsection*{Post-hoc}\label{post-hoc}}
\addcontentsline{toc}{subsubsection}{Post-hoc}

Computes the power for a given \(\alpha\), effect size, and
sample size. Used frequently to help in the interpretation of a test
that is not statistically significant, but only if an effect size that is
biologically significant is used (and not the observed effect size). Not
relevant when the test is significant.
Sensitivity. Computes the detectable effect size for a given \(\beta\) ,\(\alpha\), and
sample size. Very useful at the planning stage of an experiment.

\hypertarget{how-to-calculate-effect-size}{%
\subsection{How to calculate effect size}\label{how-to-calculate-effect-size}}

G*Power can perform power analyses for several statistical tests.
The metric for effect size depends on the test. Note that other
software packages often use different effect size metrics and that it is
important to use the correct one for each package. G\emph{Power has an
effect size calculator for many tests that only requires you to enter
the relevant values. The following table lists the effect size metrics
used by G}Power for the various tests.

\begin{longtable}[]{@{}lcl@{}}
\toprule
\begin{minipage}[b]{0.11\columnwidth}\raggedright
Test\strut
\end{minipage} & \begin{minipage}[b]{0.35\columnwidth}\centering
Taille d'effet\strut
\end{minipage} & \begin{minipage}[b]{0.46\columnwidth}\raggedright
Formule\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.11\columnwidth}\raggedright
t-test on means\strut
\end{minipage} & \begin{minipage}[t]{0.35\columnwidth}\centering
d\strut
\end{minipage} & \begin{minipage}[t]{0.46\columnwidth}\raggedright
\(d = \frac{|\mu_1 - \mu_2|}{\sqrt{({s_1}^2 + {s_2}^2)/2}}\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.11\columnwidth}\raggedright
t-test on correlations\strut
\end{minipage} & \begin{minipage}[t]{0.35\columnwidth}\centering
r\strut
\end{minipage} & \begin{minipage}[t]{0.46\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.11\columnwidth}\raggedright
other t-tests\strut
\end{minipage} & \begin{minipage}[t]{0.35\columnwidth}\centering
f\strut
\end{minipage} & \begin{minipage}[t]{0.46\columnwidth}\raggedright
\(f = \frac{\mu_1}{\sigma}\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.11\columnwidth}\raggedright
F-test (ANOVA)\strut
\end{minipage} & \begin{minipage}[t]{0.35\columnwidth}\centering
f\strut
\end{minipage} & \begin{minipage}[t]{0.46\columnwidth}\raggedright
\(f = \frac{\frac{\sqrt{\sum_{i=1}^k (\mu_i - \mu)^2}}{k}}{\sigma}\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.11\columnwidth}\raggedright
other F-tests\strut
\end{minipage} & \begin{minipage}[t]{0.35\columnwidth}\centering
\(f^2\)\strut
\end{minipage} & \begin{minipage}[t]{0.46\columnwidth}\raggedright
\(f^2 = \frac{{R_p}^2}{1-{R_p}^2}\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.11\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.35\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.46\columnwidth}\raggedright
\({R_p}\) is the squared partial correlation coefficient\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.11\columnwidth}\raggedright
Chi-square test\strut
\end{minipage} & \begin{minipage}[t]{0.35\columnwidth}\centering
w\strut
\end{minipage} & \begin{minipage}[t]{0.46\columnwidth}\raggedright
\(w = \sqrt{ \sum_{i=1}^m \frac{(p_{0i} - p_{1i})^2 }{ p_{0i}} }\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.11\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.35\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.46\columnwidth}\raggedright
\(p_{0i}\) and \(p_{1i}\) are theporportion in category \(i\) predicted by the null, \(_0\), and alternative, \(_1\), hypothesis\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{power-analysis-for-a-t-test-on-two-independent-means}{%
\section{Power analysis for a t-test on two independent means}\label{power-analysis-for-a-t-test-on-two-independent-means}}

The objective of this lab is to learn to use G*Power and understand how the 4 parameters of power analyses (\(\alpha\), \(\beta\), sample size and effect size) are related to each other. For this, you will only use the standard t-test to compare two independent means. This is the test most used by biologists, you have all used it, and it will serve admirably for this lab. What you will learn today will be applicable to all other power analyses.

Jaynie Stephenson studied the productivity of streams in the Ottawa region. She has measured fish biomass in 36 streams, 18 on the Shield and 18 in the Ottawa Valley. She found that fish biomass was lower in streams from the valley (2.64 \(g/m^2\) , standard deviation = 3.28) than from the Shield (3.31 \(g/m^2\) , standard deviation = 2.79.).

When she tested the null hypothesis that fish biomass is the same in the two regions by a t-test, she obtained:

\begin{verbatim}
Pooled-Variance Two-Sample t-Test
t = -0.5746, df = 34, p-value = 0.5693
\end{verbatim}

She therefore accepted the null hypothesis (since p is much larger than 0.05) and concluded that fish biomass is the same in the two regions.

\hypertarget{post-hoc-analysis}{%
\subsection{Post-hoc analysis}\label{post-hoc-analysis}}

Using the observed means and standard deviations, we can use G*Power to calculate the power of the two-tailed t-test for two independent means, using the observed effect size (the difference between the two means, weighted by the standard deviations) for \(\alpha\) = 0.05.

Start G*Power.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  In *\textbf{Test family} , choose: t tests
\item
  For \textbf{Statistical test} , choose: Means: Difference between two independent means (two groups)
\item
  For \textbf{Type of power analysis} , choose: Post hoc: Compute achieved power - given \(\alpha\), sample size, and effect size
\item
  At \textbf{Input Parameters} ,
\end{enumerate}

\begin{itemize}
\tightlist
\item
  in the box \textbf{Tail(s)} , chose: Two,
\item
  check that \(\alpha\) \textbf{err prob} is equal to 0.05
\item
  Enter 18 for the \textbf{Sample size} of group 1 and of group 2
\item
  then, to calculate effect size (d), click on \textbf{Determine =\textgreater{}}
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  In the window that opens,
\end{enumerate}

\begin{itemize}
\tightlist
\item
  select \textbf{n1 = n2} , then
\item
  enter the two means (\textbf{Mean group} 1 et 2)
\item
  the two standard deviations(\textbf{SD group} 1 et 2)
\item
  click on \textbf{Calculate} and \textbf{transfer to main window}
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  After you click on the \textbf{Calculate button} in the main window, you
  should get the following:
\end{enumerate}

\begin{figure}
\includegraphics[width=1\linewidth]{images/gpower_1} \caption{Post-hoc analysis with estimated effect size}\label{fig:gpower-1}
\end{figure}

Similar analysis can be done in R. You first need to calculate the effect size \texttt{d} for a t-test comparing 2 means, and then use the \texttt{pwr.t.test()} function from the \texttt{pwr} üì¶.
The easiest is to create anew function in R to estimate the effect size \texttt{d}since we are going to reuse it multiple times during the lab.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# load package pwr}
\KeywordTok{library}\NormalTok{(pwr)}
\CommentTok{\# define d for a 2 sample t{-}test}
\NormalTok{d \textless{}{-}}\StringTok{ }\ControlFlowTok{function}\NormalTok{(u1, u2, sd1, sd2) \{}
  \KeywordTok{abs}\NormalTok{(u1 }\OperatorTok{{-}}\StringTok{ }\NormalTok{u2) }\OperatorTok{/}\StringTok{ }\KeywordTok{sqrt}\NormalTok{((sd1}\OperatorTok{\^{}}\DecValTok{2} \OperatorTok{+}\StringTok{ }\NormalTok{sd2}\OperatorTok{\^{}}\DecValTok{2}\NormalTok{) }\OperatorTok{/}\StringTok{ }\DecValTok{2}\NormalTok{)}
\NormalTok{\}}

\CommentTok{\# power analysis}
\KeywordTok{pwr.t.test}\NormalTok{(}
  \DataTypeTok{n =} \DecValTok{18}\NormalTok{,}
  \DataTypeTok{d =} \KeywordTok{d}\NormalTok{(}\DataTypeTok{u1 =} \FloatTok{2.64}\NormalTok{, }\DataTypeTok{sd1 =} \FloatTok{3.28}\NormalTok{, }\DataTypeTok{u2 =} \FloatTok{3.31}\NormalTok{, }\DataTypeTok{sd2 =} \FloatTok{2.79}\NormalTok{),}
  \DataTypeTok{sig.level =} \FloatTok{0.05}\NormalTok{,}
  \DataTypeTok{type =} \StringTok{"two.sample"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##      Two-sample t test power calculation 
## 
##               n = 18
##               d = 0.220042
##       sig.level = 0.05
##           power = 0.09833902
##     alternative = two.sided
## 
## NOTE: n is number in *each* group
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# plot similar to G*Power}
\NormalTok{x \textless{}{-}}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\OperatorTok{{-}}\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DataTypeTok{length =} \DecValTok{200}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(x, }\KeywordTok{dnorm}\NormalTok{(x), }\DataTypeTok{type =} \StringTok{"l"}\NormalTok{, }\DataTypeTok{col =} \StringTok{"red"}\NormalTok{, }\DataTypeTok{lwd =} \DecValTok{2}\NormalTok{)}
\NormalTok{qc \textless{}{-}}\StringTok{ }\KeywordTok{qt}\NormalTok{(}\FloatTok{0.025}\NormalTok{, }\DecValTok{16}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v =}\NormalTok{ qc, }\DataTypeTok{col =} \StringTok{"green"}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v =} \OperatorTok{{-}}\NormalTok{qc, }\DataTypeTok{col =} \StringTok{"green"}\NormalTok{)}
\KeywordTok{lines}\NormalTok{(x, }\KeywordTok{dnorm}\NormalTok{(x, }\DataTypeTok{mean =}\NormalTok{ (}\FloatTok{3.31} \OperatorTok{{-}}\StringTok{ }\FloatTok{2.64}\NormalTok{)), }\DataTypeTok{type =} \StringTok{"l"}\NormalTok{, }\DataTypeTok{col =} \StringTok{"blue"}\NormalTok{, }\DataTypeTok{lwd =} \DecValTok{2}\NormalTok{)}

\CommentTok{\# power corresponds to the shaded area}
\NormalTok{y \textless{}{-}}\StringTok{ }\KeywordTok{dnorm}\NormalTok{(x, }\DataTypeTok{mean =}\NormalTok{ (}\FloatTok{3.31} \OperatorTok{{-}}\StringTok{ }\FloatTok{2.64}\NormalTok{))}
\KeywordTok{polygon}\NormalTok{(}
  \KeywordTok{c}\NormalTok{(x[x }\OperatorTok{\textless{}=}\StringTok{ }\OperatorTok{{-}}\NormalTok{qc], }\OperatorTok{{-}}\NormalTok{qc), }\KeywordTok{c}\NormalTok{(y[x }\OperatorTok{\textless{}=}\StringTok{ }\OperatorTok{{-}}\NormalTok{qc], }\DecValTok{0}\NormalTok{),}
  \DataTypeTok{col =} \KeywordTok{rgb}\NormalTok{(}\DataTypeTok{red =} \DecValTok{0}\NormalTok{, }\DataTypeTok{green =} \FloatTok{0.2}\NormalTok{, }\DataTypeTok{blue =} \DecValTok{1}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.5}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{Labs_BIO4158_files/figure-latex/r-power-1.pdf}
\caption{\label{fig:r-power}Post-hoc analysis with estimated effect size in R}
\end{figure}

Let's examine the figure \ref{fig:gpower-1}.

\begin{itemize}
\tightlist
\item
  The curve on the left, in red, corresponds to the expected distri- bution of the t-statistics when \(H_0\) is true (\emph{i.e.} when the two means are equal) given the sample size (18 per region) and the observed standard deviations.
\item
  The vertical green lines correspond to the critical values of t for \(\alpha = 0.05\) and a total sample size of 36 (2x18).
\item
  The shaded pink regions correspond to the rejection zones for \(H_0\). If Jaynie had obtained a \emph{t-value} outside the interval delimited by the critical values ranging from -2.03224 to 2.03224, she would then have rejected \(H_0\) , the null hypothesis of equal means. In fact, she obtained a t-value of -0.5746 and concluded that the biomass is equal in the two regions.
\item
  The curve on the right, in blue, corresponds to the expected dis- tribution of the t-statistics if \(H_1\) is true (here \(H_1\) is that there is a difference in biomass between the two regions equal to \(3.33 - 2.64 = 0.69 g/m^2\) , given the observed standard deviations). This distribution is what we should observe if \(H_1\) was true and we repeated a large number of times the experiment using random samples of 18 streams in each of the two regions and calculated a t-statistic for each sample. On average, we would obtain a t-statistic of about 0.6.
\item
  Note that there is considerable overlap of the two distributions and that a large fraction of the surface under the right curve is within the interval where \(H_0\) is accepted between the two vertical green lines at -2.03224 and 2.03224. This proportion, shaded in blue under the distribution on the right is labeled \(\beta\) and corresponds to the risk of \emph{type II error} (accept \(H_0\) when \(H_1\) is true).
\item
  Power is simply \(1-\beta\), and is here 0.098339. Therefore, if the mean biomass differed by \(0.69 g/m^2\) between the two regions, Jaynie had only 9.8\% chance of being able to detect it as a statistically significant difference at ÔÅ°=5\% with a sample size of 18 streams in each region.
\end{itemize}

\textbf{Let's recapitulate}: The difference in biomass between regions is not statistically significant according to the t-test. It is because the difference is relatively small relative to the precision of the measurements. It is therefore not surprising that that power, i.e.~the probability of detecting a statistically significant difference, is small. Therefore, this analysis is not very informative.

\textbf{Indeed, a post hoc power analysis using the observed effect size is not useful}. It is much more informative to conduct a post hoc power analysis for an effect size that is different from the observed effect size. But what effect size to use? It is the biology of the system under study that will guide you. For example, with respect to fish biomass in streams, one could argue that a two fold change in biomass (say from 2.64 to 5.28 g/m\textsuperscript{2} ) has ecologically significant repercussions. We would therefore want to know if Jaynie had a good chance of detecting a difference as large as this before accepting her conclusion that the biomass is the same in the two regions. So, what were the odds that Jaynie could detect a difference of 2.64 g/m\textsuperscript{2} between the two regions? G*Power can tell you if you cajole it the right way.

\begin{rmdcode}
Change the mean of group 2 to 5.28, recalculate effect size, and click on Calculate to obtain figure \ref{fig:gpower-2}.
\end{rmdcode}

\begin{figure}
\includegraphics[width=1\linewidth]{images/gpower_2} \caption{Post-hoc analysis using an effect size different from the one estimated}\label{fig:gpower-2}
\end{figure}

Same analysis using R (without all the code for the interesting but not really useful plot)

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pwr.t.test}\NormalTok{(}
  \DataTypeTok{n =} \DecValTok{18}\NormalTok{,}
  \DataTypeTok{d =} \KeywordTok{d}\NormalTok{(}\DataTypeTok{u1 =} \FloatTok{2.64}\NormalTok{, }\DataTypeTok{sd1 =} \FloatTok{3.28}\NormalTok{, }\DataTypeTok{u2 =} \FloatTok{5.28}\NormalTok{, }\DataTypeTok{sd2 =} \FloatTok{2.79}\NormalTok{),}
  \DataTypeTok{sig.level =} \FloatTok{0.05}\NormalTok{,}
  \DataTypeTok{type =} \StringTok{"two.sample"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##      Two-sample t test power calculation 
## 
##               n = 18
##               d = 0.8670313
##       sig.level = 0.05
##           power = 0.7146763
##     alternative = two.sided
## 
## NOTE: n is number in *each* group
\end{verbatim}

The power is 0.71, therefore Jaynie had a reasonable chance (71\%) of detecting a doubling of biomass with 18 streams in each region.

Note that this post hoc power analysis, done for an effect size considered biologically meaningful, is much more informative than the preceeding one done with the observed effect size (which is what too many students do because it is the default of so many power calculation programs). Jaynie did not detect a difference between the two regions. There are two possibilities: 1) there is really no difference between the regions, or 2) the precision of measurements is so low (because the sample size is small and/or there is large variability within a region) that it is very unlikely to be able to detect even large differences. The second power analysis can eliminate this second possibility because Jaynie had 71\% chances of detecting a doubling of biomass.

\hypertarget{a-priori-analysis}{%
\subsection{A priori analysis}\label{a-priori-analysis}}

Suppose that a difference in biomass of \(3.31-2.64 = 0.67 g/m^2\) can be ecologically significant. The next field season should be planned so that Jaynie would have a good chance of detecting such a difference in fish biomass between regions. How many streams should Jaynie sample in each region to have 80\% of detecting such a difference (given the observed variability)?

\begin{rmdcode}
Change the type of power analysis in G*Power to \textbf{A priori: Compute sample size - given \(\alpha\) , power, and effect size}. Ensure that the values for means and standard deviations are those obtained by Jaynie. Recalculate the effect size metric and enter 0.8 for power and you will obtain (\ref{fig:gpower-3}.
\end{rmdcode}

\begin{figure}
\includegraphics[width=1\linewidth]{images/gpower_3} \caption{A priori analysis}\label{fig:gpower-3}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pwr.t.test}\NormalTok{(}
  \DataTypeTok{power =} \FloatTok{0.8}\NormalTok{,}
  \DataTypeTok{d =} \KeywordTok{d}\NormalTok{(}\DataTypeTok{u1 =} \FloatTok{2.64}\NormalTok{, }\DataTypeTok{sd1 =} \FloatTok{3.28}\NormalTok{, }\DataTypeTok{u2 =} \FloatTok{3.31}\NormalTok{, }\DataTypeTok{sd2 =} \FloatTok{2.79}\NormalTok{),}
  \DataTypeTok{sig.level =} \FloatTok{0.05}\NormalTok{,}
  \DataTypeTok{type =} \StringTok{"two.sample"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##      Two-sample t test power calculation 
## 
##               n = 325.1723
##               d = 0.220042
##       sig.level = 0.05
##           power = 0.8
##     alternative = two.sided
## 
## NOTE: n is number in *each* group
\end{verbatim}

\textbf{Ouch!} The required sample would be of 326 streams in each region! It would cost a fortune and require several field teams otherwise only a few dozen streams could be sampled over the summer and it would be very unlikely that such a small difference in biomass could be detected. Sampling fewer streams would probably be in vain and could be considered as a waste of effort and time: why do the work on several dozens of streams if the odds of success are that low?

If we recalculate for a power of 95\%, we find that 538 streams would be required from each region. Increasing power means more work!

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pwr.t.test}\NormalTok{(}
  \DataTypeTok{power =} \FloatTok{0.95}\NormalTok{,}
  \DataTypeTok{d =} \KeywordTok{d}\NormalTok{(}\DataTypeTok{u1 =} \FloatTok{2.64}\NormalTok{, }\DataTypeTok{sd1 =} \FloatTok{3.28}\NormalTok{, }\DataTypeTok{u2 =} \FloatTok{3.31}\NormalTok{, }\DataTypeTok{sd2 =} \FloatTok{2.79}\NormalTok{),}
  \DataTypeTok{sig.level =} \FloatTok{0.05}\NormalTok{,}
  \DataTypeTok{type =} \StringTok{"two.sample"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##      Two-sample t test power calculation 
## 
##               n = 537.7286
##               d = 0.220042
##       sig.level = 0.05
##           power = 0.95
##     alternative = two.sided
## 
## NOTE: n is number in *each* group
\end{verbatim}

\hypertarget{sensitivity-analysis---calculate-the-detectable-effect-size}{%
\subsection{Sensitivity analysis - Calculate the detectable effect size}\label{sensitivity-analysis---calculate-the-detectable-effect-size}}

Given the observed variability, a sampling effort of 18 streams per region, and with \(\alpha\) = 0.05, what effect size could Jaynie detect with 80\% probability \(\beta=0.2\))?

\begin{rmdcode}
Change analysis type in G*Power to \textbf{Sensitivity: Compute required effect size - given \(\alpha\) , power, and sample size} and size is 18 in each region.
\end{rmdcode}

\begin{figure}
\includegraphics[width=1\linewidth]{images/gpower_4} \caption{Analyse de sensitivit√©}\label{fig:gpower-4}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pwr.t.test}\NormalTok{(}
  \DataTypeTok{power =} \FloatTok{0.8}\NormalTok{,}
  \DataTypeTok{n =} \DecValTok{18}\NormalTok{,}
  \DataTypeTok{sig.level =} \FloatTok{0.05}\NormalTok{,}
  \DataTypeTok{type =} \StringTok{"two.sample"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##      Two-sample t test power calculation 
## 
##               n = 18
##               d = 0.9612854
##       sig.level = 0.05
##           power = 0.8
##     alternative = two.sided
## 
## NOTE: n is number in *each* group
\end{verbatim}

The detectable effect size for this sample size, \(\alpha = 0.05\) and \(\beta = 0.2\) (or power of 80\%) is 0.961296.

\begin{rmdcaution}
Attention, this effect size is the metric \texttt{d} and is dependent on sampling variability.
\end{rmdcaution}

Here, \texttt{d} is approximately equal to

\[ d = \frac{| \bar{X_1} \bar{X_2} |} {\sqrt{\frac{{s_1}^2 +{s_2}^2}{2}}}\]

To convert this d value without units into a value for the detectable difference in biomass between the two regions, you need to multiply \texttt{d} by the denominator of the equation.

\[
| \bar{X_1} - \bar{X_2} | = d * \sqrt{\frac{{s_1}^2 +{s_2}^2}{2}}
\]

In R this can be done with the following code

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pwr.t.test}\NormalTok{(}
  \DataTypeTok{power =} \FloatTok{0.8}\NormalTok{,}
  \DataTypeTok{n =} \DecValTok{18}\NormalTok{,}
  \DataTypeTok{sig.level =} \FloatTok{0.05}\NormalTok{,}
  \DataTypeTok{type =} \StringTok{"two.sample"}\NormalTok{)}\OperatorTok{$}\NormalTok{d }\OperatorTok{*}\StringTok{ }\KeywordTok{sqrt}\NormalTok{((}\FloatTok{3.28}\OperatorTok{\^{}}\DecValTok{2} \OperatorTok{+}\StringTok{ }\FloatTok{2.79}\OperatorTok{\^{}}\DecValTok{2}\NormalTok{) }\OperatorTok{/}\StringTok{ }\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2.926992
\end{verbatim}

Therefore, with 18 streams per region, \(\alpha\) = 0.05 and \(\beta\) = 0.2 (so power of 80\%), Jaynie could detect a difference of 2.93 g/m\textsuperscript{2} between regions, a bit more than a doubling of biomass.

\hypertarget{important-points-to-remember}{%
\section{Important points to remember}\label{important-points-to-remember}}

\begin{itemize}
\tightlist
\item
  Post hoc power analyses are relevant only when the null hypothesis is accepted because it is impossible to make a \emph{type II error} when rejecting \(H_0\) .
\item
  With very large samples, power is very high and minute differences can be statistically detected, even if they are not biologically significant.
\item
  When using a stricter significance criteria (\(\alpha < 0.05\)) power is reduced.
\item
  Maximizing power implies more sampling effort, unless you use a more liberal statistical criteria (\(\alpha > 0.05\))
\item
  The choice of \(\beta\) is somewhat arbitrary. \(\beta=0.2\) (power of 80\%) is considered relatively high by most.
\end{itemize}

\hypertarget{correlation-and-simple-linear-regression}{%
\chapter{Correlation and simple linear regression}\label{correlation-and-simple-linear-regression}}

After completing this laboratory exercise, you should be able to:

\begin{itemize}
\tightlist
\item
  Use R to produce a scatter plot of the relationship between two
  variables.
\item
  Use R to carry out some simple data transformations.
\item
  Use R to compute the Pearson product-moment correlation
  between two variables and assess its statistical significance.
\item
  Use R to compute the correlation between pairs of ranked vari-
  ables using the Spearman rank correlation and Kendall's tau.
\item
  Use R to assess the significance of pairwise comparisons from a
  generalized correlation matrix using Bonferroni-adjusted proba-
  bilities.
\item
  Use R do a simple linear regression
\item
  Use R to test the validity of the assumptions underlying simple lin-
  ear regression
\item
  Use R to assess significance of a regression by the bootstrap
  method
\item
  Quantify effect size in simple regression and perform a power
  analysis using G*Power
\end{itemize}

\hypertarget{set-lm}{%
\section{R packages and data}\label{set-lm}}

For this la b you need:

\begin{itemize}
\tightlist
\item
  R packages:

  \begin{itemize}
  \tightlist
  \item
    car
  \item
    lmtest
  \item
    boot
  \item
    pwr
  \item
    ggplot
  \item
    performance
  \end{itemize}
\item
  data:

  \begin{itemize}
  \tightlist
  \item
    sturgeon.csv
  \end{itemize}
\end{itemize}

You need to load the packages in R with \texttt{library()} and if need needed install them first with \texttt{install.packages()}
For the data, load them using the \texttt{read.csv()} function.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(car)}
\KeywordTok{library}\NormalTok{(lmtest)}
\KeywordTok{library}\NormalTok{(performance)}
\KeywordTok{library}\NormalTok{(boot)}
\KeywordTok{library}\NormalTok{(ggplot2)}
\KeywordTok{library}\NormalTok{(pwr)}

\NormalTok{sturgeon \textless{}{-}}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"data/sturgeon.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{rmdnote}
Note that the command to read the data assumes that the data file is in a folder named \texttt{data} within the working directory. Adjust as needed.
\end{rmdnote}

\hypertarget{scatter-plots}{%
\section{Scatter plots}\label{scatter-plots}}

Correlation and regression analysis should always begin with an examination of the data: this is a critical first step in determining whether such analyses are even appropriate for your data.
Suppose we are interested in the extent to which length of male sturgeon in the vicinity of The Pas and Cumberland House covaries with weight. To address this question, we look at the correlation between \texttt{fklngth} and \texttt{rdwght}.
Recall that one of the assumptions in correlation analysis is that the
relationship between the two variables is linear. To evaluate this
assumption, a good first step is to produce a scatterplot.

\begin{itemize}
\tightlist
\item
  Load the data from \texttt{sturgeon.csv} in an obk=jcet named \texttt{sturgeon}.
  Make a scatter plot of \texttt{rdwght} vs \texttt{fklngth} fit with a locally weighted regression (Loess) smoother, and a linear regression line.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sturgeon \textless{}{-}}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"data/sturgeon.csv"}\NormalTok{)}
\KeywordTok{str}\NormalTok{(sturgeon)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 'data.frame':    186 obs. of  9 variables:
##  $ fklngth : num  37 50.2 28.9 50.2 45.6 ...
##  $ totlngth: num  40.7 54.1 31.3 53.1 49.5 ...
##  $ drlngth : num  23.6 31.5 17.3 32.3 32.1 ...
##  $ rdwght  : num  15.95 NA 6.49 NA 29.92 ...
##  $ age     : int  11 24 7 23 20 23 20 7 23 19 ...
##  $ girth   : num  40.5 53.5 31 52.5 50 54.2 48 28.5 44 39 ...
##  $ sex     : chr  "MALE" "FEMALE" "MALE" "FEMALE" ...
##  $ location: chr  "THE_PAS" "THE_PAS" "THE_PAS" "THE_PAS" ...
##  $ year    : int  1978 1978 1978 1978 1978 1978 1978 1978 1978 1978 ...
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mygraph \textless{}{-}}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}
  \DataTypeTok{data =}\NormalTok{ sturgeon[}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(sturgeon}\OperatorTok{$}\NormalTok{rdwght), ], }\CommentTok{\# source of data}
  \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ fklngth, }\DataTypeTok{y =}\NormalTok{ rdwght)}
\NormalTok{)}
\CommentTok{\# plot data points, regression, loess trace}
\NormalTok{mygraph \textless{}{-}}\StringTok{ }\NormalTok{mygraph }\OperatorTok{+}
\StringTok{  }\KeywordTok{stat\_smooth}\NormalTok{(}\DataTypeTok{method =}\NormalTok{ lm, }\DataTypeTok{se =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{color =} \StringTok{"green"}\NormalTok{) }\OperatorTok{+}\StringTok{ }\CommentTok{\# add linear regression, but no SE shading}
\StringTok{  }\KeywordTok{stat\_smooth}\NormalTok{(}\DataTypeTok{color =} \StringTok{"red"}\NormalTok{, }\DataTypeTok{se =} \OtherTok{FALSE}\NormalTok{) }\OperatorTok{+}\StringTok{ }\CommentTok{\# add loess}
\StringTok{  }\KeywordTok{geom\_point}\NormalTok{() }\CommentTok{\# add data points}

\NormalTok{mygraph }\CommentTok{\# display graph}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{Labs_BIO4158_files/figure-latex/stur-2-1.pdf}
\caption{\label{fig:stur-2}Scatter plot of Weight as a function of length in sturgeons}
\end{figure}

\begin{itemize}
\tightlist
\item
  Does this curve suggest a good correlation between the two? Based on visual inspection, does the relationship between these two variables appear linear?
\end{itemize}

There is some evidence of nonlinearity, as the curve appears to have a positive second derivative (concave up). This notwithstanding, it does appear the two variables are highly correlated.

\begin{itemize}
\tightlist
\item
  Redo the scatterplot, but after logtransformation of both axes.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# apply log transformation on defined graph}
\NormalTok{mygraph }\OperatorTok{+}\StringTok{ }\KeywordTok{scale\_x\_log10}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{scale\_y\_log10}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{Labs_BIO4158_files/figure-latex/stur-log-1.pdf}
\caption{\label{fig:stur-log}Plot weight-length in sturgeon using a log scale}
\end{figure}

Compare the diagrams before and after the transformation (Figs \ref{fig:stur-2} and \ref{fig:stur-log}). Since the relationship is more linear after transformation, correlation analysis should be done on the transformed data

\hypertarget{data-transformations-and-the-product-moment-correlation}{%
\section{Data transformations and the product-moment correlation}\label{data-transformations-and-the-product-moment-correlation}}

Recall that another assumption underlying significance testing of the product-moment correlation is that the distribution of the two variables in question is bivariate normal. We can test to see whether each of the two variables are normally distributed using the same procedures outlined in the exercise on two-sample comparisons. If the two variables are each normally distributed, then one is usually (relatively) safe in assuming the joint distribution is normal, although this needn't necessarily be true.

\begin{itemize}
\tightlist
\item
  Examine the distribution of the 4 variables (the two original variables and the log-transformed variables). What do you conclude from visual inspection of these plots?
\end{itemize}

The following graph contains the 4 QQ plots (\texttt{qqplot()}). It was produced by the code below that starts with the \texttt{par()} command to ensure that all 4 plots would appear together on the same page in 2 rows and 2 columns:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{)) }\CommentTok{\# split graph in 4 (2 rows, 2 cols) filling by rows}
\KeywordTok{qqnorm}\NormalTok{(sturgeon}\OperatorTok{$}\NormalTok{fklngth, }\DataTypeTok{ylab =} \StringTok{"fklngth"}\NormalTok{)}
\KeywordTok{qqline}\NormalTok{(sturgeon}\OperatorTok{$}\NormalTok{fklngth)}
\KeywordTok{qqnorm}\NormalTok{(}\KeywordTok{log10}\NormalTok{(sturgeon}\OperatorTok{$}\NormalTok{fklngth), }\DataTypeTok{ylab =} \StringTok{"log10(fklngth)"}\NormalTok{)}
\KeywordTok{qqline}\NormalTok{(}\KeywordTok{log10}\NormalTok{(sturgeon}\OperatorTok{$}\NormalTok{fklngth))}
\KeywordTok{qqnorm}\NormalTok{(sturgeon}\OperatorTok{$}\NormalTok{rdwght, }\DataTypeTok{ylab =} \StringTok{"rdwght"}\NormalTok{)}
\KeywordTok{qqline}\NormalTok{(sturgeon}\OperatorTok{$}\NormalTok{rdwght)}
\KeywordTok{qqnorm}\NormalTok{(}\KeywordTok{log10}\NormalTok{(sturgeon}\OperatorTok{$}\NormalTok{rdwght), }\DataTypeTok{ylab =} \StringTok{"log10(rdwgth)"}\NormalTok{)}
\KeywordTok{qqline}\NormalTok{(}\KeywordTok{log10}\NormalTok{(sturgeon}\OperatorTok{$}\NormalTok{rdwght))}
\end{Highlighting}
\end{Shaded}

\includegraphics{Labs_BIO4158_files/figure-latex/stur-4hist-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)) }\CommentTok{\# redefine plotting area to 1 plot}
\end{Highlighting}
\end{Shaded}

None of these distributions are perfectly normal, but deviations are mostly minor.

\begin{itemize}
\tightlist
\item
  To generate a scatterplot matrix of all pairs of variables, with linear regression and lowess traces, you can use \texttt{scatterplotMatrix} from \texttt{car} `r emo::ji(``package'').
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{scatterplotMatrix}\NormalTok{(}
  \OperatorTok{\textasciitilde{}}\StringTok{ }\NormalTok{fklngth }\OperatorTok{+}\StringTok{ }\KeywordTok{log10}\NormalTok{(fklngth) }\OperatorTok{+}\StringTok{ }\NormalTok{rdwght }\OperatorTok{+}\StringTok{ }\KeywordTok{log10}\NormalTok{(rdwght),}
  \DataTypeTok{data =}\NormalTok{ sturgeon,}
  \DataTypeTok{smooth =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{diagonal =} \StringTok{"density"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in applyDefaults(diagonal, defaults = list(method =
## "adaptiveDensity"), : unnamed diag arguments, will be ignored
\end{verbatim}

\includegraphics{Labs_BIO4158_files/figure-latex/stur-scatmat-1.pdf}

\begin{itemize}
\tightlist
\item
  Next, calculate the Pearson product-moment correlation between each pair (untransformed and log transformed) using the \texttt{cor()} command. However, to do this, it will be easier if you first add your transformed data as columns in the sturgeon data frame.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sturgeon}\OperatorTok{$}\NormalTok{lfklngth \textless{}{-}}\StringTok{ }\KeywordTok{with}\NormalTok{(sturgeon, }\KeywordTok{log10}\NormalTok{(fklngth))}
\NormalTok{sturgeon}\OperatorTok{$}\NormalTok{lrdwght \textless{}{-}}\StringTok{ }\KeywordTok{log10}\NormalTok{(sturgeon}\OperatorTok{$}\NormalTok{rdwght)}
\end{Highlighting}
\end{Shaded}

Then you can get the correlation matrix by:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cor}\NormalTok{(sturgeon[, }\KeywordTok{c}\NormalTok{(}\StringTok{"fklngth"}\NormalTok{, }\StringTok{"lfklngth"}\NormalTok{, }\StringTok{"lrdwght"}\NormalTok{, }\StringTok{"rdwght"}\NormalTok{)], }\DataTypeTok{use =} \StringTok{"complete.obs"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Note the \texttt{use="complete.obs"} parameter. It tells R to keep only lines of the data frame where all variables were measured. If there are missing data, some lines will be removed, but correlations will be calculated for the same subset of cases for all pairs of variables. One could use, instead, \texttt{use="pairwise.complete.obs"} , to tell R to only eliminate observations when values are missing for this particular pair of variables. In this situation, if there are missing values in the data frame, the sample size for pairwise correlations will vary. In general, I recommend you use the option \texttt{use="complete.obs"}, unless you have so many missing values that it eliminates the majority of your data.

\begin{itemize}
\tightlist
\item
  Why is the correlation between the untransformed variables smaller than between the transformed variables?
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cor}\NormalTok{(sturgeon[, }\KeywordTok{c}\NormalTok{(}\StringTok{"fklngth"}\NormalTok{, }\StringTok{"lfklngth"}\NormalTok{, }\StringTok{"lrdwght"}\NormalTok{, }\StringTok{"rdwght"}\NormalTok{)], }\DataTypeTok{use =} \StringTok{"complete.obs"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##            fklngth  lfklngth   lrdwght    rdwght
## fklngth  1.0000000 0.9921435 0.9645108 0.9175435
## lfklngth 0.9921435 1.0000000 0.9670139 0.8756203
## lrdwght  0.9645108 0.9670139 1.0000000 0.9265513
## rdwght   0.9175435 0.8756203 0.9265513 1.0000000
\end{verbatim}

Several things should be noted here.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  the correlation between fork length and round weight is high, regardless of which variables are used: so as might be expected, heavier fish are also longer, and vice versa
\item
  the correlation is greater for the transformed variables than the untransformed variables.
\end{enumerate}

\textbf{Why?} Because the correlation coefficient is inversely proportional to the amount of scatter around a straight line. If the relationship is curvilinear (as it is for the untransformed data), the scatter around a straight line will be greater than if the relationship is linear.
Hence, the correlation coefficient will be smaller.

\hypertarget{testing-the-significance-of-correlations-and-bonferroni-probabilities}{%
\section{Testing the significance of correlations and Bonferroni probabilities}\label{testing-the-significance-of-correlations-and-bonferroni-probabilities}}

It's possible to test the significance of individual correlations using the commands window.
As an example, let's try testing the significance of the correlation between lfklngth and rdwght (the smallest correlation in the above table).

\begin{itemize}
\tightlist
\item
  In the R script window, enter the following to test the correlation
  between \texttt{lfkgnth} and \texttt{rdwght} :
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cor.test}\NormalTok{(}
\NormalTok{  sturgeon}\OperatorTok{$}\NormalTok{lfklngth, sturgeon}\OperatorTok{$}\NormalTok{rdwght,}
  \DataTypeTok{alternative =} \StringTok{"two.sided"}\NormalTok{,}
  \DataTypeTok{method =} \StringTok{"pearson"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Pearson's product-moment correlation
## 
## data:  sturgeon$lfklngth and sturgeon$rdwght
## t = 24.322, df = 180, p-value < 2.2e-16
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  0.8367345 0.9057199
## sample estimates:
##       cor 
## 0.8756203
\end{verbatim}

We see here that the correlation is highly significant (\(p< 2.2e-16\)),
which is no surprise given how high the correlation coefficient is (0.8756).

It's important to bear in mind that when you are estimating correlations, the probability of finding any one correlation that is \emph{``significant''} by pure chance increases with the number of pairwise correlations examined. Suppose, for example, that you have five variables; there are then a total of 10 possible pairwise correlations, and from this set, you would probably not be surprised to find at least one that is ``significant'' purely by chance. One way of avoiding the problem is to adjust individual \(\alpha\) levels for pairwise correlations by dividing by the number of comparisons, k, such that: \(\alpha' = \frac{\alpha}{k}\) (Bonferroni probabilities), i.e.~if initially, \(\alpha = 0.05\) and there are a total of 10 comparisons, then \(\alpha'= 0.005\).

In the above example where we examined correlations between
\texttt{fklngth} and \texttt{rdwght} and their \texttt{log}, it would be appropriate to adjust
the \(\alpha\) at which significance is tested by the total number of
correlations in the matrix (in this case, 6, so \(\alpha'=0.0083\)). Does your
decision about the significance of the correlation between \texttt{lfklngth} and
\texttt{rdwght} change?

\hypertarget{non-parametric-correlations-spearmans-rank-and-kendalls-tau}{%
\section{\texorpdfstring{Non-parametric correlations: Spearman's rank and Kendall's \(\tau\)}{Non-parametric correlations: Spearman's rank and Kendall's \textbackslash tau}}\label{non-parametric-correlations-spearmans-rank-and-kendalls-tau}}

The analysis done with the sturgeon data in the section above suggests that one of the assumptions of correlation, namely, bivariate normality, may not be valid for \texttt{fklngth} and \texttt{rdwght} nor for the log transforms of these variables. Finding an appropriate transformation is sometimes like looking for a needle in a haystack; indeed, it can be much worse simply because for some distributions, there is no transformation that will normalize the data. In such cases, the best option may be to go to a non-parametric analysis that does not assume bivariate normality or linearity. All such correlations are based on the ranks rather than the data themselves: two options available in
R are Spearman and Kendall's \(\tau\) (tau).

\begin{itemize}
\tightlist
\item
  Test the correlation between \texttt{fklngth} and \texttt{rdwght} using both the Spearman and Kendall's tau. The following commands will produce the correlations:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cor.test}\NormalTok{(}
\NormalTok{  sturgeon}\OperatorTok{$}\NormalTok{lfklngth, sturgeon}\OperatorTok{$}\NormalTok{rdwght,}
  \DataTypeTok{alternative =} \StringTok{"two.sided"}\NormalTok{,}
  \DataTypeTok{method =} \StringTok{"spearman"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in cor.test.default(sturgeon$lfklngth, sturgeon$rdwght, alternative =
## "two.sided", : Cannot compute exact p-value with ties
\end{verbatim}

\begin{verbatim}
## 
##  Spearman's rank correlation rho
## 
## data:  sturgeon$lfklngth and sturgeon$rdwght
## S = 47971, p-value < 2.2e-16
## alternative hypothesis: true rho is not equal to 0
## sample estimates:
##       rho 
## 0.9522546
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cor.test}\NormalTok{(}
\NormalTok{  sturgeon}\OperatorTok{$}\NormalTok{lfklngth, sturgeon}\OperatorTok{$}\NormalTok{rdwght,}
  \DataTypeTok{alternative =} \StringTok{"two.sided"}\NormalTok{,}
  \DataTypeTok{method =} \StringTok{"kendall"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Kendall's rank correlation tau
## 
## data:  sturgeon$lfklngth and sturgeon$rdwght
## z = 16.358, p-value < 2.2e-16
## alternative hypothesis: true tau is not equal to 0
## sample estimates:
##       tau 
## 0.8208065
\end{verbatim}

Contrast these results with those obtained using the Pearson product-moment correlation. Why the difference?

Test the non-parametric correlations on pairs of the transformed variables. You should immediately note that the non-parametric correlations are identical for untransformed and transformed variables. This is because we are using the ranks, rather than the raw data, and the rank ordering of the data does not change when a transformation is applied to the raw values.

Note that the correlations for Kendall's tau (0.820) are lower than for the Spearman rank (0.952) correlation. This is because Kendall's gives more weight to ranks that are far apart, whereas Spearman's weights each rank equally. Generally, Kendalls's is more appropriate when there is more uncertainty about the reliability of close ranks.

The sturgeons in this sample were collected using nets and baited hooks of a certain size. What impact do you think this method of collection had on the shapes of the distributions of \texttt{fklngth} and \texttt{rdwght} ? Under these circumstances, do you think correlation analysis is appropriate at all?

Note that correlation analysis assumes that each variable is \emph{randomly sampled}. In the case of sturgeon, this is not the case: baited hooks and nets will only catch sturgeon above a certain minimum size. Note that in the sample, there are no small sturgeons, since the fishing gear targets only larger fish. Thus, we should be very wary of the correlation coefficients associated with our analysis, as the inclusion of smaller fish may well change our estimate of these correlations.

\hypertarget{simple-linear-regression}{%
\section{Simple linear regression}\label{simple-linear-regression}}

In correlation analysis we are interested in how pairs of variables covary: However, in regression analysis, we are attempting to estimate a model that predicts a variable (the dependent variable) from another variable (the independent variable).

As with any statistical analysis, the best way to begin is by looking at your data.
If you are interested in the relationship between two variables, say, Y and X, produce a plot of Y versus X just to get a ``feel'' for the relationship.

\begin{itemize}
\tightlist
\item
  The data file \texttt{sturgeon.csv} contains data for sturgeons collected from 1978-1980 at Cumberland House, Saskatchewan and The Pas, Manitoba. Make a scatterplot of \texttt{fklngth} (the dependent variable) versus \texttt{age} (the independent variable) for males and add a linear regression and a loess smoother. What do you conclude from this plot?
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sturgeon.male \textless{}{-}}\StringTok{ }\KeywordTok{subset}\NormalTok{(sturgeon, }\DataTypeTok{subset =}\NormalTok{ sex }\OperatorTok{==}\StringTok{ "MALE"}\NormalTok{)}
\NormalTok{mygraph \textless{}{-}}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}
  \DataTypeTok{data =}\NormalTok{ sturgeon.male, }\CommentTok{\# source of data}
  \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ age, }\DataTypeTok{y =}\NormalTok{ fklngth)}
\NormalTok{) }\CommentTok{\# aesthetics: y=fklngth, x=rdwght}
\CommentTok{\# plot data points, regression, loess trace}
\NormalTok{mygraph \textless{}{-}}\StringTok{ }\NormalTok{mygraph }\OperatorTok{+}
\StringTok{  }\KeywordTok{stat\_smooth}\NormalTok{(}\DataTypeTok{method =}\NormalTok{ lm, }\DataTypeTok{se =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{color =} \StringTok{"green"}\NormalTok{) }\OperatorTok{+}\StringTok{ }\CommentTok{\# add linear regression, but no SE shading}
\StringTok{  }\KeywordTok{stat\_smooth}\NormalTok{(}\DataTypeTok{color =} \StringTok{"red"}\NormalTok{) }\OperatorTok{+}\StringTok{ }\CommentTok{\# add loess}
\StringTok{  }\KeywordTok{geom\_point}\NormalTok{() }\CommentTok{\# add data points}
\NormalTok{mygraph }\CommentTok{\# display graph}
\end{Highlighting}
\end{Shaded}

\includegraphics{Labs_BIO4158_files/figure-latex/sturlm-1-1.pdf}

This suggests that the relationship between age and fork length is not linear.

Suppose that we want to know the growth rate of male sturgeon. One estimate (perhaps not a very good one) of the growth rate is given by the slope of the fork length - age regression.

First, let's run the regression with the lm() command, and save its
results in an object called \texttt{RegModel.1}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{RegModel}\FloatTok{.1}\NormalTok{ \textless{}{-}}\StringTok{ }\KeywordTok{lm}\NormalTok{(fklngth }\OperatorTok{\textasciitilde{}}\StringTok{ }\NormalTok{age, }\DataTypeTok{data =}\NormalTok{ sturgeon.male)}
\end{Highlighting}
\end{Shaded}

Nothing appears on the screen, but don't worry, it all got saved in memory.
To see the statistical results, type:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(RegModel}\FloatTok{.1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = fklngth ~ age, data = sturgeon.male)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -8.4936 -2.2263  0.1849  1.7526 10.8234 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept) 28.50359    1.16873   24.39   <2e-16 ***
## age          0.70724    0.05888   12.01   <2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 3.307 on 73 degrees of freedom
##   (5 observations deleted due to missingness)
## Multiple R-squared:  0.664,  Adjusted R-squared:  0.6594 
## F-statistic: 144.3 on 1 and 73 DF,  p-value: < 2.2e-16
\end{verbatim}

R output gives you:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{Call:} A friendly reminder of the model fitted and the data used.
\item
  \texttt{Residuals:} General statistics about the residuals around the fitted model.
\item
  \texttt{Coefficients:} Fitted model parameter estimates, standard errors, t values and associated probabilities.
\item
  \texttt{Residual\ standard\ error:} Square root of the residual variance.
\item
  \texttt{Multiple\ R-squared:} Coefficient of determination. It corresponds to the proportion of the total variance of the dependent variable that is accounted for by the regression (i.e.~by the independent variable)
\item
  \texttt{Adjusted\ R-squared:} The adjusted R-squared accounts for the number of parameters in the model. If you want to compare the performance of several models with different numbers of parameters, this is the one to use
\item
  \texttt{F-statistic:} This is the test of the overall significance of the model. In the simple regression case, this is the same as the test of the slope of the
  regression.
\end{enumerate}

The estimated regression equation is therefore:

\[ Fklngth = 28.50359 + 0.70724 * age\]

Given the highly significant F-value of the model (or equivalently the highly significant t-value for the slope of the line), we reject the null hypothesis that there is no relationship between fork length and age.

\hypertarget{testing-regression-assumptions}{%
\subsection{Testing regression assumptions}\label{testing-regression-assumptions}}

Simple model I regression makes four assumptions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  the X variable is measured without error;
\item
  the relationship between Y and X is linear;
\item
  that for any value of X, the Y's are independently and normally distributed;
\item
  the variance of Y for fixed X is independent of X.
\end{enumerate}

Having done the regression, we can now test the assumptions.
For most biological data, the first assumption is almost never valid; usually there is error in both Y and X. This means that in general, slope estimates are biased, but predicted values are unbiased. However, so long as the error in X is small relative to the range of X in your data, the fact that X has an associated error is not likely to influence the outcome dramatically. On the other hand, if there is substantial error in X, regression results based on a model I regression may give poor estimates of the functional relationship between Y and X. In this case, more sophisticated regression procedures must be employed which are, unfortunately, beyond the scope of this course.

The other assumptions of a model I regression can, however, be tested, or at least evaluated visually. The \texttt{plot()} command can display diagnostics for linear models.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{), }\DataTypeTok{las =} \DecValTok{1}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(RegModel}\FloatTok{.1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The \texttt{par()} command is used here to tell R to display 2 rows and 2 columns of graphs per page (there are 4 diagnostic graphs for linear models generated automatically), and the last statement is to tell R to rotate the labels of the Y axis so that they are perpendicular to the Y axis. (Yes, I know, this is not at all obvious.)

You will get:
\includegraphics{Labs_BIO4158_files/figure-latex/sturlmv-1eval-1.pdf}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Upper left} tell you about linearity, normality, and homoscedasticity of the residuals. It shows the deviations around the regression vs the predicted values. Remember that the scatterplot ( fklngth vs age ) suggested that the relationship between fork length and age is not linear. Very young and very old sturgeons tended to fall under the line, and fish of average age tended to be a bit above the line. This is exactly what the residual vs fitted plot shows. The red line is a lowess trace through these data. If the relationship was linear, it would be approximately flat and close to 0. The scatter of residuals tells you a bit about their normality and homoscedasticity, although this graph is not the best way to look at these properties. The next two are better.
\item
  \textbf{Upper right} is to assess the normality of the residuals. It is a QQ plot of the residuals . If the residuals were normally distributed, they would fall very close to the diagonal line. Here, we see it is mostly the case, except in the tails
\item
  \textbf{Bottom left} titled Scale-Location, helps with assessing homoscedasticity. It plots the square root of the absolute value of the standardized residual (residual divided by the standard error of the residuals, this scales the residuals so that their variance is 1 ) as a function of the fitted value. This graph can help you visualize whether the spread of the residuals is constant or not. If residuals are homoscedastic, then the average will not change with increasing fitted values. Here, there is slight variability, but it is not monotonous (i.e.~it does not increase or decrease systematically) and there is no strong evidence against the assumption of homoscedasticity.
\item
  \textbf{Bottom right} plots the residuals as a function of leverage and can help detecting the presence of outliers or points that have a very strong influence on the regression results. The leverage of a point measures how far it is from the other points, but only with respect to the independent variable. In the case of simple linear regression, it is a function of the difference between the observation and the mean of the independent variable. You should look more closely at any observation with a leverage value that is greater than: \(2(k+1)/n\), where \$\(k\) is the number of independent variables (here 1), and \(n\) is the number of observations. In this case there is 1 independent variable, 75 observations, and points with a leverage higher than 0.053 may warrant particular scrutiny. The plot also gives you information about how the removal of a point from the data set would change the predictions. This is measured by the Cook's distance, illustrated by the red lines on the plot. A data point with a Cook distance larger than 1 has a large influence.
\end{enumerate}

\begin{rmdwarning}
Note that R automatically labels the most extreme cases on each of these 4 plots. It does not mean that these cases are outliers, or that you necessarily need be concerned with them. In any data set, there will always be a minimum and a maximum residual.
\end{rmdwarning}

The R package \texttt{performance} offers a new and updated version of those graphs with colours and more plots to help visually assess the assumptions with the function \texttt{model\_check()}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{check\_model}\NormalTok{(RegModel}\FloatTok{.1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Loading required namespace: qqplotr
\end{verbatim}

\includegraphics{Labs_BIO4158_files/figure-latex/unnamed-chunk-29-1.pdf}

So, what is the verdict about the linear regression between \texttt{fklngth} and \texttt{age} ? It fails the linearity, possibly fails the normality, passes homoscedasticity, and this does not seem to be too strongly affected by some bizarre points.

\hypertarget{formal-tests-of-regression-assumptions}{%
\subsection{Formal tests of regression assumptions}\label{formal-tests-of-regression-assumptions}}

In my practice, I seldom use formal tests of regression assumptions and mostly rely on graphs of the residuals to guide my decisions. To my knowledge, this is what most biologists and data analysts do. However, in my early analyst life I was not always confident that I was interpreting these graphs correctly and wished that I had a formal test or a statistic quantifying the degree of deviation from the regression assumptions.

The \texttt{lmtest} R package, not part of the base R installation, but available from CRAN, contains a number of tests for linearity and homoscedasticity. And one can test for normality using the Shapiro-Wilk test seen previously.

First, you need to load (and maybe install) the \texttt{lmtest} package.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(lmtest)}
\end{Highlighting}
\end{Shaded}

\begin{rmdcode}
Run the following commands
\end{rmdcode}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{bptest}\NormalTok{(RegModel}\FloatTok{.1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  studentized Breusch-Pagan test
## 
## data:  RegModel.1
## BP = 1.1765, df = 1, p-value = 0.2781
\end{verbatim}

The Breusch-Pagan test examines whether the variability of the residuals is constant with respect to increasing fitted values. A low p value is indicative of heteroscedasticity. Here, the p value is high, and supports my visual assessment that the homoscedasticity assumption is met by these data.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dwtest}\NormalTok{(RegModel}\FloatTok{.1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Durbin-Watson test
## 
## data:  RegModel.1
## DW = 2.242, p-value = 0.8489
## alternative hypothesis: true autocorrelation is greater than 0
\end{verbatim}

The Durbin-Watson test can detect serial autocorrelation in the residuals. Under the assumption of no autocorrelation, the D statistic is 2. This test can detect violation of independence of observations (residuals), although it is not foolproof. Here there is no problem identified.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{resettest}\NormalTok{(RegModel}\FloatTok{.1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  RESET test
## 
## data:  RegModel.1
## RESET = 14.544, df1 = 2, df2 = 71, p-value = 5.082e-06
\end{verbatim}

The RESET test is a test of the assumption of linearity. If the linearity assumption is met, the RESET statistic will be close to 1. Here, the statistic is much larger (14.54), and very highly significant. This confirms our visual assessment that the relationship is not linear.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{shapiro.test}\NormalTok{(}\KeywordTok{residuals}\NormalTok{(RegModel}\FloatTok{.1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Shapiro-Wilk normality test
## 
## data:  residuals(RegModel.1)
## W = 0.98037, p-value = 0.2961
\end{verbatim}

The Shapiro-Wilk normality test on the residual confirms that
the deviation from normality of the residuals is not large.

\hypertarget{data-transformations-in-regression}{%
\section{Data transformations in regression}\label{data-transformations-in-regression}}

The analysis above revealed that the linearity assumption underlying regression analysis is not met by the \texttt{fklngth} - \texttt{age} data. If we want to use regression analysis, data transformations are required:

Let's plot the log-transformed data

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{), }\DataTypeTok{las =} \DecValTok{1}\NormalTok{)}
\KeywordTok{ggplot}\NormalTok{(}
  \DataTypeTok{data =}\NormalTok{ sturgeon.male,}
  \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{log10}\NormalTok{(age), }\DataTypeTok{y =} \KeywordTok{log10}\NormalTok{(fklngth))}
\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_smooth}\NormalTok{(}\DataTypeTok{color =} \StringTok{"red"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_smooth}\NormalTok{(}\DataTypeTok{method =} \StringTok{"lm"}\NormalTok{, }\DataTypeTok{se =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{color =} \StringTok{"green"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{Labs_BIO4158_files/figure-latex/sturtr-1-1.pdf}

We can fit the linear regression model on the log-transformed variables.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{RegModel}\FloatTok{.2}\NormalTok{ \textless{}{-}}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\KeywordTok{log10}\NormalTok{(fklngth) }\OperatorTok{\textasciitilde{}}\StringTok{ }\KeywordTok{log10}\NormalTok{(age), }\DataTypeTok{data =}\NormalTok{ sturgeon.male)}
\KeywordTok{summary}\NormalTok{(RegModel}\FloatTok{.2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = log10(fklngth) ~ log10(age), data = sturgeon.male)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.082794 -0.016837 -0.000719  0.021102  0.087446 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept)  1.19199    0.02723   43.77   <2e-16 ***
## log10(age)   0.34086    0.02168   15.72   <2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.03015 on 73 degrees of freedom
##   (5 observations deleted due to missingness)
## Multiple R-squared:  0.772,  Adjusted R-squared:  0.7688 
## F-statistic: 247.1 on 1 and 73 DF,  p-value: < 2.2e-16
\end{verbatim}

Note that by using the log transformed data, the proportion of variation explained by the regression has increased by 10\% (from 0.664 to 0.772), a substantial increase. So the relationship has become more linear. Good. Let's look at the residual diagnostic plots:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{), }\DataTypeTok{las =} \DecValTok{1}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(RegModel}\FloatTok{.2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Labs_BIO4158_files/figure-latex/sturtr-3-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{check\_model}\NormalTok{(RegModel}\FloatTok{.2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Labs_BIO4158_files/figure-latex/sturtr-3-2.pdf}

So things appear a little better than before, although still not ideal. For example, the Residual vs fitted plot still suggests a potential nonlinearity. The QQ plot is nicer than before, indicating that residuals are more normally distributed after the log-log transformation. There is no indication of heteroscedasticity. And, although there are still a few points with somewhat high leverage, none have a Cook's distance above 0.5. It thus seems that transforming data improved things: more linear, more normal, less dependence on extreme data. Do the formal tests support this visual assessment?

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{bptest}\NormalTok{(RegModel}\FloatTok{.2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  studentized Breusch-Pagan test
## 
## data:  RegModel.2
## BP = 0.14282, df = 1, p-value = 0.7055
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dwtest}\NormalTok{(RegModel}\FloatTok{.2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Durbin-Watson test
## 
## data:  RegModel.2
## DW = 2.1777, p-value = 0.6134
## alternative hypothesis: true autocorrelation is greater than 0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{resettest}\NormalTok{(RegModel}\FloatTok{.2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  RESET test
## 
## data:  RegModel.2
## RESET = 4.4413, df1 = 2, df2 = 71, p-value = 0.01523
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{shapiro.test}\NormalTok{(}\KeywordTok{residuals}\NormalTok{(RegModel}\FloatTok{.2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Shapiro-Wilk normality test
## 
## data:  residuals(RegModel.2)
## W = 0.98998, p-value = 0.8246
\end{verbatim}

Indeed, they do: residuals are still homoscedastic (Breusch-Pagan test), show no autocorrelation (Durbin-Watson test), are normal (Shapiro-Wilk test), and they are more linear (p value of the RESET test is now 0.015, instead of 0.000005). Linearity has improved, but is still violated somewhat.

\hypertarget{dealing-with-outliers}{%
\section{Dealing with outliers}\label{dealing-with-outliers}}

In this case, there are no real clear outliers. Yes, observations 8, 24, and 112 are labeled as the most extreme in the last set of residual diagnostic plots. But they are still within what I consider the ``reasonable'' range. But how does one define a limit to the reasonable? When is an extreme value a real outlier we have to deal with? Opinions vary about the issue, but I favor conservatism.

My rule is that, unless the value is clearly impossible or an error in data entry, I do not delete ``outliers''; rather, I analyze all my data. Why? Because, I want my data to reflect natural or real variability. Indeed, variability is often what interests biologists the most.

Keeping extreme values is the fairest way to proceed, but it often creates other issues. These values will often be the main reason why the data fail to meet the assumptions of the statistical analysis. One solution is to run the analysis with and without the outliers, and compare the results. In many cases, the two analyses will be qualitatively similar: the same conclusions will be reached, and the effect size will not be very different. Sometimes, however, this comparison will reveal that the presence of the outliers changes the story. The logical conclusion then is that the results depend on the outliers and that the data at hand are not very conclusive. As an example, let's rerun the analysis after eliminating observations labeled 8, 24, and 112.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{RegModel}\FloatTok{.3}\NormalTok{ \textless{}{-}}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\KeywordTok{log10}\NormalTok{(fklngth) }\OperatorTok{\textasciitilde{}}\StringTok{ }\KeywordTok{log10}\NormalTok{(age), }\DataTypeTok{data =}\NormalTok{ sturgeon.male, }\DataTypeTok{subset =} \OperatorTok{!}\NormalTok{(}\KeywordTok{rownames}\NormalTok{(sturgeon.male) }\OperatorTok{\%in\%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"8"}\NormalTok{, }\StringTok{"24"}\NormalTok{, }\StringTok{"112"}\NormalTok{)))}
\KeywordTok{summary}\NormalTok{(RegModel}\FloatTok{.3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = log10(fklngth) ~ log10(age), data = sturgeon.male, 
##     subset = !(rownames(sturgeon.male) %in% c("8", "24", "112")))
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.069163 -0.017390  0.000986  0.018590  0.047647 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept)  1.22676    0.02431   50.46   <2e-16 ***
## log10(age)   0.31219    0.01932   16.16   <2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.02554 on 70 degrees of freedom
##   (5 observations deleted due to missingness)
## Multiple R-squared:  0.7885, Adjusted R-squared:  0.7855 
## F-statistic:   261 on 1 and 70 DF,  p-value: < 2.2e-16
\end{verbatim}

The intercept, slope, and R squared are about the same, and the significance of the slope is still astronomical. Removing the ``outliers'' has little effect in this case.

As for the diagnostic residual plots and the formal tests of assumptions:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(RegModel}\FloatTok{.3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Labs_BIO4158_files/figure-latex/sturtr-6-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{bptest}\NormalTok{(RegModel}\FloatTok{.3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  studentized Breusch-Pagan test
## 
## data:  RegModel.3
## BP = 0.3001, df = 1, p-value = 0.5838
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dwtest}\NormalTok{(RegModel}\FloatTok{.3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Durbin-Watson test
## 
## data:  RegModel.3
## DW = 2.0171, p-value = 0.5074
## alternative hypothesis: true autocorrelation is greater than 0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{resettest}\NormalTok{(RegModel}\FloatTok{.3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  RESET test
## 
## data:  RegModel.3
## RESET = 3.407, df1 = 2, df2 = 68, p-value = 0.0389
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{shapiro.test}\NormalTok{(}\KeywordTok{residuals}\NormalTok{(RegModel}\FloatTok{.3}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Shapiro-Wilk normality test
## 
## data:  residuals(RegModel.3)
## W = 0.98318, p-value = 0.4502
\end{verbatim}

No real difference either. Overall, this suggests that the most extreme values do not have undue influence on the results.

\hypertarget{quantifying-effect-size-in-regression-and-power-analysis}{%
\section{Quantifying effect size in regression and power analysis}\label{quantifying-effect-size-in-regression-and-power-analysis}}

Biological interpretation differs from statistical interpretation. Statistically, we conclude that size increase with age (i.e.~the slope is positive and different from 0). But this conclusion alone does not tell if the difference between young and old fish is large. The slope and the scatterplot are more informative than the p-value here. The slope (in log-log space) is 0.34.
This means that for each unit increase of X (\texttt{log10(age)}), there is an increase of 0.34 units of \texttt{log10(fklngth)}. In other words, when age is multiplied by 10, fork length is multiplied by about 2 (10\^{}0.34). Humm, length increases more slowly than age. This slope value (0.34) is an estimate of raw effect size. It measure how much length changes with age.

\hypertarget{power-to-detect-a-given-slope}{%
\subsection{Power to detect a given slope}\label{power-to-detect-a-given-slope}}

You can compute power with G*Power for some slope value that you deem of sufficient magnitude to warrant detection.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Go to \textbf{t Tests: Linear bivariate regression: One group, size of slope}.
\item
  Select \textbf{Post hoc: Compute achieved power- given \(\alpha\), sample size,and effect size}
\end{enumerate}

For example, suppose that sturgeon biologists deem that a slope of 0.1 for the relationship between \texttt{log10(fklngth)} and \texttt{log10(age)} is meaningful and you wanted to estimate the power to detect such a slope with a sample of 20 sturgeons. Results from the log-log regression contain most of what you need:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(RegModel}\FloatTok{.2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = log10(fklngth) ~ log10(age), data = sturgeon.male)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.082794 -0.016837 -0.000719  0.021102  0.087446 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept)  1.19199    0.02723   43.77   <2e-16 ***
## log10(age)   0.34086    0.02168   15.72   <2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.03015 on 73 degrees of freedom
##   (5 observations deleted due to missingness)
## Multiple R-squared:  0.772,  Adjusted R-squared:  0.7688 
## F-statistic: 247.1 on 1 and 73 DF,  p-value: < 2.2e-16
\end{verbatim}

Note the Residual standard error value (0.03015). You will need this.
The other thing you need is an estimate of the standard deviation of \texttt{log10(age)}.
R can (of course) compute it. Be careful, the \texttt{sd()} function will return \texttt{NA} if there are missing values. You can get around this by adding \texttt{na.rm=TRUE} as an argument ot the \texttt{sd()} function.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sd}\NormalTok{(}\KeywordTok{log10}\NormalTok{(sturgeon.male}\OperatorTok{$}\NormalTok{age), }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.1616675
\end{verbatim}

You can then enter these values (slope to be detected, sample size, alpha, standard deviation of the independent variable) to calculate another quantity that G*Power needs (standard deviation of y) using the Determine panel. Finally you can calculate Power. The filled panels should look like this

\begin{rmdnote}
Note: The SD of y can't just be taken from the data because if the slope
chanages (e.g.~H1) then this will change the SD of y. SD y needs to be
estimated from the observed scatter around the line and the
hypothesized slope).
\end{rmdnote}

\begin{figure}
\includegraphics[width=1\linewidth]{images/pow_lm1_en} \caption{Power analysis for age-length in sturgeon with N = 20 and slope = 0.1}\label{fig:lm-pow-fig-1}
\end{figure}

Power to detect a significant slope, if the slope is 0.1, variability of data points around the regression is like in our sample, for a sample of 20 sturgeons, with \(\alpha = 0.05\) is 0.621. Only about 2/3 of samples of that size would detect a significant effect of \texttt{age} on \texttt{fklngth}.

In R, you can do the analysis also but we will use another trick to work with the \texttt{pwr.t.test()} function.
First we, need to estimate the effect size \texttt{d}. IN this case \texttt{d} is estimated as:
\[ d = \frac{b}{s_b\sqrt{n-k-1}} \]
where \(b\) is the slope, \(s_b\) is the standard error on the slope, \(n\) is the number of observations and \(k\) is the number of independent variables (1 for simple liner regression).

SE of the slope is 0.02168. The model was fitted using 75 fishes (n=75). We can then estimate \texttt{d}.
\[ d = \frac{b}{s_b\sqrt{n-k-1}} = \frac{0.1}{0.02168\sqrt{74-1-1}}=0.54\]

We can simply use the \texttt{pwr.t.test()} function to estimate the power.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(pwr)}

\CommentTok{\# analyse de puissance}
\KeywordTok{pwr.t.test}\NormalTok{(}\DataTypeTok{n =} \DecValTok{20}\NormalTok{, }\DataTypeTok{d =} \FloatTok{0.54}\NormalTok{, }\DataTypeTok{sig.level =} \FloatTok{0.05}\NormalTok{, }\DataTypeTok{type =} \StringTok{"one.sample"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##      One-sample t test power calculation 
## 
##               n = 20
##               d = 0.54
##       sig.level = 0.05
##           power = 0.6299804
##     alternative = two.sided
\end{verbatim}

You can see that the results is really similar but not exactly the same than with G*power which is normal since we did not use the exact same formula to estimate power.

\hypertarget{sample-size-required-to-achieve-desired-power}{%
\subsection{Sample size required to achieve desired power}\label{sample-size-required-to-achieve-desired-power}}

To estimate the sample size required to achieve 99\% power to detect a slope of 0.1 (in log-log space), with alpha=0.05, you simply change the type of analysis:

\begin{figure}
\includegraphics[width=1\linewidth]{images/pow_lm2_en} \caption{A priori power analysis to estimate the sample size needed to have a power of 0.99}\label{fig:lm-pow-fig-2}
\end{figure}

In R you can simply do:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(pwr)}

\CommentTok{\# analyse de puissance}
\KeywordTok{pwr.t.test}\NormalTok{(}\DataTypeTok{power =} \FloatTok{0.99}\NormalTok{, }\DataTypeTok{d =} \FloatTok{0.54}\NormalTok{, }\DataTypeTok{sig.level =} \FloatTok{0.05}\NormalTok{, }\DataTypeTok{type =} \StringTok{"one.sample"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##      One-sample t test power calculation 
## 
##               n = 64.96719
##               d = 0.54
##       sig.level = 0.05
##           power = 0.99
##     alternative = two.sided
\end{verbatim}

By increasing sample size to 66, with the same assumptions as before, power increases to 99\%.

\hypertarget{bootstrapping-the-simple-linear-regression}{%
\section{Bootstrapping the simple linear regression}\label{bootstrapping-the-simple-linear-regression}}

A non-parametric test for the intercept and slope of a linear regression can be obtained by bootstrapping.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# load boot}
\KeywordTok{library}\NormalTok{(boot)}
\CommentTok{\# function to obtain regression weights}
\NormalTok{bs \textless{}{-}}\StringTok{ }\ControlFlowTok{function}\NormalTok{(formula, data, indices) \{}
\NormalTok{  d \textless{}{-}}\StringTok{ }\NormalTok{data[indices, ] }\CommentTok{\# allows boot to select sample}
\NormalTok{  fit \textless{}{-}}\StringTok{ }\KeywordTok{lm}\NormalTok{(formula, }\DataTypeTok{data =}\NormalTok{ d)}
  \KeywordTok{return}\NormalTok{(}\KeywordTok{coef}\NormalTok{(fit))}
\NormalTok{\}}
\CommentTok{\# bootstrapping with 1000 replications}
\NormalTok{results \textless{}{-}}\StringTok{ }\KeywordTok{boot}\NormalTok{(}
  \DataTypeTok{data =}\NormalTok{ sturgeon.male,}
  \DataTypeTok{statistic =}\NormalTok{ bs,}
  \DataTypeTok{R =} \DecValTok{1000}\NormalTok{, }\DataTypeTok{formula =} \KeywordTok{log10}\NormalTok{(fklngth) }\OperatorTok{\textasciitilde{}}\StringTok{ }\KeywordTok{log10}\NormalTok{(age)}
\NormalTok{)}
\CommentTok{\# view results}
\NormalTok{results}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## ORDINARY NONPARAMETRIC BOOTSTRAP
## 
## 
## Call:
## boot(data = sturgeon.male, statistic = bs, R = 1000, formula = log10(fklngth) ~ 
##     log10(age))
## 
## 
## Bootstrap Statistics :
##      original        bias    std. error
## t1* 1.1919926 -0.0001148875  0.03322507
## t2* 0.3408557  0.0002948147  0.02629906
\end{verbatim}

For each parameter in the model (here the intercept is labeled \texttt{t1\textbackslash{}*} and the slope of the regression line is labeled \texttt{t2\textbackslash{}*}) , you obtain:

Pour chaque param√®tre du mod√®le (ici l'ordonn√©e √† l'origine est
appel√©e t1* et la pente de la r√©gression \texttt{t2\textbackslash{}*}), R imprime :

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{original} original parameter estimate (on all non-bootstrapped data)
\item
  \texttt{bias} the difference between the mean value of all bootstrap estimates and the original value
\item
  \texttt{std.\ error} standard error of the bootstrap estimate
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(results, }\DataTypeTok{index =} \DecValTok{1}\NormalTok{) }\CommentTok{\# intercept}
\end{Highlighting}
\end{Shaded}

\includegraphics{Labs_BIO4158_files/figure-latex/lm-boot-2-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(results, }\DataTypeTok{index =} \DecValTok{2}\NormalTok{) }\CommentTok{\# log10(age)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Labs_BIO4158_files/figure-latex/lm-boot-2-2.pdf}

The distribution of the bootstrapped estimates is rather Gaussian, with only small deviations in the tails (where it counts for confidence intervals\ldots). One could use the standard error of the bootstrap estimates to calculate a symmetrical confidence interval as mean +- t SE. But, given that R can as easily calculate a bias-corrected adjusted (BCa) confidence interval, or one based on the actual distribution, (Percentile) why not have it do it all:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# interval de confiance pour l\textquotesingle{}ordonn√©e √† l\textquotesingle{}origine}
\KeywordTok{boot.ci}\NormalTok{(results, }\DataTypeTok{type =} \StringTok{"all"}\NormalTok{, }\DataTypeTok{index =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in boot.ci(results, type = "all", index = 1): bootstrap variances needed
## for studentized intervals
\end{verbatim}

\begin{verbatim}
## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS
## Based on 1000 bootstrap replicates
## 
## CALL : 
## boot.ci(boot.out = results, type = "all", index = 1)
## 
## Intervals : 
## Level      Normal              Basic         
## 95%   ( 1.127,  1.257 )   ( 1.129,  1.256 )  
## 
## Level     Percentile            BCa          
## 95%   ( 1.128,  1.255 )   ( 1.116,  1.248 )  
## Calculations and Intervals on Original Scale
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# intervalle de confiance pour la pente}
\KeywordTok{boot.ci}\NormalTok{(results, }\DataTypeTok{type =} \StringTok{"all"}\NormalTok{, }\DataTypeTok{index =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in boot.ci(results, type = "all", index = 2): bootstrap variances needed
## for studentized intervals
\end{verbatim}

\begin{verbatim}
## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS
## Based on 1000 bootstrap replicates
## 
## CALL : 
## boot.ci(boot.out = results, type = "all", index = 2)
## 
## Intervals : 
## Level      Normal              Basic         
## 95%   ( 0.2890,  0.3921 )   ( 0.2878,  0.3908 )  
## 
## Level     Percentile            BCa          
## 95%   ( 0.2909,  0.3939 )   ( 0.2958,  0.3980 )  
## Calculations and Intervals on Original Scale
\end{verbatim}

Here the 4 types of CI that R managed to calculate are essentially the same. Had data been violating more strongly the standard assumptions (normality, homoscedasticity), then the different methods (Normal, Basic, Percentile, and BCa) would have diverged more. In that case, which one is best? BCa has the favor of most, currently.

\hypertarget{two---sample-comparisons}{%
\chapter{Two - sample comparisons}\label{two---sample-comparisons}}

After completing this laboratory exercise, you should be able to:

\begin{itemize}
\tightlist
\item
  Use R to visually examine data.
\item
  Use R to compare the means of two normally distributed samples.
\item
  Use R to compare the means of two non-normally distributed
  samples.
\item
  Use R to compare the means of two paired samples
\end{itemize}

\hypertarget{set-ttest}{%
\section{R packages and data}\label{set-ttest}}

For this la b you need:

\begin{itemize}
\tightlist
\item
  R packages:

  \begin{itemize}
  \tightlist
  \item
    car
  \item
    lmtest
  \item
    boot
  \item
    pwr
  \item
    ggplot2
  \item
    performance
  \end{itemize}
\item
  data:

  \begin{itemize}
  \tightlist
  \item
    sturgeon.csv
  \item
    skulldat\_2020.csv
  \end{itemize}
\end{itemize}

You need to load the packages in R with \texttt{library()} and if need needed install them first with \texttt{install.packages()}
For the data, load them using the \texttt{read.csv()} function.

\hypertarget{visual-examination-of-sample-data}{%
\section{Visual examination of sample data}\label{visual-examination-of-sample-data}}

One of the first steps in any type of data analysis is to visualize your data with plots and summary statistics, to get an idea of underlying distributions, possible outliers, and trends in your data. This often begins with plots of the data, such as histograms, probability plots, and box plots, that allow you to get a feel for whether your data are normally distributed, whether they are correlated one to the other, or whether there are any suspicious looking points that may lead you to go back to the original data file to check for errors.

Suppose we want to test the null hypothesis that the size, as indexed by fork length ( \texttt{fklngth} in file \texttt{sturgeon.csv} - the length, in cm, from the tip of the nose to the base of the fork in the caudal fin), of sturgeon at The Pas and Cumberland House is the same. To begin, we have a look at the underlying distributions of the sample data to get a feel for whether the data are normally distributed in each sample. We will not actually test for normality at this point; the assumption of normality in parametric analyses refers always to the residuals and not the raw data themselves. However, if the raw data are non-normally distributed, then you usually have good reason to suspect that the residuals also will be non-normally distributed.

An excellent way to visually compare a data distribution to a normal distribution is to superimpose a histogram of the data and a normal curve. To do so, we must proceed in two steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  tell R that we want to make a histogram with a density curve superimposed
\item
  tell R that we want this to be done for both locations.
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Using the data file \texttt{sturgeon.csv} , generate histograms for \texttt{fklngth} data at The Pas and Cumberland House.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# use "sturgeon" dataframe to make plot called mygraph}
\CommentTok{\# and define x axis as representing fklngth}
\NormalTok{mygraph \textless{}{-}}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}
  \DataTypeTok{data =}\NormalTok{ sturgeon,}
  \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ fklngth)}
\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{"Fork length (cm)"}\NormalTok{)}
\CommentTok{\# add data to the mygraph ggplot}
\NormalTok{mygraph \textless{}{-}}\StringTok{ }\NormalTok{mygraph }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_density}\NormalTok{() }\OperatorTok{+}\StringTok{ }\CommentTok{\# add data density smooth}
\StringTok{  }\KeywordTok{geom\_rug}\NormalTok{() }\OperatorTok{+}\StringTok{ }\CommentTok{\# add rug (bars at the bottom of the plot)}
\StringTok{  }\KeywordTok{geom\_histogram}\NormalTok{( }\CommentTok{\# add black semitransparent histogram}
    \KeywordTok{aes}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ ..density..),}
    \DataTypeTok{color =} \StringTok{"black"}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.3}
\NormalTok{  ) }\OperatorTok{+}
\StringTok{  }\CommentTok{\# add normal curve in red, with mean and sd from fklength}
\StringTok{  }\KeywordTok{stat\_function}\NormalTok{(}
    \DataTypeTok{fun =}\NormalTok{ dnorm,}
    \DataTypeTok{args =} \KeywordTok{list}\NormalTok{(}
      \DataTypeTok{mean =} \KeywordTok{mean}\NormalTok{(sturgeon}\OperatorTok{$}\NormalTok{fklngth),}
      \DataTypeTok{sd =} \KeywordTok{sd}\NormalTok{(sturgeon}\OperatorTok{$}\NormalTok{fklngth)}
\NormalTok{    ),}
    \DataTypeTok{color =} \StringTok{"red"}
\NormalTok{  )}
\CommentTok{\# display graph, by location}
\NormalTok{mygraph }\OperatorTok{+}\StringTok{ }\KeywordTok{facet\_grid}\NormalTok{(. }\OperatorTok{\textasciitilde{}}\StringTok{ }\NormalTok{location)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{Labs_BIO4158_files/figure-latex/t-1-1.pdf}
\caption{\label{fig:t-1}Distribution of sturgeon length at 2 locations}
\end{figure}

Based on your visual inspection, are the two samples normally distributed?
Visual inspection of these plots suggests that this variable is approximately normally distributed in each sample.

Since we are interested in finding out if mean fish size differs among the two locations, it is probably also a good idea to generate a graph that compares the two groups of data. A box plot works well for this.

\begin{itemize}
\tightlist
\item
  Generate a box plot of \texttt{fklngth} grouped by \texttt{location} . What do you conclude about differences in size among the two locations?
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ sturgeon, }\KeywordTok{aes}\NormalTok{(}
  \DataTypeTok{x =}\NormalTok{ location,}
  \DataTypeTok{y =}\NormalTok{ fklngth}
\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_boxplot}\NormalTok{(}\DataTypeTok{notch =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{Labs_BIO4158_files/figure-latex/t-2-1.pdf}
\caption{\label{fig:t-2}Boxplot of sturgeon legnth at 2 locations}
\end{figure}

It would appear as though there are not big differences in fish size among the two locations, although fish size at The Pas looks to be more variable, with a bigger range in size and outliers (defined as values \textgreater{} 1.5 * inter-quartile range) at both ends of the distribution.

\hypertarget{comparing-means-of-two-independent-samples-parametric-and-non-parametric-comparisons}{%
\section{Comparing means of two independent samples: parametric and non-parametric comparisons}\label{comparing-means-of-two-independent-samples-parametric-and-non-parametric-comparisons}}

Test the null hypothesis that the mean fklngth of The Pas and Cumberland House samples are the same. Using 3 different tests:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  parametric test with equal variances
\item
  parametric test with unequal variances
\item
  non-parametric test
\end{enumerate}

What do you conclude?

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# t{-}test assuming equal variances}
\KeywordTok{t.test}\NormalTok{(}
\NormalTok{  fklngth }\OperatorTok{\textasciitilde{}}\StringTok{ }\NormalTok{location,}
  \DataTypeTok{data =}\NormalTok{ sturgeon,}
  \DataTypeTok{alternative =} \StringTok{"two.sided"}\NormalTok{,}
  \DataTypeTok{var.equal =} \OtherTok{TRUE}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Two Sample t-test
## 
## data:  fklngth by location
## t = 2.1359, df = 184, p-value = 0.03401
## alternative hypothesis: true difference in means between group CUMBERLAND and group THE_PAS is not equal to 0
## 95 percent confidence interval:
##  0.1308307 3.2982615
## sample estimates:
## mean in group CUMBERLAND    mean in group THE_PAS 
##                 45.08439                 43.36984
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# t{-}test assuming unequal variances}
\KeywordTok{t.test}\NormalTok{(}
\NormalTok{  fklngth }\OperatorTok{\textasciitilde{}}\StringTok{ }\NormalTok{location,}
  \DataTypeTok{data =}\NormalTok{ sturgeon,}
  \DataTypeTok{alternative =} \StringTok{"two.sided"}\NormalTok{,}
  \DataTypeTok{var.equal =} \OtherTok{FALSE}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Welch Two Sample t-test
## 
## data:  fklngth by location
## t = 2.2201, df = 169.8, p-value = 0.02774
## alternative hypothesis: true difference in means between group CUMBERLAND and group THE_PAS is not equal to 0
## 95 percent confidence interval:
##  0.1900117 3.2390804
## sample estimates:
## mean in group CUMBERLAND    mean in group THE_PAS 
##                 45.08439                 43.36984
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# test non param√©trique}
\KeywordTok{wilcox.test}\NormalTok{(}
\NormalTok{  fklngth }\OperatorTok{\textasciitilde{}}\StringTok{ }\NormalTok{location,}
  \DataTypeTok{data =}\NormalTok{ sturgeon,}
  \DataTypeTok{alternative =} \StringTok{"two.sided"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Wilcoxon rank sum test with continuity correction
## 
## data:  fklngth by location
## W = 4973, p-value = 0.06296
## alternative hypothesis: true location shift is not equal to 0
\end{verbatim}

Based on the \emph{t-test}, we would reject the null hypothesis, \emph{i.e.} there is a significant (but not highly significant) difference in mean fork length between the two populations.

Note that using the Wilcoxon rank sum test, we do not reject the null hypothesis. The two different tests therefore give us two different results. The significant difference obtained using the t-test may, at least in part, be due to deviations from normality or homoscedasticity; on the other hand, the non-significant difference obtained using the U -statistic may be due to the fact that for fixed sample size, the power of a non- parametric test is lower than the corresponding parametric test. Given the p values obtained from both tests, and the fact that for samples of this size (84 and 101), the t-test is comparatively robust with respect to non-normality, I would be inclined to reject the null hypothesis. In practice to avoid P-hacking, you should decide which test is appropriate first and then apply and interpret it, or if you decide to do all you should present results of all and interpret accordingly.

Before accepting the results of the parametric t-test and rejecting the null hypothesis that there is no difference in size between the two locations, one should do some sort of assessment to determine if the model fits the assumption of normally distributed residuals and equal variances. Preliminary examination of the raw data suggested the data appeared roughly normal but there might be problems with variances (since the spread of data for The\_Pas was much greater than for Cumberland). We can examine this more closely by looking at the residuals. An easy way to do so, is to fit a linear model and use the residual diagnostic plots:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m1 \textless{}{-}}\StringTok{ }\KeywordTok{lm}\NormalTok{(fklngth }\OperatorTok{\textasciitilde{}}\StringTok{ }\NormalTok{location, }\DataTypeTok{data =}\NormalTok{ sturgeon)}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(m1)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{Labs_BIO4158_files/figure-latex/t-6-1.pdf}
\caption{\label{fig:t-6}Model assumption checks}
\end{figure}

The first plot above shows the spread of the residuals around the estimated values for the two groups and allows us to get a feel for whether there are problems with the assumption of homogeneity of variances. If the variances were equal, the vertical spread of the two clusters of points should be about the same. The above plot shows that the vertical spread of the group with the smaller mean is greater than it is for the larger mean, suggesting again that there are problems with the variances. We can test this formally by examining the mean differences in the absolute value of the residuals.

The second graph above is a normal QQ plot (or probability plot) of the residuals of the model. Note that these generally fall on a straight line, suggesting there is no real problem with normality. We can do a formal test for normality on the residuals using the Shapiro-Wilk test.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{shapiro.test}\NormalTok{(}\KeywordTok{residuals}\NormalTok{(m1))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Shapiro-Wilk normality test
## 
## data:  residuals(m1)
## W = 0.97469, p-value = 0.001857
\end{verbatim}

Hummm. The test indicates that the residuals are not normal. But, given that (a) the distribution is not very far (at least visually) from normal, and that (b) the number of observations in each location is reasonably large (i.e.~\textgreater30), we do not need to be overly concerned with this violation of the normality assumption.

How about equality of variance?

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(car)}
\KeywordTok{leveneTest}\NormalTok{(m1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in leveneTest.default(y = y, group = group, ...): group coerced to
## factor.
\end{verbatim}

\begin{verbatim}
## Levene's Test for Homogeneity of Variance (center = median)
##        Df F value    Pr(>F)    
## group   1  11.514 0.0008456 ***
##       184                      
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{bptest}\NormalTok{(m1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  studentized Breusch-Pagan test
## 
## data:  m1
## BP = 8.8015, df = 1, p-value = 0.00301
\end{verbatim}

The above are the results of two tests implemented in R (in the \texttt{car} and \texttt{lmtest} packages üì¶ that can be used to test for equal variances in t-tests or linear models involving only discontinuous or categorical independent variables. \textbf{Doing the two of them is overkill}. There is not much to prefer one test over another. Levene test is possibly the
better known. It tests whether the mean of absolute values of the residuals differs among groups. The Breusch-Pagan test has the advantage of being applicable to more linear models (it can deal with regression-type continuous independent variables, at least to some extent). It tests whether the studentized (i.e.~scaled by their sd estimate) squared residuals vary with the independent variables in a linear model. In this case, both indicate that variances are unequal.

On the basis of these results, we conclude that there is evidence (albeit weak) to reject the null hypothesis of no difference in \texttt{fklngth} by \texttt{location}. We have modified the \texttt{t-test} to accommodate unequal variances, and are satisfied that the assumption of normally distributed residuals is sufficiently met. Thus, it appears that \texttt{fklngth} at Cumberland is greater than \texttt{fklngth} at The Pas.

\hypertarget{bootstrap-and-permutation-tests-to-compare-2-means}{%
\section{Bootstrap and permutation tests to compare 2 means}\label{bootstrap-and-permutation-tests-to-compare-2-means}}

\hypertarget{bootstrap}{%
\subsection{Bootstrap}\label{bootstrap}}

Bootstrap and permutation tests can be used to compare means (or other statistics) between pairs of samples. The general idea is simple, and it can be implemented in more ways than I can count. Here, I use existing tools and the fact that a comparison of means can be construed as a test of a linear model. We will be able to use similar code later on when we fit more complex (but fun!) models.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(boot)}
\end{Highlighting}
\end{Shaded}

The first section defines the function that I called bs that simply extracts coefficients from a fitted model:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# function to obtain model coefficients for each iteration}
\NormalTok{bs \textless{}{-}}\StringTok{ }\ControlFlowTok{function}\NormalTok{(formula, data, indices) \{}
\NormalTok{  d \textless{}{-}}\StringTok{ }\NormalTok{data[indices, ]}
\NormalTok{  fit \textless{}{-}}\StringTok{ }\KeywordTok{lm}\NormalTok{(formula, }\DataTypeTok{data =}\NormalTok{ d)}
  \KeywordTok{return}\NormalTok{(}\KeywordTok{coef}\NormalTok{(fit))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

The second section with the \texttt{boot()} command is where the real work is done: take data in sturgeon, bootstrap \(R = 1000\) times, each time fit the model \texttt{fklngth} vs \texttt{location}, and keep the values calculated by the \texttt{bs()} function.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# bootstrapping with 1000 replications}
\NormalTok{results \textless{}{-}}\StringTok{ }\KeywordTok{boot}\NormalTok{(}
  \DataTypeTok{data =}\NormalTok{ sturgeon, }\DataTypeTok{statistic =}\NormalTok{ bs, }\DataTypeTok{R =} \DecValTok{1000}\NormalTok{,}
  \DataTypeTok{formula =}\NormalTok{ fklngth }\OperatorTok{\textasciitilde{}}\StringTok{ }\NormalTok{location}
\NormalTok{)}
\CommentTok{\# view results}
\NormalTok{results}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## ORDINARY NONPARAMETRIC BOOTSTRAP
## 
## 
## Call:
## boot(data = sturgeon, statistic = bs, R = 1000, formula = fklngth ~ 
##     location)
## 
## 
## Bootstrap Statistics :
##      original       bias    std. error
## t1* 45.084391 -0.007566011   0.4422655
## t2* -1.714546 -0.013615039   0.7657404
\end{verbatim}

So we get the original estimates for the two coefficients in this model: the mean at the first (alphabetical) location, Cumberland, and the difference in means between Cumberland and The Pas ). It is the second parameter, the difference between means, which is of interest here.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(results, }\DataTypeTok{index =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{Labs_BIO4158_files/figure-latex/tb-3-1.pdf}
\caption{\label{fig:tb-3}Distribution of bootstrapped mean difference}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# get 95\% confidence intervals}
\KeywordTok{boot.ci}\NormalTok{(results, }\DataTypeTok{type =} \StringTok{"bca"}\NormalTok{, }\DataTypeTok{index =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS
## Based on 1000 bootstrap replicates
## 
## CALL : 
## boot.ci(boot.out = results, type = "bca", index = 2)
## 
## Intervals : 
## Level       BCa          
## 95%   (-3.154, -0.167 )  
## Calculations and Intervals on Original Scale
\end{verbatim}

The 95\% CI for the difference between the two means does not include 0. Hence, the bootstrap test indicates that the two means are not equals.

\hypertarget{permutation}{%
\subsection{Permutation}\label{permutation}}

Permutation tests for linear models can easily be done using the \texttt{lmPerm} package üì¶.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m1Perm \textless{}{-}}\StringTok{ }\KeywordTok{lmp}\NormalTok{(}
\NormalTok{  fklngth }\OperatorTok{\textasciitilde{}}\StringTok{ }\NormalTok{location,}
  \DataTypeTok{data =}\NormalTok{ sturgeon,}
  \DataTypeTok{perm =} \StringTok{"Prob"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Settings:  unique SS "
\end{verbatim}

The \texttt{lmp()} function does all the work for us. Here it is run with the option perm to control the stopping rule used. Option Prob stops the sampling when the estimated standard deviation of the p-value falls below some fraction of the estimated. It is one of many stopping rules that one could use to do permutations on a subset of all the possibilities (because it would take foreeeever to do them all, even on your fast machine).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(m1Perm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lmp(formula = fklngth ~ location, data = sturgeon, perm = "Prob")
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -18.40921  -3.75370  -0.08439   3.76598  23.48055 
## 
## Coefficients:
##           Estimate Iter Pr(Prob)  
## location1   0.8573 2214   0.0434 *
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 5.454 on 184 degrees of freedom
## Multiple R-Squared: 0.02419, Adjusted R-squared: 0.01889 
## F-statistic: 4.562 on 1 and 184 DF,  p-value: 0.03401
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{Iter} coefficient: the Prob stopping rule stopped after 2214 iterations. Note that this number will vary each time you run this snippet of code. These are random permutation results, so expect variability.
\item
  \texttt{Pr(Prob)} coefficient: The estimated probability associated to H0 is 0.0434 . The observed difference in fklngth between the two locations was larger than the permuted differences in about (1 - 0.0434= about 95.7\%) of the 2214 cases. Mind you, 2214 permutations is not a large number, so small p values can't be expected to be very precise. If it is critical that you get more precise p values, more permutations would be needed. Two parameters can be tweaked: maxIter, the maximum number of iterations (default=5000), and Ca, that stops iterations when estimated standard error of the estimated p is less than Ca*p.~Default 0.1.
\item
  \texttt{F-statistic}: The rest is the standard output for the model fitted to the data, with the standard parametric test. Here the p-value, assuming all assumptions are met, is 0.034.
\end{enumerate}

\hypertarget{comparing-the-means-of-paired-samples}{%
\section{Comparing the means of paired samples}\label{comparing-the-means-of-paired-samples}}

In some experimental designs, individuals are measured twice: common examples are the measurement of the same individual at two different times during development, or of the same individual subjected to two different experimental treatments. In these cases, the two samples are not independent (they include the same individuals), and a paired comparison must be made.

The file \texttt{skulldat\_2020.csv} shows measurements of lower face width of 15 North American girls measured at age 5 and again at age 6 years (data from Newman and Meredith, 1956).

\begin{itemize}
\tightlist
\item
  Let's first run a standard t-test comparing the face width at age 5 and 6, not taking into account that the data are not independent and that they are consecutive measurements on the same individuals.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{skull \textless{}{-}}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"data/skulldat\_2020.csv"}\NormalTok{)}
\KeywordTok{t.test}\NormalTok{(width }\OperatorTok{\textasciitilde{}}\StringTok{ }\NormalTok{age,}
  \DataTypeTok{data =}\NormalTok{ skull,}
  \DataTypeTok{alternative =} \StringTok{"two.sided"}\NormalTok{,}
  \DataTypeTok{paired =} \OtherTok{FALSE}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Welch Two Sample t-test
## 
## data:  width by age
## t = -1.7812, df = 27.93, p-value = 0.08576
## alternative hypothesis: true difference in means between group 5 and group 6 is not equal to 0
## 95 percent confidence interval:
##  -0.43002624  0.03002624
## sample estimates:
## mean in group 5 mean in group 6 
##        7.461333        7.661333
\end{verbatim}

So far, we specified the t-test using a \texttt{formula} notation as \texttt{y\ \textasciitilde{}\ x} where \texttt{y} is the variable for which we want to compare the means and \texttt{x} is a variable defining the groups. This works really well when the samples are not paired and when the data is presented in a \emph{long} format. For example the\texttt{skull} data is presented in a long format and contains 3 variables:

\begin{itemize}
\tightlist
\item
  \texttt{width}: head width for each observations
\item
  \texttt{age}: age at measurement 5 or 6
\item
  \texttt{id}: person identity
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(skull)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   width age id
## 1  7.33   5  1
## 2  7.53   6  1
## 3  7.49   5  2
## 4  7.70   6  2
## 5  7.27   5  3
## 6  7.46   6  3
\end{verbatim}

When data are paired, we need to indicate how they are paired. In the \texttt{skull}data, samples are paired by an individual identity, \texttt{id}, with mearurement taken at different ages. However, the function \texttt{t.test} does not cope well with this data structure. We need to transpose the data from a \emph{long} to a \emph{wide} format where we have a column per group, with the data of a given individual on the same line. Here is how we can do it.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{skull\_w \textless{}{-}}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{id =} \KeywordTok{unique}\NormalTok{(skull}\OperatorTok{$}\NormalTok{id))}
\NormalTok{skull\_w}\OperatorTok{$}\NormalTok{width5 \textless{}{-}}\StringTok{ }\NormalTok{skull}\OperatorTok{$}\NormalTok{width[}\KeywordTok{match}\NormalTok{(skull\_w}\OperatorTok{$}\NormalTok{id, skull}\OperatorTok{$}\NormalTok{id) }\OperatorTok{\&}\StringTok{ }\NormalTok{skull}\OperatorTok{$}\NormalTok{age }\OperatorTok{==}\StringTok{ }\DecValTok{5}\NormalTok{]}
\NormalTok{skull\_w}\OperatorTok{$}\NormalTok{width6 \textless{}{-}}\StringTok{ }\NormalTok{skull}\OperatorTok{$}\NormalTok{width[}\KeywordTok{match}\NormalTok{(skull\_w}\OperatorTok{$}\NormalTok{id, skull}\OperatorTok{$}\NormalTok{id) }\OperatorTok{\&}\StringTok{ }\NormalTok{skull}\OperatorTok{$}\NormalTok{age }\OperatorTok{==}\StringTok{ }\DecValTok{6}\NormalTok{]}
\KeywordTok{head}\NormalTok{(skull\_w)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   id width5 width6
## 1  1   7.33   7.53
## 2  2   7.49   7.70
## 3  3   7.27   7.46
## 4  4   7.93   8.21
## 5  5   7.56   7.81
## 6  6   7.81   8.01
\end{verbatim}

Now, let's run the appropriate paired t-test. What do you conclude?
Compare this with the previous result and explain any differences.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{t.test}\NormalTok{(skull\_w}\OperatorTok{$}\NormalTok{width5, skull\_w}\OperatorTok{$}\NormalTok{width6,}
  \DataTypeTok{alternative =} \StringTok{"two.sided"}\NormalTok{,}
  \DataTypeTok{paired =} \OtherTok{TRUE}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Paired t-test
## 
## data:  skull_w$width5 and skull_w$width6
## t = -19.72, df = 14, p-value = 1.301e-11
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -0.2217521 -0.1782479
## sample estimates:
## mean of the differences 
##                    -0.2
\end{verbatim}

The first analysis above assumes that the two samples of girls at age 5 and 6 are independent samples, whereas the second analysis assumes that the same girl is measured twice, once at age 5 and once at age 6 years.

Note that in the former case, we accept the null based on \(p = 0.05\), but in the latter we reject the null. In other words, the appropriate (paired sample) test shows a very significant effect of age, whereas the inappropriate one does not. The reason is because there is a strong correlation between face width at age 5 and face width at age 6:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{graphskull \textless{}{-}}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ skull\_w, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ width5, }\DataTypeTok{y =}\NormalTok{ width6)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_point}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Skull width at age 5"}\NormalTok{, }\DataTypeTok{y =} \StringTok{"Skull width at age 6"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_smooth}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale\_fill\_continuous}\NormalTok{(}\DataTypeTok{low =} \StringTok{"lavenderblush"}\NormalTok{, }\DataTypeTok{high =} \StringTok{"red"}\NormalTok{)}
\NormalTok{graphskull}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `geom_smooth()` using method = 'loess' and formula 'y ~ x'
\end{verbatim}

\begin{figure}
\centering
\includegraphics{Labs_BIO4158_files/figure-latex/tpaired-3-1.pdf}
\caption{\label{fig:tpaired-3}Relation between head width at age 5 and 6}
\end{figure}

With \emph{r} = 0.9930841. In the presence of correlation, the standard error of the pairwise difference in face width at age 5 and 6 is much smaller than the standard error of the difference between the mean face width at age 5 and 6. Thus, the associated t-statistic will be much larger for a paired sample test, \emph{i.e.} the power of the test is much greater, and the p values are smaller.

\begin{itemize}
\tightlist
\item
  Repeat the above procedure with the nonparametric alternative, the Wilcoxon signed-rank test. What do you conclude?
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{wilcox.test}\NormalTok{(skull\_w}\OperatorTok{$}\NormalTok{width5, skull\_w}\OperatorTok{$}\NormalTok{width6,}
  \DataTypeTok{alternative =} \StringTok{"two.sided"}\NormalTok{,}
  \DataTypeTok{paired =} \OtherTok{TRUE}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in wilcox.test.default(skull_w$width5, skull_w$width6, alternative =
## "two.sided", : cannot compute exact p-value with ties
\end{verbatim}

\begin{verbatim}
## 
##  Wilcoxon signed rank test with continuity correction
## 
## data:  skull_w$width5 and skull_w$width6
## V = 0, p-value = 0.0007193
## alternative hypothesis: true location shift is not equal to 0
\end{verbatim}

So, we reach the same conclusion as we did using the paired sample t- test and conclude there are significant differences in skull sizes of girls aged 5 and 6 (what a surprise!).

But, wait a minute. We have used two-tailed tests here. But, given what we know about how children grow, a one-tail hypothesis would be preferable. This can be done by changing the alternative option. One uses the alternative hypothesis to decide if it is ``less'' or greater". Here, we expect that if there is an effect (i.e the alternative hypothesis), width5 will be less than width6

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{t.test}\NormalTok{(skull\_w}\OperatorTok{$}\NormalTok{width5, skull\_w}\OperatorTok{$}\NormalTok{width6,}
  \DataTypeTok{alternative =} \StringTok{"less"}\NormalTok{,}
  \DataTypeTok{paired =} \OtherTok{TRUE}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Paired t-test
## 
## data:  skull_w$width5 and skull_w$width6
## t = -19.72, df = 14, p-value = 6.507e-12
## alternative hypothesis: true difference in means is less than 0
## 95 percent confidence interval:
##        -Inf -0.1821371
## sample estimates:
## mean of the differences 
##                    -0.2
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{wilcox.test}\NormalTok{(skull\_w}\OperatorTok{$}\NormalTok{width5, skull\_w}\OperatorTok{$}\NormalTok{width6,}
  \DataTypeTok{alternative =} \StringTok{"less"}\NormalTok{,}
  \DataTypeTok{paired =} \OtherTok{TRUE}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in wilcox.test.default(skull_w$width5, skull_w$width6, alternative =
## "less", : cannot compute exact p-value with ties
\end{verbatim}

\begin{verbatim}
## 
##  Wilcoxon signed rank test with continuity correction
## 
## data:  skull_w$width5 and skull_w$width6
## V = 0, p-value = 0.0003597
## alternative hypothesis: true location shift is less than 0
\end{verbatim}

\begin{rmdwarning}
Note that instead of rerunning the t-test specifying a one-tailed test, you can:

\begin{itemize}
\tightlist
\item
  if the sign of the estimate goes in the same direction as the alternative hypothesis, simply divide by 2 the probability you obtain with the two-tailed test
\item
  if not the sign of the estimate is in the opposite direction of the alternative hypothesis, use \(1 - p/2\)
  \end{rmdwarning}
\end{itemize}

\hypertarget{ruxe9fuxe9rences}{%
\section{R√©f√©rences}\label{ruxe9fuxe9rences}}

Bumpus, H.C. (1898) The elimination of the unfit as illustrated by the introduced sparrow, Passer domesticus. Biological Lectures, Woods Hole Biology Laboratory, Woods Hole, 11 th Lecture: 209 - 226.

Newman, K.J. and H.V. Meredith. (1956) Individual growth in skele- tal bigonial diameter during the childhood period from 5 to 11 years of age. Amer. J. Anat. 99: 157 - 187.

\hypertarget{one-way-anova}{%
\chapter{One-way ANOVA}\label{one-way-anova}}

After completing this laboratory exercise, you should be able to:

\begin{itemize}
\tightlist
\item
  Use R to do a one-way parametric ANOVA with multiple comparisons
\item
  Use R to test the validity of the parametric ANOVA assumptions
\item
  Use R to perform a one-way non-parametric ANOVA
\item
  Use R to transform your data so that the assumptions of parametric ANOVA are met.
\end{itemize}

\hypertarget{set-ano}{%
\section{R packages and data}\label{set-ano}}

For this lab you need:

\begin{itemize}
\tightlist
\item
  R packages:

  \begin{itemize}
  \tightlist
  \item
    ggplot2
  \item
    multcomp
  \item
    car
  \end{itemize}
\item
  data

  \begin{itemize}
  \tightlist
  \item
    dam10dat.csv
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ggplot2)}
\KeywordTok{library}\NormalTok{(car)}
\KeywordTok{library}\NormalTok{(multcomp)}
\end{Highlighting}
\end{Shaded}

\hypertarget{one-way-anova-with-multiple-comparisons}{%
\section{One-way ANOVA with multiple comparisons}\label{one-way-anova-with-multiple-comparisons}}

The one-way ANOVA is the multi-group analog of the \emph{t}-test, which is used to compare two groups/levels. It makes essentially the same assumptions, and in the case of two groups/levels, is in fact mathematically equivalent to the \emph{t}-test.

In 1960-1962, the Grand Rapids Dam was built on the Saskatchewan River upstream of Cumberland House. There are anecdotal reports that during dam construction, a number of large sturgeon were stranded and died in shallow pools. Surveys of sturgeon were carried out in 1954, 1958, 1965 and 1966 with fork length (\texttt{fklngth}) and round weight (\texttt{rdwght}) being recorded (not necessarily both measurements for each individual). These data are in the data file \texttt{Dam10dat.csv}.

\hypertarget{visualiser-les-donnuxe9es}{%
\subsection{Visualiser les donn√©es}\label{visualiser-les-donnuxe9es}}

\begin{itemize}
\tightlist
\item
  Using \texttt{Dam10dat.csv}, you must first change the data type of the
  numerical variable year , so that R recognizes that we wish to treat this
  variable as a factor variable and not a continuous variable.
\end{itemize}

\begin{verbatim}
## 'data.frame':    118 obs. of  21 variables:
##  $ year    : Factor w/ 4 levels "1954","1958",..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ fklngth : num  45 50 39 46 54.5 49 42.5 49 56 54 ...
##  $ totlngth: num  49 NA 43 50.5 NA 51.7 45.5 52 60.2 58.5 ...
##  $ drlngth : logi  NA NA NA NA NA NA ...
##  $ drwght  : num  16 20.5 10 17.5 19.7 21.3 9.5 23.7 31 27.3 ...
##  $ rdwght  : num  24.5 33 15.5 28.5 32.5 35.5 15.3 40.5 51.5 43 ...
##  $ sex     : int  1 1 1 2 1 2 1 1 1 1 ...
##  $ age     : int  24 33 17 31 37 44 23 34 33 47 ...
##  $ lfkl    : num  1.65 1.7 1.59 1.66 1.74 ...
##  $ ltotl   : num  1.69 NA 1.63 1.7 NA ...
##  $ ldrl    : logi  NA NA NA NA NA NA ...
##  $ ldrwght : num  1.2 1.31 1 1.24 1.29 ...
##  $ lrdwght : num  1.39 1.52 1.19 1.45 1.51 ...
##  $ lage    : num  1.38 1.52 1.23 1.49 1.57 ...
##  $ rage    : int  4 6 3 6 7 7 4 6 6 7 ...
##  $ ryear   : int  1954 1954 1954 1954 1954 1954 1954 1954 1954 1954 ...
##  $ ryear2  : int  1958 1958 1958 1958 1958 1958 1958 1958 1958 1958 ...
##  $ ryear3  : int  1966 1966 1966 1966 1966 1966 1966 1966 1966 1966 ...
##  $ location: int  1 1 1 1 1 1 1 1 1 1 ...
##  $ girth   : logi  NA NA NA NA NA NA ...
##  $ lgirth  : logi  NA NA NA NA NA NA ...
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Next, have a look at the fklngth data, just as we did in the last lab for t-tests. Create a histogram with density line grouped by year to get a feel for what's happening with your data and a boxplot of length per year. What can you say about these data?
\end{itemize}

\begin{figure}
\centering
\includegraphics{Labs_BIO4158_files/figure-latex/ano-2-1.pdf}
\caption{\label{fig:ano-2}Distribution of sturgeon length per year}
\end{figure}

\begin{figure}
\centering
\includegraphics{Labs_BIO4158_files/figure-latex/unnamed-chunk-38-1.pdf}
\caption{\label{fig:unnamed-chunk-38}Boxplot of sturgeon length per year}
\end{figure}

It appears as though there may have been a small drop in \texttt{fklngth} after the construction of the dam, but the data are variable and the effects are not clear. There might also be some problems with normality in the 1954 and 1966 samples, and it looks as though there are outliers in the 1958 and 1966 samples. Let's proceed with testing the assumptions of the ANOVA by running the analysis and looking at the residuals.

\hypertarget{testing-the-assumptions-of-a-parametric-anova}{%
\subsection{Testing the assumptions of a parametric ANOVA}\label{testing-the-assumptions-of-a-parametric-anova}}

Parametric one-way ANOVAs have three major assumptions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  the residuals are normally distributed
\item
  the error variance is the same for all groups (homoscedasticity)
\item
  the residuals are independent.
\end{enumerate}

These assumptions must be tested before we can accept the results of any parametric ANOVA.

\begin{itemize}
\tightlist
\item
  Carry out a one-way ANOVA on fklngth by year and produce the residual diagnostic plots
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Fit anova model and plot residual diagnostics}
\NormalTok{anova.model1 \textless{}{-}}\StringTok{ }\KeywordTok{lm}\NormalTok{(fklngth }\OperatorTok{\textasciitilde{}}\StringTok{ }\NormalTok{year, }\DataTypeTok{data =}\NormalTok{ dam10dat)}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(anova.model1)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{Labs_BIO4158_files/figure-latex/ano-3-1.pdf}
\caption{\label{fig:ano-3}Diagnostic plots for a one-way ANOVA}
\end{figure}

\begin{rmdwarning}
Double check that the independent variable is a \textbf{factor}. If the dependent variable is a \textbf{character}, then you will obtain only 3 graphs and an error message like:

`hat values (leverages) are all = 0.1

and there are no factor predictors; no plot no. 5`
\end{rmdwarning}

D'apr√®s les graphiques, on peut douter de la normalit√© et de l'homog√©n√©it√© des variances. Judging from the plots, it looks as though there may be problems with both normality and variance heterogeneity. Note that there is one point (case 59) with large expected values and a large residual that appear to lie well off the line: this is the outlier we noted earlier. This point might be expected to inflate the variance for the group it belongs to. Formal tests may also provide some insight as to whether we should be concerned about normality and variance heterogeneity.

\begin{itemize}
\tightlist
\item
  Perform a normality test on the residuals from the ANOVA.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{shapiro.test}\NormalTok{(}\KeywordTok{residuals}\NormalTok{(anova.model1))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Shapiro-Wilk normality test
## 
## data:  residuals(anova.model1)
## W = 0.91571, p-value = 1.63e-06
\end{verbatim}

This test confirms our suspicions from the probability plot: the residuals are not normally distributed. Recall, however, that the power here is high, so only small deviations from normality are required to reject the null.

\begin{itemize}
\tightlist
\item
  Next, test for homoscedasticity:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{leveneTest}\NormalTok{(fklngth }\OperatorTok{\textasciitilde{}}\StringTok{ }\NormalTok{year, }\DataTypeTok{data =}\NormalTok{ dam10dat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Levene's Test for Homogeneity of Variance (center = median)
##        Df F value  Pr(>F)  
## group   3  2.8159 0.04234 *
##       114                  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

The probability value tells you that you can reject the null hypothesis that there is no difference in variances among years. Thus, we conclude there is evidence that the variances in the groups are not equal.

\hypertarget{performing-the-anova}{%
\subsection{Performing the ANOVA}\label{performing-the-anova}}

Let's look at the results of the ANOVA, assuming for the moment that assumptions are met well enough.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(anova.model1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = fklngth ~ year, data = dam10dat)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -11.2116  -2.6866  -0.7116   2.2103  26.7885 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept)  48.0243     0.8566  56.061  < 2e-16 ***
## year1958      0.1872     1.3335   0.140  0.88859    
## year1965     -5.5077     1.7310  -3.182  0.00189 ** 
## year1966     -3.3127     1.1684  -2.835  0.00542 ** 
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 5.211 on 114 degrees of freedom
## Multiple R-squared:  0.1355, Adjusted R-squared:  0.1128 
## F-statistic: 5.957 on 3 and 114 DF,  p-value: 0.0008246
\end{verbatim}

\begin{itemize}
\tightlist
\item
  \emph{Coefficients: Estimates} Note the 4 coefficients printed. They can be used to obtain the predicted values for the model (i.e.~the group means). The mean fklngth for the first year (1954) is 48.0243. The coefficients for the 3 other years are the difference between the mean for that year and for 1954. So, the mean for 1965 is (48.0243-5.5077=42.5166). For each estimated coefficient, there is a standard error, a t-value and associated probability (for H0 that the coefficient is 0). Note here that coefficients for 1965 and 1966 are both negative and significantly less than 0. Fish were smaller after the construction of the dam than in 1954. Take these p-values with a grain of salt: these are not corrected for multiple comparisons, and they constitute only a subset of the possible comparisons. In general, I pay little attention to this part of the output and look more at what comes next.
\item
  \emph{Residual standard error}: The square root of the variance of the residuals (observed minus fitted values) corresponds to the amount of variability that is unexplained by the models (here an estimate of how much size varied among fish, once corrected for differences among years)
\item
  \emph{Mutiple R-squared} The R-squared is the proportion of the variance of the dependent variable that can be explained by the model. Here the model explains only 13.5\% of the variability. Size differences among year are relatively small compared to the ranges of sizes that can occur within years. This corresponds well to the visual impression left by the histograms of \texttt{fklngth} per \texttt{year}
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  \emph{F-Statistic} This is the p-value for the ``omnibus'' test, the test that all means are equal. Here it is much smaller than 0.05 and hence we would reject H0 and conclude that fklngth varies among the years
\end{enumerate}

The \texttt{anova()} command produces the standard ANOVA table that contains most of the same information:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{anova}\NormalTok{(anova.model1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Analysis of Variance Table
## 
## Response: fklngth
##            Df  Sum Sq Mean Sq F value    Pr(>F)    
## year        3  485.26 161.755  5.9574 0.0008246 ***
## Residuals 114 3095.30  27.152                      
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

The total variability in fklngth sums of square is partitioned into what can be accounted for by year (485.26) and what is left unexplained as residual variability (3095.30). Year indeed explains \((485.26/(3095.30+485.26)=.1355\) or 13.55\% of the variability). The mean square of the residuals is their variance.

\hypertarget{performing-multiple-comparisons-of-means-test}{%
\subsection{Performing multiple comparisons of means test}\label{performing-multiple-comparisons-of-means-test}}

\begin{itemize}
\tightlist
\item
  The \texttt{pairwise.t.test()} function can be used to compare means and adjust (or not) probabilities for multiple comparisons by choosing one of the options for the argument \texttt{p.adj}:
\end{itemize}

Comparing all means without corrections for multiple comparisons.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pairwise.t.test}\NormalTok{(dam10dat}\OperatorTok{$}\NormalTok{fklngth, dam10dat}\OperatorTok{$}\NormalTok{year,}
  \DataTypeTok{p.adj =} \StringTok{"none"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Pairwise comparisons using t tests with pooled SD 
## 
## data:  dam10dat$fklngth and dam10dat$year 
## 
##      1954   1958   1965  
## 1958 0.8886 -      -     
## 1965 0.0019 0.0022 -     
## 1966 0.0054 0.0079 0.1996
## 
## P value adjustment method: none
\end{verbatim}

Option \texttt{"bonf"} adjusts the p-values according to the Bonferroni correction. In this case, since there are 6 p-values calculated, it amounts to simply multiplying the uncorrected p-values by 6 (unless the result is above 1, in that case the adjusted p-value is 1).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pairwise.t.test}\NormalTok{(dam10dat}\OperatorTok{$}\NormalTok{fklngth, dam10dat}\OperatorTok{$}\NormalTok{year,}
  \DataTypeTok{p.adj =} \StringTok{"bonf"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Pairwise comparisons using t tests with pooled SD 
## 
## data:  dam10dat$fklngth and dam10dat$year 
## 
##      1954  1958  1965 
## 1958 1.000 -     -    
## 1965 0.011 0.013 -    
## 1966 0.033 0.047 1.000
## 
## P value adjustment method: bonferroni
\end{verbatim}

Option \texttt{"holm"} is the sequential Bonferroni correction, where the p-values are ranked from (i=1) smallest to (N) largest. The correction factor for p-values is then \$\((N-i+1)\). Here, for example, we have N=6 pairs that are compared. The lowest uncorrected p-value is 0.0019 for 1954 vs 1965. The corrected p-value becomes \(0.0019*(6-1+1)= 0.011\). The second lowest p-value is 0.0022. The corrected p/value is therefore \(0.0022*(6-2+1)=0.011\). For the highest p-value, the correction is \((N-N+1)=1\), hence it is equal to the uncorrected probability.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pairwise.t.test}\NormalTok{(dam10dat}\OperatorTok{$}\NormalTok{fklngth, dam10dat}\OperatorTok{$}\NormalTok{year,}
  \DataTypeTok{p.adj =} \StringTok{"holm"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Pairwise comparisons using t tests with pooled SD 
## 
## data:  dam10dat$fklngth and dam10dat$year 
## 
##      1954  1958  1965 
## 1958 0.889 -     -    
## 1965 0.011 0.011 -    
## 1966 0.022 0.024 0.399
## 
## P value adjustment method: holm
\end{verbatim}

The ``fdr'' option is for controlling the false discovery rate.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pairwise.t.test}\NormalTok{(dam10dat}\OperatorTok{$}\NormalTok{fklngth, dam10dat}\OperatorTok{$}\NormalTok{year,}
  \DataTypeTok{p.adj =} \StringTok{"fdr"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Pairwise comparisons using t tests with pooled SD 
## 
## data:  dam10dat$fklngth and dam10dat$year 
## 
##      1954   1958   1965  
## 1958 0.8886 -      -     
## 1965 0.0066 0.0066 -     
## 1966 0.0108 0.0119 0.2395
## 
## P value adjustment method: fdr
\end{verbatim}

The four post-hoc tests here tell us the same thing: differences are all between two groups of years: 1954/58 and 1965/66, since all comparisons show differences between the 50's and 60's but no differences within the 50's or 60's. So, in this particular case, the conclusion is not affected by the choice of adjustment method. But in other situations, you will observe contradictory results.

Which one to choose? Unadjusted p-values are certainly suspect when there are multiple tests. On the other hand, the traditional \emph{Bonferroni} correction is very conservative, and becomes even more so when there are a large number of comparisons. Recent work suggest that the \emph{fdr} approach may be a good compromise when there are a lot of comparisons. The \emph{Tukey} method of multiple comparisons is one of the most popular and is easily performed with R (note, however, that there is a pesky bug that manifests itself when the independent variable can look like a number rather than a factor, hence the little pirouette with \texttt{paste0()} to add a letter \texttt{m} before the first digit):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dam10dat}\OperatorTok{$}\NormalTok{myyear \textless{}{-}}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(}\KeywordTok{paste0}\NormalTok{(}\StringTok{"m"}\NormalTok{, dam10dat}\OperatorTok{$}\NormalTok{year))}
\KeywordTok{TukeyHSD}\NormalTok{(}\KeywordTok{aov}\NormalTok{(fklngth }\OperatorTok{\textasciitilde{}}\StringTok{ }\NormalTok{myyear, }\DataTypeTok{data =}\NormalTok{ dam10dat))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Tukey multiple comparisons of means
##     95% family-wise confidence level
## 
## Fit: aov(formula = fklngth ~ myyear, data = dam10dat)
## 
## $myyear
##                   diff        lwr        upr     p adj
## m1958-m1954  0.1872141  -3.289570  3.6639986 0.9990071
## m1965-m1954 -5.5076577 -10.021034 -0.9942809 0.0100528
## m1966-m1954 -3.3126964  -6.359223 -0.2661701 0.0274077
## m1965-m1958 -5.6948718 -10.436304 -0.9534397 0.0116943
## m1966-m1958 -3.4999106  -6.875104 -0.1247171 0.0390011
## m1966-m1965  2.1949612  -2.240630  6.6305526 0.5710111
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(}\KeywordTok{TukeyHSD}\NormalTok{(}\KeywordTok{aov}\NormalTok{(fklngth }\OperatorTok{\textasciitilde{}}\StringTok{ }\NormalTok{myyear, }\DataTypeTok{data =}\NormalTok{ dam10dat)))}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{Labs_BIO4158_files/figure-latex/ano-13-1.pdf}
\caption{\label{fig:ano-13}Inter-annual differences in sturgeon length}
\end{figure}

The confidence intervals, corrected for multiple tests by the Tukey method, are plotted for differences among years. Unfortunately, the labels are not all printed because they would overlap, but the order is the same as in the preceding table. The \texttt{multcomp} üì¶ can produce a better plot version, but requires a bit more code:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Alternative way to compute Tukey multiple comparisons}
\CommentTok{\# set up a one{-}way ANOVA}
\NormalTok{anova\_fkl\_year \textless{}{-}}\StringTok{ }\KeywordTok{aov}\NormalTok{(fklngth }\OperatorTok{\textasciitilde{}}\StringTok{ }\NormalTok{myyear, }\DataTypeTok{data =}\NormalTok{ dam10dat)}
\CommentTok{\# set up all{-}pairs comparisons for factor \textasciigrave{}year\textquotesingle{}}

\NormalTok{meandiff \textless{}{-}}\StringTok{ }\KeywordTok{glht}\NormalTok{(anova\_fkl\_year, }\DataTypeTok{linfct =} \KeywordTok{mcp}\NormalTok{(}
  \DataTypeTok{myyear =}
    \StringTok{"Tukey"}
\NormalTok{))}
\KeywordTok{confint}\NormalTok{(meandiff)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##   Simultaneous Confidence Intervals
## 
## Multiple Comparisons of Means: Tukey Contrasts
## 
## 
## Fit: aov(formula = fklngth ~ myyear, data = dam10dat)
## 
## Quantile = 2.5918
## 95% family-wise confidence level
##  
## 
## Linear Hypotheses:
##                    Estimate lwr      upr     
## m1958 - m1954 == 0   0.1872  -3.2689   3.6433
## m1965 - m1954 == 0  -5.5077  -9.9942  -1.0212
## m1966 - m1954 == 0  -3.3127  -6.3411  -0.2843
## m1965 - m1958 == 0  -5.6949 -10.4081  -0.9817
## m1966 - m1958 == 0  -3.4999  -6.8550  -0.1448
## m1966 - m1965 == 0   2.1950  -2.2142   6.6041
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(meandiff)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{Labs_BIO4158_files/figure-latex/ano-14-1.pdf}
\caption{\label{fig:ano-14}Inter-annual differences in sturgeon length}
\end{figure}

This is better. Also useful is a plot the means and their confidence intervals with the Tukey groupings shown as letters above:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Compute and plot means and Tukey CI}
\NormalTok{means \textless{}{-}}\StringTok{ }\KeywordTok{glht}\NormalTok{(}
\NormalTok{  anova\_fkl\_year,}
  \DataTypeTok{linfct =} \KeywordTok{mcp}\NormalTok{(}\DataTypeTok{myyear =} \StringTok{"Tukey"}\NormalTok{)}
\NormalTok{)}
\NormalTok{cimeans \textless{}{-}}\StringTok{ }\KeywordTok{cld}\NormalTok{(means)}
\CommentTok{\# use sufficiently large upper margin}
\CommentTok{\# plot}
\NormalTok{old\_par \textless{}{-}}\StringTok{ }\KeywordTok{par}\NormalTok{(}\DataTypeTok{mai =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{1.25}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(cimeans)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{Labs_BIO4158_files/figure-latex/ano-15-1.pdf}
\caption{\label{fig:ano-15}Inter-annual differences in sturgeon length}
\end{figure}

Note the letters appearing on top. Years labelled with the same letter do not differ significantly.

\hypertarget{data-transformations-and-non-parametric-anova}{%
\section{Data transformations and non-parametric ANOVA}\label{data-transformations-and-non-parametric-anova}}

In the above example to examine differences in fklngth among years , we detected evidence of non-normality and variance heterogeneity. If the assumptions underlying a parametric ANOVA are not valid, there are several options:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  if sample sizes in each group are reasonably large, parametric ANOVA is reasonably robust with respect to the normality assumption, for the same reason that the t-test is, so the results are probably not too bad;
\item
  we can transform the data;
\item
  we can go the non-parametric route.
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Repeat the one-way ANOVA in the section above, but this time run the analysis on the log 10 fklngth . With this transformation, do some of the problems encountered previously disappear?
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Fit anova model on log10 of fklngth and plot residual diagnostics}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\NormalTok{anova.model2 \textless{}{-}}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\KeywordTok{log10}\NormalTok{(fklngth) }\OperatorTok{\textasciitilde{}}\StringTok{ }\NormalTok{year, }\DataTypeTok{data =}\NormalTok{ dam10dat)}
\KeywordTok{plot}\NormalTok{(anova.model2)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{Labs_BIO4158_files/figure-latex/ano-16-1.pdf}
\caption{\label{fig:ano-16}Diagnostic plots for the ANOVA of sturgeon length by year}
\end{figure}

Looking at the residuals, things look barely better than before without the log transformation. Running the Wilks-Shapiro
test for normality on the residuals, we get:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{shapiro.test}\NormalTok{(}\KeywordTok{residuals}\NormalTok{(anova.model2))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Shapiro-Wilk normality test
## 
## data:  residuals(anova.model2)
## W = 0.96199, p-value = 0.002048
\end{verbatim}

So, it would appear that we still have some problems with the assumption of normality and are just on the border line of meeting the assumption of homogeneity of variances. You have several choices here:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  try to find a different transformation to satisfy the assumptions,
\item
  assume the data are close enough to meeting the assumptions, or
\item
  perform a non-parametric ANOVA.
\end{enumerate}

\begin{itemize}
\tightlist
\item
  The most commonly used non-parametric analog of the parametric one-way ANOVA is the Kruskall-Wallis one-way ANOVA. Perform a Kruskall-Wallis one-way ANOVA of \texttt{fklngth} , and compare these results to the parametric analysis above. What do you conclude?
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{kruskal.test}\NormalTok{(fklngth }\OperatorTok{\textasciitilde{}}\StringTok{ }\NormalTok{year, }\DataTypeTok{data =}\NormalTok{ dam10dat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Kruskal-Wallis rank sum test
## 
## data:  fklngth by year
## Kruskal-Wallis chi-squared = 15.731, df = 3, p-value = 0.001288
\end{verbatim}

So, the conclusion is the same as with the parametric ANOVA: we reject the null that the mean rank is the same for each year. Thus, despite violation of one or more assumptions, the parametric analysis is telling us the same thing as the non-parametric analysis: the conclusion is, therefore, quite robust.

\hypertarget{dealing-with-outliers-1}{%
\section{Dealing with outliers}\label{dealing-with-outliers-1}}

Our preliminary analysis of the relationship between \texttt{fklngth} and \texttt{year} suggested there might be some outliers in the data. These were evident in the box plots of \texttt{fklngth} by \texttt{year} and flagged as cases 59, 23 and 87 in the residual probability plot and residual-fit plot. In general, you have to have very good reasons for removing outliers from a data set (e.g., you know there was a mistake made in the data collection/entry). However, it is often useful to know how the analysis changes if you remove the outliers from the data set.

\begin{itemize}
\tightlist
\item
  Repeat the original ANOVA of \texttt{fklngth} by \texttt{year} but work with a subset of the data without the outliers. Have any of the conclusions changed?
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{damsubset \textless{}{-}}\StringTok{ }\NormalTok{dam10dat[}\OperatorTok{{-}}\KeywordTok{c}\NormalTok{(}\DecValTok{23}\NormalTok{, }\DecValTok{59}\NormalTok{, }\DecValTok{87}\NormalTok{), ] }\CommentTok{\# removes obs 23, 59 and 87}
\NormalTok{aov\_damsubset \textless{}{-}}\StringTok{ }\KeywordTok{aov}\NormalTok{(fklngth }\OperatorTok{\textasciitilde{}}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(year), damsubset)}
\KeywordTok{summary}\NormalTok{(aov\_damsubset)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                  Df Sum Sq Mean Sq F value   Pr(>F)    
## as.factor(year)   3  367.5  122.50   6.894 0.000267 ***
## Residuals       111 1972.4   17.77                     
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{shapiro.test}\NormalTok{(}\KeywordTok{residuals}\NormalTok{(aov\_damsubset))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Shapiro-Wilk normality test
## 
## data:  residuals(aov_damsubset)
## W = 0.98533, p-value = 0.2448
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{leveneTest}\NormalTok{(fklngth }\OperatorTok{\textasciitilde{}}\StringTok{ }\NormalTok{year, damsubset)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Levene's Test for Homogeneity of Variance (center = median)
##        Df F value   Pr(>F)   
## group   3  4.6237 0.004367 **
##       111                    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

Elimination of three outliers, in this case, makes things better in terms of the normality assumption, but does not improve the
variances. Moreover, the fact that the conclusion drawn from the original ANOVA with outliers retained does not change upon their removal reinforces the fact that there is no good reason to remove the points. Instead of a Kruskall-Wallis rank-based test, a permutation test could be used.

\hypertarget{permutation-test}{%
\section{Permutation test}\label{permutation-test}}

This is an example for a more complex way of doing permutation that we used when \texttt{lmPerm} was not available.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#}
\CommentTok{\# Permutation Test for one{-}way ANOVA}
\CommentTok{\# modified from code written by David C. Howell}
\CommentTok{\# http://www.uvm.edu/\textasciitilde{}dhowell/StatPages/}
\CommentTok{\# More\_Stuff/Permutation\%20Anova/PermTestsAnova.html}
\CommentTok{\# set desired number of permutations}
\NormalTok{nreps \textless{}{-}}\StringTok{ }\DecValTok{500}
\CommentTok{\# to simplify reuse of this code, copy desired dataframe to mydata}
\NormalTok{mydata \textless{}{-}}\StringTok{ }\NormalTok{dam10dat}
\CommentTok{\# copy model formula to myformula}
\NormalTok{myformula \textless{}{-}}\StringTok{ }\KeywordTok{as.formula}\NormalTok{(}\StringTok{"fklngth \textasciitilde{} year"}\NormalTok{)}
\CommentTok{\# copy dependent variable vector to mydep}
\NormalTok{mydep \textless{}{-}}\StringTok{ }\NormalTok{mydata}\OperatorTok{$}\NormalTok{fklngth}
\CommentTok{\# copy independent variable vector to myindep}
\NormalTok{myindep \textless{}{-}}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(mydata}\OperatorTok{$}\NormalTok{year)}
\CommentTok{\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#}
\CommentTok{\# You should not need to modify code chunk below}
\CommentTok{\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#}
\CommentTok{\# Compute observed F value for original sample}
\NormalTok{mod1 \textless{}{-}}\StringTok{ }\KeywordTok{lm}\NormalTok{(myformula, }\DataTypeTok{data =}\NormalTok{ mydata) }\CommentTok{\# Standard Anova}
\NormalTok{sum\_anova \textless{}{-}}\StringTok{ }\KeywordTok{summary}\NormalTok{(}\KeywordTok{aov}\NormalTok{(mod1)) }\CommentTok{\# Save summary to variable}
\NormalTok{obs\_f \textless{}{-}}\StringTok{ }\NormalTok{sum\_anova[[}\DecValTok{1}\NormalTok{]]}\OperatorTok{$}\StringTok{"F value"}\NormalTok{[}\DecValTok{1}\NormalTok{] }\CommentTok{\# Save observed F value}
\CommentTok{\# Print standard ANOVA results}
\KeywordTok{cat}\NormalTok{(}
  \StringTok{" The standard ANOVA for these data follows "}\NormalTok{,}
  \StringTok{"}\CharTok{\textbackslash{}n}\StringTok{"}
\NormalTok{)}

\KeywordTok{print}\NormalTok{(sum\_anova, }\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\KeywordTok{cat}\NormalTok{(}\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\KeywordTok{cat}\NormalTok{(}\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\KeywordTok{print}\NormalTok{(}\StringTok{"Resampling as in Manly with unrestricted sampling of observations. "}\NormalTok{)}

\CommentTok{\# Now start resampling}
\NormalTok{boot\_f \textless{}{-}}\StringTok{ }\KeywordTok{numeric}\NormalTok{(nreps) }\CommentTok{\# initalize vector to receive permuted}
\NormalTok{values}
\NormalTok{boot\_f[}\DecValTok{1}\NormalTok{] \textless{}{-}}\StringTok{ }\NormalTok{obs\_f}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{2}\OperatorTok{:}\NormalTok{nreps) \{}
\NormalTok{  newdependent \textless{}{-}}\StringTok{ }\KeywordTok{sample}\NormalTok{(mydep, }\KeywordTok{length}\NormalTok{(mydep)) }\CommentTok{\# randomize dep}
\NormalTok{  var}
\NormalTok{  mod2 \textless{}{-}}\StringTok{ }\KeywordTok{lm}\NormalTok{(newdependent }\OperatorTok{\textasciitilde{}}\StringTok{ }\NormalTok{myindep) }\CommentTok{\# refit model}
\NormalTok{  b \textless{}{-}}\StringTok{ }\KeywordTok{summary}\NormalTok{(}\KeywordTok{aov}\NormalTok{(mod2))}
\NormalTok{  boot\_f[i] \textless{}{-}}\StringTok{ }\NormalTok{b[[}\DecValTok{1}\NormalTok{]]}\OperatorTok{$}\StringTok{"F value"}\NormalTok{[}\DecValTok{1}\NormalTok{] }\CommentTok{\# store F stats}
\NormalTok{\}}
\NormalTok{permprob \textless{}{-}}\StringTok{ }\KeywordTok{length}\NormalTok{(boot\_f[boot\_f }\OperatorTok{\textgreater{}=}\StringTok{ }\NormalTok{obs\_f]) }\OperatorTok{/}\StringTok{ }\NormalTok{nreps}
\KeywordTok{cat}\NormalTok{(}
  \StringTok{" The permutation probability value is: "}\NormalTok{, permprob,}
  \StringTok{"}\CharTok{\textbackslash{}n}\StringTok{"}
\NormalTok{)}
\CommentTok{\# end of code chunk for permutation}
\end{Highlighting}
\end{Shaded}

Version \texttt{lmPerm} du test de permutation.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\# lmPerm version of permutation test}
\KeywordTok{library}\NormalTok{(lmPerm)}
\CommentTok{\# for generality, copy desired dataframe to mydata}
\CommentTok{\# and model formula to myformula}
\NormalTok{mydata \textless{}{-}}\StringTok{ }\NormalTok{dam10dat}
\NormalTok{myformula \textless{}{-}}\StringTok{ }\KeywordTok{as.formula}\NormalTok{(}\StringTok{"fklngth \textasciitilde{} year"}\NormalTok{)}
\CommentTok{\# Fit desired model on the desired dataframe}
\NormalTok{mymodel \textless{}{-}}\StringTok{ }\KeywordTok{lm}\NormalTok{(myformula, }\DataTypeTok{data =}\NormalTok{ mydata)}
\CommentTok{\# Calculate permutation p{-}value}
\KeywordTok{anova}\NormalTok{(}\KeywordTok{lmp}\NormalTok{(myformula, }\DataTypeTok{data =}\NormalTok{ mydata, }\DataTypeTok{perm =} \StringTok{"Prob"}\NormalTok{, }\DataTypeTok{center =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{Ca =} \FloatTok{0.001}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\cleardoublepage

\hypertarget{appendix-appendix}{%
\appendix \addcontentsline{toc}{chapter}{\appendixname}}


\hypertarget{software-tools}{%
\chapter{Software Tools}\label{software-tools}}

For those who are not familiar with software packages required for using R Markdown, we give a brief introduction to the installation and maintenance of these packages.

\hypertarget{r-and-r-packages}{%
\section{R and R packages}\label{r-and-r-packages}}

R can be downloaded and installed from any CRAN (the Comprehensive R Archive Network) mirrors, e.g., \url{https://cran.rstudio.com}. Please note that there will be a few new releases of R every year, and you may want to upgrade R occasionally.

To install the \textbf{bookdown} package, you can type this in R:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"bookdown"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This installs all required R packages. You can also choose to install all optional packages as well, if you do not care too much about whether these packages will actually be used to compile your book (such as \textbf{htmlwidgets}):

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"bookdown"}\NormalTok{, }\DataTypeTok{dependencies =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

If you want to test the development version of \textbf{bookdown} on GitHub, you need to install \textbf{devtools} first:

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{if}\NormalTok{ (}\OperatorTok{!}\KeywordTok{requireNamespace}\NormalTok{(}\StringTok{"devtools"}\NormalTok{)) }\KeywordTok{install.packages}\NormalTok{(}\StringTok{"devtools"}\NormalTok{)}
\NormalTok{devtools}\OperatorTok{::}\KeywordTok{install\_github}\NormalTok{(}\StringTok{"rstudio/bookdown"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

R packages are also often constantly updated on CRAN or GitHub, so you may want to update them once in a while:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{update.packages}\NormalTok{(}\DataTypeTok{ask =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Although it is not required, the RStudio IDE can make a lot of things much easier when you work on R-related projects. The RStudio IDE can be downloaded from \url{https://www.rstudio.com}.

\hypertarget{pandoc}{%
\section{Pandoc}\label{pandoc}}

An R Markdown document (\texttt{*.Rmd}) is first compiled to Markdown (\texttt{*.md}) through the \textbf{knitr} package, and then Markdown is compiled to other output formats (such as LaTeX or HTML) through Pandoc.\index{Pandoc} This process is automated by the \textbf{rmarkdown} package. You do not need to install \textbf{knitr} or \textbf{rmarkdown} separately, because they are the required packages of \textbf{bookdown} and will be automatically installed when you install \textbf{bookdown}. However, Pandoc is not an R package, so it will not be automatically installed when you install \textbf{bookdown}. You can follow the installation instructions on the Pandoc homepage (\url{http://pandoc.org}) to install Pandoc, but if you use the RStudio IDE, you do not really need to install Pandoc separately, because RStudio includes a copy of Pandoc. The Pandoc version number can be obtained via:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rmarkdown}\OperatorTok{::}\KeywordTok{pandoc\_version}\NormalTok{()}
\CommentTok{\#\# [1] \textquotesingle{}2.9.2.1\textquotesingle{}}
\end{Highlighting}
\end{Shaded}

If you find this version too low and there are Pandoc features only in a later version, you can install the later version of Pandoc, and \textbf{rmarkdown} will call the newer version instead of its built-in version.

\hypertarget{latex}{%
\section{LaTeX}\label{latex}}

LaTeX\index{LaTeX} is required only if you want to convert your book to PDF. You may see \url{https://www.latex-project.org/get/} for more information about LaTeX and its installation, but we strongly recommend that you install the lightweight and cross-platform LaTeX distribution named \href{https://yihui.org/tinytex/}{TinyTeX} and based on TeX Live. TinyTeX can be easily installed through the R package \textbf{tinytex} (which should be automatically installed when you install \textbf{bookdown}):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tinytex}\OperatorTok{::}\KeywordTok{install\_tinytex}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

With TinyTeX, you should never see error messages like this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{! LaTeX Error: File \textasciigrave{}titling.sty\textquotesingle{} not found.}

\NormalTok{Type X to quit or \textless{}RETURN\textgreater{} to proceed,}
\NormalTok{or enter new name. (Default extension: sty)}

\NormalTok{Enter file name:}
\NormalTok{! Emergency stop.}
\NormalTok{\textless{}read *\textgreater{}}

\NormalTok{l.107 \^{}\^{}M}

\NormalTok{pandoc: Error producing PDF}
\NormalTok{Error: pandoc document conversion failed with error 43}
\NormalTok{Execution halted}
\end{Highlighting}
\end{Shaded}

The above error means you used a package that contains \texttt{titling.sty}, but it was not installed. LaTeX package names are often the same as the \texttt{*.sty} filenames, so in this case, you can try to install the \texttt{titling} package. If you use TinyTeX with R Markdown, missing LaTeX packages will be installed automatically, so you never need to worry about such problems.

LaTeX distributions and packages are also updated from time to time, and you may consider updating them especially when you run into LaTeX problems. You can find out the version of your LaTeX distribution by:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{system}\NormalTok{(}\StringTok{"pdflatex {-}{-}version"}\NormalTok{)}
\CommentTok{\#\# pdfTeX 3.14159265{-}2.6{-}1.40.21 (TeX Live 2020/Debian)}
\CommentTok{\#\# kpathsea version 6.3.2}
\CommentTok{\#\# Copyright 2020 Han The Thanh (pdfTeX) et al.}
\CommentTok{\#\# There is NO warranty.  Redistribution of this software is}
\CommentTok{\#\# covered by the terms of both the pdfTeX copyright and}
\CommentTok{\#\# the Lesser GNU General Public License.}
\CommentTok{\#\# For more information about these matters, see the file}
\CommentTok{\#\# named COPYING and the pdfTeX source.}
\CommentTok{\#\# Primary author of pdfTeX: Han The Thanh (pdfTeX) et al.}
\CommentTok{\#\# Compiled with libpng 1.6.37; using libpng 1.6.37}
\CommentTok{\#\# Compiled with zlib 1.2.11; using zlib 1.2.11}
\CommentTok{\#\# Compiled with xpdf version 4.02}
\end{Highlighting}
\end{Shaded}

To update TinyTeX, you may run:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tinytex}\OperatorTok{::}\KeywordTok{tlmgr\_update}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

From year to year, you may need to upgrade TinyTeX, too (otherwise you cannot install or update any LaTeX packages), in which case you may reinstall TinyTeX:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tinytex}\OperatorTok{::}\KeywordTok{reinstall\_tinytex}\NormalTok{()}
\end{Highlighting}
\end{Shaded}


\printindex

\end{document}
