[["multiple-regression.html", "7 Multiple regression", " 7 Multiple regression After completing this laboratory exercise, you should be able to: Use R to fit a multiple regression model, and compare the adequacy of several models using inferential and information theo- retic criteria Use R to test hypotheses about the effects of different independent variables on the dependent variable of interest. Use R to evaluate multicollinearity among (supposedly) independent variables and its effects. Use R to do curvilinear (polynomial) regression. "],["set-reg-mul.html", "7.1 R packages and data", " 7.1 R packages and data For this lab you need: R packages: ggplot2 car lmtest simpleboot boot MuMIn data files: Mregdat.csv "],["points-to-keep-in-mind.html", "7.2 Points to keep in mind", " 7.2 Points to keep in mind Multiple regression models are used in cases where there is one dependent variable and several independent, continuous variables. In many biological systems, the variable of interest may be influenced by several different factors, so that accurate description or prediction requires that several independent variables be included in the regression model. Before beginning, be aware that multiple regression takes time to learn well. Beginners should keep in mind several important points: An overall regression model may be statistically significant even if none of the individual regression coefficients in the model are (caused by multicollinearity) A multiple regression model may be “nonsignificant” even though some of the individual coefficients are “significant” (caused by overfitting) Unless “independent” variables are uncorrelated in the sample, different model selection procedures may yield different results. "],["first-look-at-the-data.html", "7.3 First look at the data", " 7.3 First look at the data The file Mregdat.Rdata contains data collected in 30 wetlands in the Ottawa-Cornwall- Kingston area. The data included are the richness (number of species) of: birds (bird , and its log transform logbird), plants (plant, logpl), mammals (mammal, logmam), herptiles (herptile, logherp) total species richness of all four groups combined (totsp, logtot) GPS coordinates of the wetland (lat , long) its area (logarea) the percentage of the wetland covered by water at all times during the year (swamp) the percentage of forested land within 1 km of the wetland (cpfor2) the density (in m/hectare) of hard-surface roads within 1 km of the wetland (thtden). We will focus on herptiles for this exercise, so we better first have a look at how this variable is distributed and correlated to the potential independent variables: mydata &lt;- read.csv(&quot;data/Mregdat.csv&quot;) scatterplotMatrix( ~ logherp + logarea + cpfor2 + thtden + swamp, regLine = TRUE, smooth = TRUE, diagonal = TRUE, data = mydata ) Figure 7.1: Matrice de rélation et densité pour la richesse spécifique des amphibiens et reptiles "],["multiple-regression-models-from-scratch.html", "7.4 Multiple regression models from scratch", " 7.4 Multiple regression models from scratch We begin the multiple regression exercise by considering a situation with one dependent variable and three (possibly) independent variables. First, we will start from scratch and build a multiple regression model based on what we know from building simple regression models. Next, we will look at automated methods of building multiple regressions models using simultaneous, forward, and backward stepwise procedures. Using the subset of the Mregdat.csv data file, regress logherp on logarea. On the basis of the regression, what do you conclude? model_loga &lt;- lm(logherp ~ logarea, data = mydata) summary(model_loga) ## ## Call: ## lm(formula = logherp ~ logarea, data = mydata) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.38082 -0.09265 0.00763 0.10409 0.46977 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.18503 0.15725 1.177 0.249996 ## logarea 0.24736 0.06536 3.784 0.000818 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1856 on 26 degrees of freedom ## (2 observations deleted due to missingness) ## Multiple R-squared: 0.3552, Adjusted R-squared: 0.3304 ## F-statistic: 14.32 on 1 and 26 DF, p-value: 0.0008185 par(mfrow = c(2, 2)) plot(model_loga) Figure 7.2: Checking model asusmptions for regression of logherp as a function of logarea It looks like there is a positive relationship between herptile species richness and wetland area: the larger the wetland, the greater the number of species. Note, however, that about 2/3 of the observed variability in species richness among wetlands is not “explained” by wetland area (R2 = 0.355). Residual analysis shows no major problems with normality, heteroscedasticity or independence of residuals. Rerun the above regression, this time replacing logarea with cpfor2 as the independent variable, such that the expression in the formula field reads: logherp ~ cpfor2 . What do you conclude? model_logcp &lt;- lm(logherp ~ cpfor2, data = mydata) summary(model_logcp) ## ## Call: ## lm(formula = logherp ~ cpfor2, data = mydata) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.49095 -0.10266 0.05881 0.16027 0.25159 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.609197 0.104233 5.845 3.68e-06 *** ## cpfor2 0.002706 0.001658 1.632 0.115 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2202 on 26 degrees of freedom ## (2 observations deleted due to missingness) ## Multiple R-squared: 0.09289, Adjusted R-squared: 0.058 ## F-statistic: 2.662 on 1 and 26 DF, p-value: 0.1148 According to this result, we would accept the null hypothesis, and conclude that there is no relationship between herptile density and the proportion of forest on adjacent lands. But what happens when we enter both variables into the regression simultaneously? Rerun the above regression one more time, this time adding both inde- pendent variables into the model at once, such that logherp ~ logarea + cpfor2 . What do you conclude? model_mcp &lt;- lm(logherp ~ logarea + cpfor2, data = mydata) summary(model_mcp) ## ## Call: ## lm(formula = logherp ~ logarea + cpfor2, data = mydata) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.40438 -0.11512 0.01774 0.08187 0.36179 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.027058 0.166749 0.162 0.872398 ## logarea 0.247789 0.061603 4.022 0.000468 *** ## cpfor2 0.002724 0.001318 2.067 0.049232 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.175 on 25 degrees of freedom ## (2 observations deleted due to missingness) ## Multiple R-squared: 0.4493, Adjusted R-squared: 0.4052 ## F-statistic: 10.2 on 2 and 25 DF, p-value: 0.0005774 Now we reject both null hypotheses that the slope of the regression of logherp on logarea is zero and that the slope of the regression of logherp on cpfor2 is zero. Why is cpfor2 a significant predictor of logherp in the combined model when it was not significant in the simple linear model? The answer lies in the fact that it is sometimes necessary to control for one variable in order to detect the effect of another variable. In this case, there is a significant relationship between logherp and logarea that masks the relationship between logherp and cpfor2 . When both variables are entered into the model at once, the effect of logarea is controlled for, making it possible to detect a cpfor2 effect (and vice versa). Run another multiple regression, this time substituting thtden for cpfor2 as an independent variable (logherp ~ logarea + thtden). model_mden &lt;- lm(logherp ~ logarea + thtden, data = mydata) summary(model_mden) ## ## Call: ## lm(formula = logherp ~ logarea + thtden, data = mydata) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.31583 -0.12326 0.02095 0.13201 0.31674 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.37634 0.14926 2.521 0.018437 * ## logarea 0.22504 0.05701 3.947 0.000567 *** ## thtden -0.04196 0.01345 -3.118 0.004535 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1606 on 25 degrees of freedom ## (2 observations deleted due to missingness) ## Multiple R-squared: 0.5358, Adjusted R-squared: 0.4986 ## F-statistic: 14.43 on 2 and 25 DF, p-value: 6.829e-05 In this case we reject the null hypotheses that there are no effects of wetland area ( logarea ) and road density ( thtden ) on herptile richness ( logherp ). Note here that road density has a negative effect on richness, whereas wetland area and forested area ( cpfor2; results from previous regression) both have positive effects on herptile richness. The R2 of this model is even higher than the previous multiple regression model, reflecting a higher correlation between logherp and thtden than between logherp and cpfor2 (if you run a simple regression between logherp and thtden and compare it to the cpfor2 regression you should be able to detect this). Thus far, it appears that herptile richness is related to wetland area ( logarea ), road density ( thtden ), and possibly forest cover on adjacent lands ( cpfor2 ). But, does it necessarily follow that if we build a regression model with all three independent variables, that all three will show significant relationships? No, because we have not yet examined the relationship between Logarea , cpfor2 and thtden . Suppose, for example, two of the variables (say, cpfor2 and thtden ) are perfectly correlated. Then the thtden effect is nothing more than the cpfor2 effect (and vice versa), so that once we include one or the other in the regression model, none of the remaining variability would be explained by the third variable. Fit a regression model with logherp as the dependent variable and logarea , cpfor2 and thtden as the independent variables. What do you conclude? model_mtri &lt;- lm(logherp ~ logarea + cpfor2 + thtden, data = mydata) summary(model_mtri) ## ## Call: ## lm(formula = logherp ~ logarea + cpfor2 + thtden, data = mydata) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.30729 -0.13779 0.02627 0.11441 0.29582 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.284765 0.191420 1.488 0.149867 ## logarea 0.228490 0.057647 3.964 0.000578 *** ## cpfor2 0.001095 0.001414 0.774 0.446516 ## thtden -0.035794 0.015726 -2.276 0.032055 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1619 on 24 degrees of freedom ## (2 observations deleted due to missingness) ## Multiple R-squared: 0.5471, Adjusted R-squared: 0.4904 ## F-statistic: 9.662 on 3 and 24 DF, p-value: 0.0002291 Several things to note here: The regression coefficient for cpfor2 has become non-significant: once the variability explained by logarea and thtden is removed, a non-significant part of the remaining variability is explained by cpfor2. The R2 for this model (.547 is only marginally larger than the R2 for the model with only logarea and thtden (.536, which is again consistent with the non-significant coefficient for cpfor2. Note also that although the regression coefficient for thtden has not changed much from that obtained when just thtden and logarea were included in the fitted model (-.036 vs -.042, the standard error for the regression coefficient for thtden has increased slightly, meaning the estimate is less precise. If the correlation between thtden and cpfor2 was greater, the change in precision would also be greater. We can compare the fit of the last two models (i.e., the model with all 3 variables and the model with only logarea and thtden to decide which model is best to include. anova(model_mtri, model_mden) ## Analysis of Variance Table ## ## Model 1: logherp ~ logarea + cpfor2 + thtden ## Model 2: logherp ~ logarea + thtden ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 24 0.62937 ## 2 25 0.64508 -1 -0.015708 0.599 0.4465 Note that this is the identical result we obtained via the t-test of the effect of cpfor2 in the model with all 3 variables above as they are testing the same thing (this should make sense to you). From this analysis, we would conclude that the full model with all three variables included does not offer a significant improvement in fit over the model with only logarea and thtden. This isn’t surprising given that we already know that we cannot reject the null hypothesis of no effect of cpfor2 in the full model. Overall, we would conclude, on the basis of these analyses, that: Given the three variables thtden , logarea and cpfor2 , the best model is one that includes the first two variables. There is evidence of a negative relationship between herptile richness and the density of roads on adjacent lands. There is evidence that the larger the wetland area, the greater the herptile species richness. Note that by “best”, I don’t mean the best possible model, I mean the best one given the three predictor variables we started with. It seems pretty clear that there are other factors controlling richness in wetlands, since even with the “best” model, almost half of the variability in richness is unexplained. "],["stepwise-multiple-regression-procedures.html", "7.5 Stepwise multiple regression procedures", " 7.5 Stepwise multiple regression procedures There are a number of techniques available for selecting the multiple regression model that best suits your data. When working with only three independent variables it is often sufficient to work through the different combinations of possible variables yourself, until you are satisfied you have fit the best model. This is, essentially, what we did in the first section of this lab. However, the process can become tedious when dealing with numerous independent variables, and you may find an automatic procedure for fitting models to be easier to work with. Stepwise regression in R relies on the Akaike Information Criterion, as a measure of goodness of fit \\[AIC = 2k + 2ln(L))\\] where k is the number of regressors, and L is the maximized value of the likelihood function for the model). This is a statistic that rewards prediction precision while penalizing model complexity. If a new model has an AIC lower than that of the current model, the new model is a better fit to the data. Still working with the Mregdat data, run a stepwise multiple regression on the same set of variables: # Stepwise Regression step_mtri &lt;- step(model_mtri, direction = &quot;both&quot;) ## Start: AIC=-98.27 ## logherp ~ logarea + cpfor2 + thtden ## ## Df Sum of Sq RSS AIC ## - cpfor2 1 0.01571 0.64508 -99.576 ## &lt;none&gt; 0.62937 -98.267 ## - thtden 1 0.13585 0.76522 -94.794 ## - logarea 1 0.41198 1.04135 -86.167 ## ## Step: AIC=-99.58 ## logherp ~ logarea + thtden ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 0.64508 -99.576 ## + cpfor2 1 0.01571 0.62937 -98.267 ## - thtden 1 0.25092 0.89600 -92.376 ## - logarea 1 0.40204 1.04712 -88.013 step_mtri$anova # display results ## Step Df Deviance Resid. Df Resid. Dev AIC ## 1 NA NA 24 0.6293717 -98.26666 ## 2 - cpfor2 1 0.01570813 25 0.6450798 -99.57640 Examining the output, we find: R calculated the AIC for the starting model (here the full model with the 3 independent variables. The AIC for models where terms are deleted. Note here that the only way to reduce the AIC is to drop 2. The AIC for models where terms are added or deleted from the model selected in the first step (i.e. logherp ~ logarea + thtden. Note that none of these models are better. Instead of starting from the full (saturated) model and removing and possibly re-adding terms (i.e. direction = “both”), one can start from the null model and only add terms: # Forward selection approach model_null &lt;- lm(logherp ~ 1, data = mydata) step_f &lt;- stepAIC( model_null, scope = ~ . + logarea + cpfor2 + thtden, direction = &quot;forward&quot; ) ## Start: AIC=-82.09 ## logherp ~ 1 ## ## Df Sum of Sq RSS AIC ## + logarea 1 0.49352 0.8960 -92.376 ## + thtden 1 0.34241 1.0471 -88.013 ## + cpfor2 1 0.12907 1.2605 -82.820 ## &lt;none&gt; 1.3895 -82.091 ## ## Step: AIC=-92.38 ## logherp ~ logarea ## ## Df Sum of Sq RSS AIC ## + thtden 1 0.25093 0.64508 -99.576 ## + cpfor2 1 0.13078 0.76522 -94.794 ## &lt;none&gt; 0.89600 -92.376 ## ## Step: AIC=-99.58 ## logherp ~ logarea + thtden ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 0.64508 -99.576 ## + cpfor2 1 0.015708 0.62937 -98.267 step_f$anova # display results ## Stepwise Model Path ## Analysis of Deviance Table ## ## Initial Model: ## logherp ~ 1 ## ## Final Model: ## logherp ~ logarea + thtden ## ## ## Step Df Deviance Resid. Df Resid. Dev AIC ## 1 27 1.3895281 -82.09073 ## 2 + logarea 1 0.4935233 26 0.8960048 -92.37639 ## 3 + thtden 1 0.2509250 25 0.6450798 -99.57640 You should first notice that the final result is the same as the default stepwise regression and as what we got building the model from scratch. In forward selection, R first fits the least complex model (i.e, with only an intercept), and then adds variables, one by one, according to AIC statistics. Thus, in the above example, the model was first fit with only an intercept. Next, logarea was added, followed by thtden. cpfor2 was not added because it would make AIC increase to above that of the model fit with the first two variables. Generally speaking, when doing multiple regressions, it is good practice to try several different methods (e.g. all regressions, stepwise, and backward elimination, etc.) and see whether you get the same results. If you don’t, then the “best” model may not be so obvious, and you will have to think very carefully about the inferences you draw. In this case, regardless of whether we use automatic, or forward/backward stepwise regression, we arrive at the same model. When doing multiple regression, always bear in mind the following: Different procedures may produce different “best” models, i.e. the “best” model obtained using forward stepwise regression needn’t necessarily be the same as that obtained using backward stepwise. It is good practice to try several different methods and see whether you end up with the same result. If you don’t, it is almost invariably due to multicollinearity among the independent variables. Be wary of stepwise regression. As the authors of SYSTAT, another commonly used statistical package, note: Stepwise regression is probably the most abused computerized statistical technique ever devised. If you think you need automated stepwise regression to solve a particular problem, you probably don’t. Professional statisticians rarely use automated stepwise regression because it does not necessarily find the “best” fitting model, the “real” model, or alternative “plausible” models. Furthermore, the order in which variables enter or leave a stepwise program is usually of no theoretical significance. You are always better off thinking about why a model could generate your data and then testing that model. Remember that just because there is a significant regression of Y on X doesn’t mean that X causes Y: correlation does not imply causation! "],["detecting-multicollinearity.html", "7.6 Detecting multicollinearity", " 7.6 Detecting multicollinearity Multicollinearity is the presence of correlations among independent variables. In extreme cases (perfect collinearity) it will prevent you from fitting some models. When collinearity is not perfect, it reduces your ability to test for the effect of individual variables, but does not affect the ability of the model to predict. The help file for the HH 📦package contains this clear passage about one of the indices of multicollinearity, the variance inflation factors: A simple diagnostic of collinearity is the variance inflation factor, VIF one for each regression coefficient (other than the intercept). Since the condition of collinearity involves the predictors but not the response, this measure is a function of the X’s but not of Y. The VIF for predictor i is \\[1/(1-R_i^2)\\] where Ri2 is the R2 from a regression of predictor i against the remaining predictors. If Ri2 is close to 1, this means that predictor i is well explained by a linear function of the remaining predictors, and, therefore, the presence of predictor i in the model is redundant. Values of VIF exceeding 5 are considered evidence of collinearity: The information carried by a predictor having such a VIF is contained in a subset of the remaining predictors. If, however, all of a model’s regression coefficients differ significantly from 0 (p-value &lt; .05), a somewhat larger VIF may be tolerable. VIFs indicate by how much the variance of each regression coefficient is increased by the presence of collinearity. There are several vif() functions (I know of at least three in the packages car, HH and DAAG) and I do not know if and how they differ. To quantify multicollinarity, one can simply call the vif() function from the package car: library(car) vif(model_mtri) ## logarea cpfor2 thtden ## 1.022127 1.344455 1.365970 Here there is no evidence that multicollinearity is a problem since all vif are close to 1. "],["polynomial-regression.html", "7.7 Polynomial regression", " 7.7 Polynomial regression In the regression models considered so far, we have assumed that the relationship between the dependent and independent variables is linear. If not, in some cases it can be made linear by transforming one or both variables. On the other hand, for many biological relationships no transformation in the world will help, and we are forced to go with some sort of non-linear regression method. The simplest type of nonlinear regression method is polynomial regression, in which you fit regression models that include independent variables raised to some power greater than one, e.g. X2, X3, etc. Plot the relationship between the residuals of the logherp ~ logarea regression and swamp. # problème avec les données de manquantes dans logherp mysub &lt;- subset(mydata, !is.na(logherp)) # ajouter les résidus dans les donnée mysub$resloga &lt;- residuals(model_loga) ggplot(data = mysub, aes(y = resloga, x = swamp)) + geom_point() + geom_smooth() ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Figure 7.3: Relation entre swamp et les résidus de la régression entre logherp et logarea Visual inspection of this graph suggests that there is a strong, but highly nonlinear, relationship between these two variables. Try regressing the residuals of the logherp ~ logarea regression on swamp. What do you conclude? model_resloga &lt;- lm(resloga ~ swamp, mysub) summary(model_resloga) ## ## Call: ## lm(formula = resloga ~ swamp, data = mysub) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.35088 -0.13819 0.00313 0.10849 0.45802 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.084571 0.109265 0.774 0.446 ## swamp -0.001145 0.001403 -0.816 0.422 ## ## Residual standard error: 0.1833 on 26 degrees of freedom ## Multiple R-squared: 0.02498, Adjusted R-squared: -0.01252 ## F-statistic: 0.666 on 1 and 26 DF, p-value: 0.4219 In other words, the fit is terrible, even though you can see from the graph that there is in fact quite a strong relationship between the two - it’s just that it is a non-linear relationship. (If you look at model assumptions for this model, you will see strong evidence of nonlinearity, as expected.) The pattern might be well described by a quadratic relation. Rerun the above regression but add a second term in the Formula field to represent swamp2 . If you simply add swamp2 in the model R won’t fit a quadratic effect, you need to use the functionI() which indicates that the formula within should be evaluated before fitting the model. The expression should appear as: \\[ residuals ~ swamp + I(swamp^2)\\]. What do you conclude? What does examination of the residuals from this multiple regression tell you? model_resloga2 &lt;- lm(resloga ~ swamp + I(swamp^2), mysub) summary(model_resloga2) ## ## Call: ## lm(formula = resloga ~ swamp + I(swamp^2), data = mysub) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.181185 -0.085350 0.007377 0.067327 0.242455 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -7.804e-01 1.569e-01 -4.975 3.97e-05 *** ## swamp 3.398e-02 5.767e-03 5.892 3.79e-06 *** ## I(swamp^2) -2.852e-04 4.624e-05 -6.166 1.90e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1177 on 25 degrees of freedom ## Multiple R-squared: 0.6132, Adjusted R-squared: 0.5823 ## F-statistic: 19.82 on 2 and 25 DF, p-value: 6.972e-06 par(mfrow = c(2, 2)) plot(model_resloga2) It is clear that once the effects of area are controlled for, a considerable amount of the remaining variability in herptile richness is explained by swamp , in a nonlinear fashion. If you examine model assumptions, you will see that compared to the linear model, the fit is much better. Based on the results from the above analyses, how would you modify the regression model arrived at above? What, in your view, is the “best” overall model? Why? How would you rank the various factors in terms of their effects on herptile species richness? In light of these results, we might want to try and fit a model which includes logarea, thtden, cpfor2, swamp and swamp^2^ : model_poly1 &lt;- lm( logherp ~ logarea + cpfor2 + thtden + swamp + I(swamp^2), data = mydata ) summary(model_poly1) ## ## Call: ## lm(formula = logherp ~ logarea + cpfor2 + thtden + swamp + I(swamp^2), ## data = mydata) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.201797 -0.056170 -0.002072 0.051814 0.205626 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3.203e-01 1.813e-01 -1.766 0.0912 . ## logarea 2.202e-01 3.893e-02 5.656 1.09e-05 *** ## cpfor2 -7.864e-04 9.955e-04 -0.790 0.4380 ## thtden -2.929e-02 1.048e-02 -2.795 0.0106 * ## swamp 3.113e-02 5.898e-03 5.277 2.70e-05 *** ## I(swamp^2) -2.618e-04 4.727e-05 -5.538 1.45e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1072 on 22 degrees of freedom ## (2 observations deleted due to missingness) ## Multiple R-squared: 0.8181, Adjusted R-squared: 0.7767 ## F-statistic: 19.78 on 5 and 22 DF, p-value: 1.774e-07 Note that on the basis of this analysis, we could potentially drop cpfor2 and refit using the remaining variables: model_poly2 &lt;- lm( logherp ~ logarea + thtden + swamp + I(swamp^2), data = mydata ) summary(model_poly2) ## ## Call: ## lm(formula = logherp ~ logarea + thtden + swamp + I(swamp^2), ## data = mydata) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.19621 -0.05444 -0.01202 0.07116 0.21295 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3.461e-01 1.769e-01 -1.957 0.0626 . ## logarea 2.232e-01 3.842e-02 5.810 6.40e-06 *** ## thtden -2.570e-02 9.364e-03 -2.744 0.0116 * ## swamp 2.956e-02 5.510e-03 5.365 1.89e-05 *** ## I(swamp^2) -2.491e-04 4.409e-05 -5.649 9.46e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1063 on 23 degrees of freedom ## (2 observations deleted due to missingness) ## Multiple R-squared: 0.8129, Adjusted R-squared: 0.7804 ## F-statistic: 24.98 on 4 and 23 DF, p-value: 4.405e-08 How about multicollinearity in this model? vif(model_poly2) ## logarea thtden swamp I(swamp^2) ## 1.053193 1.123491 45.845845 45.656453 VIF for the two swamp terms are much higher than the standard threshold of 5. However, this is expected for polynomial terms, and not really a concern given that both terms are highly significant in the model. The high VIF means that these two coefficients are not estimated precisely, but using both in the model still allows to make a good prediction (i.e. account for the response to swamp). "],["checking-assumptions-of-a-multiple-regression-model.html", "7.8 Checking assumptions of a multiple regression model", " 7.8 Checking assumptions of a multiple regression model All the model selection techniques or the manual model crafting assumes that the standard assumptions (independence, normality, homoscedasticity, linearity) are met. Given that a large number of models can be fitted, it may seem that testing the assumptions at each step would be an herculean task. However, it is generally sufficient to examine the residuals of the full (saturated) model and of the final model. Terms not contributing significantly to the fit do not affect residuals much, and therefore, the residuals to the full model, or the residuals to the final model, are generally sufficient. Let’s have a look at the diagnostic plots for the final model. Here we use the check_model() function from the performance 📦. library(performance) check_model(model_poly2) Figure 7.4: Conditions d’application du modèle model_poly2 Alternatively it can be done with the classic method par(mfrow = c(2, 2)) plot(model_poly2) Figure 7.5: Conditions d’application du modèle model_poly2 Everything looks about right here. For the skeptic, let’s run the formal tests. shapiro.test(residuals(model_poly2)) ## ## Shapiro-Wilk normality test ## ## data: residuals(model_poly2) ## W = 0.9837, p-value = 0.9278 The residuals do not deviate from normality. Good. library(lmtest) bptest(model_poly2) ## ## studentized Breusch-Pagan test ## ## data: model_poly2 ## BP = 3.8415, df = 4, p-value = 0.4279 No deviation from homoscedasticity either. Good. dwtest(model_poly2) ## ## Durbin-Watson test ## ## data: model_poly2 ## DW = 1.725, p-value = 0.2095 ## alternative hypothesis: true autocorrelation is greater than 0 No serial correlation in the residuals, so no evidence of non-independence. resettest(model_poly2, type = &quot;regressor&quot;, data = mydata) ## ## RESET test ## ## data: model_poly2 ## RESET = 0.9823, df1 = 8, df2 = 15, p-value = 0.4859 And no significant deviation from linearity. So it seems that all is fine. "],["visualizing-effect-size.html", "7.9 Visualizing effect size", " 7.9 Visualizing effect size Les coefficients de la régression multiple peuvent mesurer la taille d’effet, quoiqu’il puisse être nécessaire de les standardiser pour qu’ils ne soient pas influencés par les unités de mesure. Mais un graphique est souvent plus informatif. Dans ce contexte, les graphiques des résidus partiels (appelés components+residual plots dans R) sont particulièrement utiles. Ces graphique illustrent comment la variable dépendante, corrigée pour l’effet des autres variables dans le modèle, varie avec chacune des variables indépendantes du modèle. Voyons voir: # Evaluate visually linearity and effect size # component + residual plot crPlots(model_poly2) Figure 7.6: Graphiques de résidus partiels du modèle model_poly2 Notez que l’échelle de l’axe des y varie sur chaque graphique. Pour thtden, la variable dépendante (log10(richesse des herptiles)) varie d’environ 0.4 unités entre la valeur minimum et maximum de thtden. Pour logarea, la variation est d’environ 0.6 unité log. Pour swamp, l’interprétation est plus compliquée parce qu’il y a deux termes qui quantifient son effet, et que ces termes ont des signes opposés (positif pour swamp et négatif pour swamp^2) ce qui donne une relation curvilinéaire de type parabole. Le graphique ne permet pas de bien visualiser cela. Ceci dit, ces graphique n’indiquent pas vraiment de violation de linéarité. Pour illustrer ce qui serait visible sur ces graphiques si il y avait une déviation de linéarité, enlevons le terme du second degré pour swamp, puis on va refaire ces graphiques et effectuer le test RESET. model_nopoly &lt;- lm( logherp ~ logarea + thtden + swamp, data = mydata ) crPlots(model_nopoly) Figure 7.7: Graphiques de résidus partiels du modèle model_nopoly La relation non-linéaire avec swamp devient évidente. Et le test RESET détecte bien cette non-linéarité: resettest(model_nopoly, type = &quot;regressor&quot;) ## ## RESET test ## ## data: model_nopoly ## RESET = 6.7588, df1 = 6, df2 = 18, p-value = 0.0007066 "],["tester-la-présence-dinteractions.html", "7.10 Tester la présence d’interactions", " 7.10 Tester la présence d’interactions Lorsqu’il y a plusieurs variables indépendantes, vous devriez toujours garder à l’esprit la possibilité d’interactions. Dans la majorité des situations de régression multiple cela n’est pas évident parce que l’addition de termes d’interaction augmente la multicolinéarité des termes du modèle, et parce qu’il n’y a souvent pas assez d’observations pour éprouver toutes les interactions ou que les observations ne sont pas suffisamment balancées pour faire des tests puissants pour les interactions. Retournons à notre modèle “final” et voyons ce qui se passe si on essaie d’ajuster un modèle saturé avec toutes les interactions: fullmodel_withinteractions &lt;- lm( logherp ~ logarea * cpfor2 * thtden * swamp * I(swamp^2), data = mydata ) summary(fullmodel_withinteractions) ## ## Call: ## lm(formula = logherp ~ logarea * cpfor2 * thtden * swamp * I(swamp^2), ## data = mydata) ## ## Residuals: ## ALL 28 residuals are 0: no residual degrees of freedom! ## ## Coefficients: (4 not defined because of singularities) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -5.948e+03 NaN NaN NaN ## logarea 3.293e+03 NaN NaN NaN ## cpfor2 7.080e+01 NaN NaN NaN ## thtden 9.223e+02 NaN NaN NaN ## swamp 1.176e+02 NaN NaN NaN ## I(swamp^2) -3.517e-01 NaN NaN NaN ## logarea:cpfor2 -3.771e+01 NaN NaN NaN ## logarea:thtden -4.781e+02 NaN NaN NaN ## cpfor2:thtden -1.115e+01 NaN NaN NaN ## logarea:swamp -7.876e+01 NaN NaN NaN ## cpfor2:swamp -1.401e+00 NaN NaN NaN ## thtden:swamp -1.920e+01 NaN NaN NaN ## logarea:I(swamp^2) 5.105e-01 NaN NaN NaN ## cpfor2:I(swamp^2) 3.825e-03 NaN NaN NaN ## thtden:I(swamp^2) 7.826e-02 NaN NaN NaN ## swamp:I(swamp^2) -2.455e-03 NaN NaN NaN ## logarea:cpfor2:thtden 5.359e+00 NaN NaN NaN ## logarea:cpfor2:swamp 8.743e-01 NaN NaN NaN ## logarea:thtden:swamp 1.080e+01 NaN NaN NaN ## cpfor2:thtden:swamp 2.620e-01 NaN NaN NaN ## logarea:cpfor2:I(swamp^2) -5.065e-03 NaN NaN NaN ## logarea:thtden:I(swamp^2) -6.125e-02 NaN NaN NaN ## cpfor2:thtden:I(swamp^2) -1.551e-03 NaN NaN NaN ## logarea:swamp:I(swamp^2) -4.640e-04 NaN NaN NaN ## cpfor2:swamp:I(swamp^2) 3.352e-05 NaN NaN NaN ## thtden:swamp:I(swamp^2) 2.439e-04 NaN NaN NaN ## logarea:cpfor2:thtden:swamp -1.235e-01 NaN NaN NaN ## logarea:cpfor2:thtden:I(swamp^2) 7.166e-04 NaN NaN NaN ## logarea:cpfor2:swamp:I(swamp^2) NA NA NA NA ## logarea:thtden:swamp:I(swamp^2) NA NA NA NA ## cpfor2:thtden:swamp:I(swamp^2) NA NA NA NA ## logarea:cpfor2:thtden:swamp:I(swamp^2) NA NA NA NA ## ## Residual standard error: NaN on 0 degrees of freedom ## (2 observations deleted due to missingness) ## Multiple R-squared: 1, Adjusted R-squared: NaN ## F-statistic: NaN on 27 and 0 DF, p-value: NA Notez les coefficients manquants aux dernières lignes: on ne peut inclure les 32 termes si on a seulement 28 observations. Il manque des observations, le R carré est 1, et le modèle “prédit” parfaitement les données. Si on essaie une méthode automatique pour identifier le “meilleur” modèle dans ce gâchis, R refuse: step(fullmodel_withinteractions) ## Error in step(fullmodel_withinteractions): AIC is -infinity for this model, so &#39;step&#39; cannot proceed Bon, est-ce qu’on oublie tout ça et qu’on accepte le modèle final sans ce soucier des interactions? Non, pas encore. Il y a un compromis possible: comparer notre modèle “final” à un modèle qui contient au moins un sous-ensemble des interactions, par exemple toutes les interactions du second degré, pour éprouver si l’addition de ces interactions améliore beaucoup l’ajustement du modèle. full_model_2ndinteractions &lt;- lm( logherp ~ logarea + cpfor2 + thtden + swamp + I(swamp^2) + logarea:cpfor2 + logarea:thtden + logarea:swamp + cpfor2:thtden + cpfor2:swamp + thtden:swamp, data = mydata ) summary(full_model_2ndinteractions) ## ## Call: ## lm(formula = logherp ~ logarea + cpfor2 + thtden + swamp + I(swamp^2) + ## logarea:cpfor2 + logarea:thtden + logarea:swamp + cpfor2:thtden + ## cpfor2:swamp + thtden:swamp, data = mydata) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.216880 -0.036534 0.003506 0.042990 0.175490 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.339e-01 6.325e-01 0.686 0.502581 ## logarea -1.254e-01 2.684e-01 -0.467 0.646654 ## cpfor2 -9.344e-03 7.205e-03 -1.297 0.213032 ## thtden -1.833e-01 9.035e-02 -2.028 0.059504 . ## swamp 3.569e-02 7.861e-03 4.540 0.000334 *** ## I(swamp^2) -3.090e-04 7.109e-05 -4.347 0.000500 *** ## logarea:cpfor2 2.582e-03 2.577e-03 1.002 0.331132 ## logarea:thtden 7.017e-02 3.359e-02 2.089 0.053036 . ## logarea:swamp -5.290e-04 2.249e-03 -0.235 0.816981 ## cpfor2:thtden -2.095e-04 6.120e-04 -0.342 0.736544 ## cpfor2:swamp 4.651e-05 5.431e-05 0.856 0.404390 ## thtden:swamp 2.248e-04 4.764e-04 0.472 0.643336 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.108 on 16 degrees of freedom ## (2 observations deleted due to missingness) ## Multiple R-squared: 0.8658, Adjusted R-squared: 0.7735 ## F-statistic: 9.382 on 11 and 16 DF, p-value: 4.829e-05 Ce modèle s’ajuste un peu mieux aux données que les modèle “final” (il explique 86.6% de la variance de logherp, comparé à 81.2% pour le modèle “final” sans interactions), mais il compte deux fois plus de paramètres. De plus, si vous examinez les coefficients, il se passe d’étranges choses: le signe pour logare a changé par exemple. C’est un des symptômes de la multicolinéarité. Allons voir les facteurs d’inflation de la variance: vif(full_model_2ndinteractions) ## logarea cpfor2 thtden swamp I(swamp^2) ## 49.86060 78.49622 101.42437 90.47389 115.08457 ## logarea:cpfor2 logarea:thtden logarea:swamp cpfor2:thtden cpfor2:swamp ## 66.97792 71.69894 67.27034 14.66814 29.41422 ## thtden:swamp ## 20.04410 Aie! tous les VIF sont plus grands que 5, pas seulement les termes incluant swamp. Cette forte multicolinéarité empêche de quantifier avec précision l’effet de ces interactions. De plus, ce modèle avec interactions n’est pas plus informatif que le modèle “final” puisque son AIC est plus élevé (souvenez-vous qu’on privilégie le modèle avec la valeur d’AIC la plus basse): AIC(model_poly1) ## [1] -38.3433 AIC(full_model_2ndinteractions) ## [1] -34.86123 On peut également utiliser la fonction anova() pour comparer l’ajustement des deux modèles et vérifier si l’addition des termes d’intération améliore significativement l’ajustement: anova(model_poly1, full_model_2ndinteractions) ## Analysis of Variance Table ## ## Model 1: logherp ~ logarea + cpfor2 + thtden + swamp + I(swamp^2) ## Model 2: logherp ~ logarea + cpfor2 + thtden + swamp + I(swamp^2) + logarea:cpfor2 + ## logarea:thtden + logarea:swamp + cpfor2:thtden + cpfor2:swamp + ## thtden:swamp ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 22 0.25282 ## 2 16 0.18651 6 0.066314 0.9481 0.489 Ici, l’addition des termes d’interaction ne réduit pas significativement la variabilité résiduelle du modèle “complet”. Qu’en est-il de la si on compare le modèle avec interaction et notre modèle “final”? anova(model_poly2, full_model_2ndinteractions) ## Analysis of Variance Table ## ## Model 1: logherp ~ logarea + thtden + swamp + I(swamp^2) ## Model 2: logherp ~ logarea + cpfor2 + thtden + swamp + I(swamp^2) + logarea:cpfor2 + ## logarea:thtden + logarea:swamp + cpfor2:thtden + cpfor2:swamp + ## thtden:swamp ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 23 0.25999 ## 2 16 0.18651 7 0.073486 0.9006 0.5294 Le test indique que ces deux modèles ont des variances résiduelles comparables, et donc que l’addition des termes d’interaction et de cpfor2 au modèle final n’apporte pas grand chose. "],["recherche-du-meilleur-modèle-fondée-sur-la-théorie-de-linformation.html", "7.11 Recherche du meilleur modèle fondée sur la théorie de l’information", " 7.11 Recherche du meilleur modèle fondée sur la théorie de l’information Une des principales critiques des méthodes pas-à-pas (stepwise) est que les valeurs de p ne sont pas strictement interprétables à cause du grand nombre de tests qui sont implicites dans le processus. C’est le problème des comparaisons ou tests multiples: en construisant un modèle linéaire (comme une régression multiple) à partir d’un grand nombre de variables et de leurs interactions, il y a tellement de combinaisons possibles qu’un ajustement de Bonferroni rendrait les tests trop conservateurs. Une alternative, défendue par Burnham et Anderson (2002, Model selection and multimodel inference: a practical information-theoretic approach. 2nd ed), est d’utiliser l’AIC (ou mieux encore AICc qui est plus approprié quand le nombre d’observations est inférieur à 40 fois le nombre de variables indépendantes) pour ordonner les modèles et identifier un sousensemble de modèles qui sont les meilleurs. On peut ensuite calculer les moyennes des coefficients pondérées par la probabilité que chacun des modèles soit le meilleur pour obtenir des coefficients qui sont plus robustes et moins sensibles à la multicolinéarité. L’approche de comparaison par AIC a d’abord été développé pour comparer un ensemble de modèle préalablement défini basé sur les connaissance du sytème et les hypothèses biologiques. Cependant, certains ont développé une approche plutôt brutale et sans scrupule de faire tous les modèles possibles et de les comparer par AIC. Cette approche a été suivie dans le package MuMIn. Les comparaisons de modèle par AICdoivent être faites en utilisant exactement le même jeu de données pour chaque modèle. Il faut donc s’arrurer d’enlever les données manquantes et de spécifier dans la fonction lm de ne pas marcher s’il y a des données manquantes. Je ne supporte pas l’approche stepwise ni l’approche par AIC. Je déteste l’approche par la fonction dredge() qui selon moi va à l’encontre de la philosophie des AIC et de la parsimonie. Je soutiens de dévelooper un modèle basé sur des hypothèses biologiques et de reporter ce modèle avec tous les effets significatifs ou non. # refaire le modèle en s&#39;assurant qu&#39;il n&#39;y a pas de &quot;NA&quot; # et en spécificant na.action full_model_2ndinteractions &lt;- update( full_model_2ndinteractions, . ~ ., data = mysub, na.action = &quot;na.fail&quot; ) library(MuMIn) dd &lt;- dredge(full_model_2ndinteractions) ## Fixed term is &quot;(Intercept)&quot; L’objet dd contient tous les modèles possibles (i.e. ceux qui ont toutes les combinaisons possibles) en utilisant les termes du modèle full_model_2ndinteractions ajusté précédemment. On peut ensuite extraire de l’objet dd le sous-ensemble de modèles qui ont un AICc semblable au meilleur modèle (Burnham et Anderson suggèrent que les modèles qui dévient par plus de 7 unités d’AICc du meilleur modèle ont peu de support empirique). # get models within 2 units of AICc from the best model top_models_1 &lt;- get.models(dd, subset = delta &lt; 2) avgmodel1 &lt;- model.avg(top_models_1) # compute average parameters summary(avgmodel1) # display averaged model ## ## Call: ## model.avg(object = top_models_1) ## ## Component model call: ## lm(formula = logherp ~ &lt;2 unique rhs&gt;, data = mysub, na.action = na.fail) ## ## Component models: ## df logLik AICc delta weight ## 12345 7 27.78 -35.95 0.00 0.55 ## 1234 6 25.78 -35.56 0.39 0.45 ## ## Term codes: ## I(swamp^2) logarea swamp thtden logarea:thtden ## 1 2 3 4 5 ## ## Model-averaged coefficients: ## (full average) ## Estimate Std. Error Adjusted SE z value Pr(&gt;|z|) ## (Intercept) -2.145e-01 2.308e-01 2.406e-01 0.891 0.373 ## logarea 1.356e-01 1.089e-01 1.119e-01 1.213 0.225 ## swamp 3.180e-02 5.971e-03 6.273e-03 5.070 4e-07 *** ## I(swamp^2) -2.669e-04 4.770e-05 5.011e-05 5.326 1e-07 *** ## thtden -6.985e-02 5.233e-02 5.361e-02 1.303 0.193 ## logarea:thtden 2.131e-02 2.487e-02 2.545e-02 0.837 0.403 ## ## (conditional average) ## Estimate Std. Error Adjusted SE z value Pr(&gt;|z|) ## (Intercept) -2.145e-01 2.308e-01 2.406e-01 0.891 0.3727 ## logarea 1.356e-01 1.089e-01 1.119e-01 1.213 0.2253 ## swamp 3.180e-02 5.971e-03 6.273e-03 5.070 4e-07 *** ## I(swamp^2) -2.669e-04 4.770e-05 5.011e-05 5.326 1e-07 *** ## thtden -6.985e-02 5.233e-02 5.361e-02 1.303 0.1927 ## logarea:thtden 3.882e-02 2.114e-02 2.237e-02 1.735 0.0827 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 confint(avgmodel1) # display CI for averaged coefficients ## 2.5 % 97.5 % ## (Intercept) -0.6860022996 0.257064603 ## logarea -0.0836067896 0.354883299 ## swamp 0.0195105703 0.044099316 ## I(swamp^2) -0.0003650809 -0.000168656 ## thtden -0.1749296690 0.035236794 ## logarea:thtden -0.0050266778 0.082666701 La liste des modèles qui sont à 4 unités ou moins de l’AICc du meilleur modèle. Les variables dans chaque modèle sont codées et on retrouve la clé en dessous du tableau. Pour chaque modèle, en plus de l’AICc, le poids Akaike est calculé. C’est un estimé de la probabilité que ce modèle est le meilleur. Ici on voit que le premier modèle (le meilleur) a seulement 34% des chance d’être vraiment le meilleur. À partir de ce sous-ensemble de modèles, la moyenne pondérée des coefficients (en utilisant les poids Akaike) est calculée, avec in IC à 95%. Notez que les termes absents d’un modèle sont considérés avoir un coefficient de 0 pour ce terme. "],["bootstrap-et-régression-multiple.html", "7.12 Bootstrap et régression multiple", " 7.12 Bootstrap et régression multiple Quand les données ne rencontrent pas les conditions d’application de normalité et d’homoscédasticité et que les transformations n’arrivent pas à corriger ces violations, le bootstrap peut être utilisé pour calculer des intervalles de confiance pour les coefficients. Si la distribution des coefficients bootstrappés est symétrique et approximativement normale, on peut utiliser les percentiles empiriques pour calculer les limites de confiance. Le code qui suit, utilisant le package simpleboot, a été conçu pour être facilement modifiable et calcule les limites des IC à partir des percentiles empiriques. ############################################################ ####### # Bootstrap analysis the simple way with library simpleboot # Define model to be bootstrapped and the data source used mymodel &lt;- lm(logherp ~ logarea + thtden + swamp + I(swamp^2), data = mydata) # Set the number of bootstrap iterations nboot &lt;- 1000 library(simpleboot) # R is the number of bootstrap iterations # Setting rows to FALSE indicates resampling of residuals mysimpleboot &lt;- lm.boot(mymodel, R = nboot, rows = FALSE) # Extract bootstrap coefficients myresults &lt;- sapply(mysimpleboot$boot.list, function(x) x$coef) # Transpose matrix so that lines are bootstrap iterations # and columns are coefficients tmyresults &lt;- t(myresults) Vous pouvez ensuite faire des graphiques pour voir les résultats. Lorsque vous tournerez ce code, il y aura une pause pour vous permettre d’examiner la distribution pour chaque coefficient du modèle sur des graphiques: # Plot histograms of bootstrapped coefficients ncoefs &lt;- length(data.frame(tmyresults)) par(mfrow = c(1, 2), mai = c(0.5, 0.5, 0.5, 0.5), ask = TRUE) for (i in 1:ncoefs) { lab &lt;- colnames(tmyresults)[i] x &lt;- tmyresults[, i] plot(density(x), main = lab, xlab = &quot;&quot;) abline(v = mymodel$coef[i], col = &quot;red&quot;) abline(v = quantile(x, c(0.025, 0.975))) hist(x, main = lab, xlab = &quot;&quot;) abline(v = quantile(x, c(0.025, 0.975))) abline(v = mymodel$coef[i], col = &quot;red&quot;) } Figure 7.8: Distribution des estimé par bootstrap pour logarea Le graphique de droite illustre la densité lissée (kernel density) et celui de gauche est l’histogramme des estimés bootstrap du coefficient. La ligne rouge sur le graphique indique la valeur du coefficient ordinaire (pas bootstrap) et les deux lignes verticales noires marquent les limites de l’intervalle de confiance à 95%. Ici l’IC ne contient pas 0, et donc on peut conclure que l’effet de logarea sur logherp est significativement positif. Les limites précises peuvent être obtenues par: # Display empirical bootstrap quantiles (not corrected for bias) p &lt;- c(0.005, 0.01, 0.025, 0.05, 0.95, 0.975, 0.99, 0.995) apply(tmyresults, 2, quantile, p) ## (Intercept) logarea thtden swamp I(swamp^2) ## 0.5% -0.75190854 0.1437474 -0.047894948 0.01610660 -0.0003549979 ## 1% -0.71503313 0.1499690 -0.045785709 0.01710535 -0.0003498040 ## 2.5% -0.65877271 0.1590264 -0.042492649 0.01961127 -0.0003335924 ## 5% -0.61683811 0.1680194 -0.040369219 0.02117658 -0.0003180897 ## 95% -0.07537645 0.2783590 -0.011475445 0.03801454 -0.0001830251 ## 97.5% -0.02203447 0.2893995 -0.009071042 0.04022615 -0.0001688640 ## 99% 0.01373337 0.3015650 -0.005676100 0.04227593 -0.0001470395 ## 99.5% 0.03300897 0.3125414 -0.004550531 0.04324638 -0.0001397980 Ces intervalles de confiances ne sont pas fiables si la distribution des estimés bootstrap n’est pas Gaussienne. Dans ce cas, il vaut mieux calculer des coefficients non-biaisés (bias-corrected accelerated confidence limits, BCa): ################################################ # Bootstrap analysis in multiple regression with BCa confidence intervals # Preferable when parameter distribution is far from normal # Bootstrap 95% BCa CI for regression coefficients library(boot) # function to obtain regression coefficients for each iteration bs &lt;- function(formula, data, indices) { d &lt;- data[indices, ] # allows boot to select sample fit &lt;- lm(formula, data = d) return(coef(fit)) } # bootstrapping with 1000 replications results &lt;- boot( data = mydata, statistic = bs, R = 1000, formula = logherp ~ logarea + thtden + swamp + I(swamp^2) ) # view results Pour obtenir les résultats, le code suivant va produire le graphique standard pour chaque coefficient, et les estimés BCa pour l’intervalle de confiance plot(results, index = 1) # intercept plot(results, index = 2) # logarea plot(results, index = 3) # thtden plot(results, index = 4) # swamp plot(results, index = 5) # swamp2 # get 95% confidence intervals boot.ci(results, type = &quot;bca&quot;, index = 1) boot.ci(results, type = &quot;bca&quot;, index = 2) boot.ci(results, type = &quot;bca&quot;, index = 3) boot.ci(results, type = &quot;bca&quot;, index = 4) boot.ci(results, type = &quot;bca&quot;, index = 5) Pour logarea, cela donne: ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 1000 bootstrap replicates ## ## CALL : ## boot.ci(boot.out = results, type = &quot;bca&quot;, index = 2) ## ## Intervals : ## Level BCa ## 95% ( 0.1118, 0.3194 ) ## Calculations and Intervals on Original Scale Notez que l’intervalle BCa va de 0.12 à 0.32, alors que l’intervalle standard était de 0.16 à 0.29. L’intervalle BCa est ici plus grand du côté inférieur et plus petit du côté supérieur comme il se doit compte tenu de la distribution non-Gaussienne et asymétrique des estimés bootstrap. "],["perm_reg_mult.html", "7.13 Test de permutation", " 7.13 Test de permutation Les tests de permutations sont plus rarement effectués en régression multiple que le bootstrap. Voici un fragment de code pour le faire tout de même. ############################################################ ########## # Permutation in multiple regression # # using lmperm library library(lmPerm) # Fit desired model on the desired dataframe my_model &lt;- lm(logherp ~ logarea + thtden + swamp + I(swamp^2), data = mydata ) my_model_prob &lt;- lmp( logherp ~ logarea + thtden + swamp + I(swamp^2), data = mydata, perm = &quot;Prob&quot; ) summary(my_model) summary(my_model_prob) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
