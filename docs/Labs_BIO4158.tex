% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  12pt,
]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={BIO4158 Applied biostats with R},
  pdfauthor={Julien Martin},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
%\usepackage{booktabs}
\usepackage{ctable}
\usepackage{fancyhdr}
\usepackage{float}
\usepackage[margin=2cm]{geometry}

\floatplacement{figure}{H}

%\usepackage[sf,bf]{titlesec}

\hypersetup{colorlinks=true, urlcolor=blue}

\renewcommand{\chaptername}{Chapitre}
\renewcommand{\contentsname}{Table des Mati√®res}
\renewcommand{\partname}{Partie}

\usepackage{framed,color}
\definecolor{incolor}{RGB}{240,240,240}
\definecolor{outcolor}{RGB}{248,248,248}

\renewcommand{\textfraction}{0.05}
\renewcommand{\topfraction}{0.8}
\renewcommand{\bottomfraction}{0.8}
\renewcommand{\floatpagefraction}{0.75}

%\renewenvironment{quote}{\begin{VF}}{\end{VF}}

\ifxetex
 \usepackage{letltxmacro}
 \setlength{\XeTeXLinkMargin}{1pt}
 \LetLtxMacro\SavedIncludeGraphics\includegraphics
 \def\includegraphics#1#{% #1 catches optional stuff (star/opt. arg.)
   \IncludeGraphicsAux{#1}%
 }%
 \newcommand*{\IncludeGraphicsAux}[2]{%
   \XeTeXLinkBox{%
     \SavedIncludeGraphics#1{#2}%
   }%
 }%
\fi

\makeatletter
\newenvironment{kframe}{%
\medskip{}
\setlength{\fboxsep}{.8em}
\def\at@end@of@kframe{}%
\ifinner\ifhmode%
 \def\at@end@of@kframe{\end{minipage}}%
 \begin{minipage}{\columnwidth}%
\fi\fi%
\def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
\colorbox{incolor}{##1}\hskip-\fboxsep
    % There is no \\@totalrightmargin, so:
    \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
\MakeFramed {\advance\hsize-\width
  \@totalleftmargin\z@ \linewidth\hsize
  \@setminipage}}%
{\par\unskip\endMakeFramed%
\at@end@of@kframe}
\makeatother

\makeatletter
\@ifundefined{Shaded}{
}{\renewenvironment{Shaded}{\begin{kframe}}{\end{kframe}}}
\makeatother

% \let\oldverbatim\verbatim
% \renewenvironment{Shaded}{\vspace{0.2cm}\begin{kframe}}{\end{kframe}}
% \renewenvironment{verbatim}{\begin{shaded}\begin{oldverbatim}}{\end{oldverbatim}\end{shaded}}

\newenvironment{rmdblock}[1]
 {
 \begin{itemize}
 \renewcommand{\labelitemi}{
   \raisebox{-.7\height}[0pt][0pt]{
     {\setkeys{Gin}{width=3em,keepaspectratio}\includegraphics{images/#1}}
   }
 }
 \begin{kframe}
 \setlength{\fboxsep}{1em}
 \item
 }
 {
 \end{kframe}
 \end{itemize}
 }
\newenvironment{rmdnote}
  {\begin{rmdblock}{note}}
  {\end{rmdblock}}
\newenvironment{rmdcaution}
  {\begin{rmdblock}{caution}}
  {\end{rmdblock}}
\newenvironment{rmdimportant}
  {\begin{rmdblock}{important}}
  {\end{rmdblock}}
\newenvironment{rmdtip}
  {\begin{rmdblock}{tip}}
  {\end{rmdblock}}
\newenvironment{rmdwarning}
  {\begin{rmdblock}{warning}}
  {\end{rmdblock}}
\newenvironment{rmdcode}
  {\begin{rmdblock}{screen}}
  {\end{rmdblock}}

\usepackage{makeidx}
\makeindex

\urlstyle{tt}

\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother

% \frontmatter
\usepackage[]{natbib}
\bibliographystyle{plainnat}

\title{BIO4158 Applied biostats with R}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{Laboratory manual}
\author{Julien Martin}
\date{21-09-2021}

\begin{document}
\maketitle

%\cleardoublepage\newpage\thispagestyle{empty}\null
%\cleardoublepage\newpage\thispagestyle{empty}\null
%\cleardoublepage\newpage
%\thispagestyle{empty}
%\begin{center}
%\includegraphics{images/missing.png}
%\end{center}

%\setlength{\abovedisplayskip}{-5pt}
%\setlength{\abovedisplayshortskip}{-5pt}

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{note}{%
\chapter*{Note}\label{note}}
\addcontentsline{toc}{chapter}{Note}

Development version. Lab material will appear slowly during the Fall 2021 term.

\hypertarget{preface}{%
\chapter*{Preface}\label{preface}}
\addcontentsline{toc}{chapter}{Preface}

The laboratory exercises outlined in the following pages are designed to allow you to develop some expertise in using statistical software (R)
to analyze data. R is powerful statistical software but, like all software, it has its limitations. In particular, it is dumb: it cannot think for you,
it cannot tell you whether the analysis you are attempting to do is appropriate or even makes any sense, and it cannot interpret your
results.

\hypertarget{general-points-to-keep-in-mind}{%
\section*{General points to keep in mind}\label{general-points-to-keep-in-mind}}
\addcontentsline{toc}{section}{General points to keep in mind}

\begin{itemize}
\item
  Before attempting any statistical procedure, you must familiarize yourself with what the procedure is actually doing. This does not mean you actually have to know the underlying mathematics (although this certainly helps!), but you should at least understand the principles involved in the analysis. Therefore, before doing a laboratory exercise, read the appropriate section(s) in the lecture notes. Otherwise, the output from your analyses - even if done correctly - will seem like drivel.
\item
  The laboratories are designed to complement the lectures, and vice versa. Owing to scheduling constraints, it may not be possible to synchronize the two perfectly. But feel free to bring questions about the laboratories to class, or questions about the lectures to the labs.
\item
  Work on the laboratories at your own speed: some can be donemuch more quickly than others, and one laboratory need not correspond to one laboratory session. In fact, for some laboratorieswe have allotted two laboratory sessions. Although you will notbe ``graded'' on the laboratories per se, be aware that completingthe labs is essential. If you do not complete the labs, it is veryunlikely that you will be able to complete the assignments and thefinal exam/term paper. So take these laboratories seriously!
\item
  The objective of the first lab is to allow you to acquire or reviewthe minimum knowledge required to complete the following laboratory exercises with R. There are always several methods toaccomplish something in R, but you will only find simple ways inthis manual. Those amongst you that want to go further will easilyfind many examples of more detailed and sophisticated methods.In particular, I point you to the following resources:

  \begin{itemize}
  \tightlist
  \item
    R for beginners
    \url{http://cran.r-project.org/doc/contrib/Paradis-rdebuts_en.pdf}
  \item
    An introduction to R
    \url{http://cran.r-project.org/doc/manuals/R-intro.html}
  \item
    If you prefer paper books, the CRAN web site has a commented list at :
    \url{http://www.r-project.org/doc/bib/R-books.html}
  \item
    Excellent list of R books
    \url{https://www.bigbookofr.com/}
  \item
    R reference card by Tom Short
    \url{http://cran.r-project.org/doc/contrib/Short-refcard.pdf}
  \end{itemize}
\end{itemize}

\hypertarget{what-is-r-and-why-use-it-in-this-course}{%
\section*{What is R and why use it in this course?}\label{what-is-r-and-why-use-it-in-this-course}}
\addcontentsline{toc}{section}{What is R and why use it in this course?}

R is multiplatform free software forming a system for statistical computation and graphics. R is also a programming language specially designed for statistical data analysis. It is a dialect of the S language. S- Plus is another dialect of the S language, very similar to R, incorporated into a commercial package. S-Plus has a built-in graphical design intreface that some find convivial.

R has 2 major advantages for this course. Initially, you will find that it also has one inconvenience. However, this ``inconvenience'' will rapidly force you to acquire very good working habits. So, I see it as a third advantage.

The first advantage is that you can install it freely on you personal computer(s). This is important because it is by doing analyses that you will learn and eventually master biostatistics. This implies that you have easy and unlimited access to a statistical software package. The second advantage is that R can do everything in statistics. R was conceived to be extensible and has become the preferred tool for statisticians around the world. The question is not ``Can R do this?'' but rather ``How can I do this in R?''. And search engine are your friends.

No other software package offers you these two advantages.

The inconvenience of R is that one has to type commands (or copy and paste code) rather than use a menu and select options. If you do not know what command to use, nothing will happen. It is therefore not that easy when you start. However, it is possible to rapidly learn to make basic operations (open a data file, plot data, and run a simple analysis). And once you understand the operating principle, you can easily find examples on the Web for more complex analyses and graphs for which you can adapt the code.

This is exactly what you will do in the first lab to familiarize yourself with R.

Why is this inconvenience really an advantage in my mind? Because this way of doing things is more efficient and will save you time on the long run. I guarantee it. Believe me, you will never do an analysis only once. As you'll proceed through analyses, you will find data entry errors, discover that the analysis must be run separately for subgroups, find extra data, have to rerun the analysis on transformed data, or you will make some analytical error along the way. If you use a graphical interface with menus, redoing an analysis implies that you reclick here, enter values there, select some options, etc. Each of these steps is a potential source of error. If, instead, you use lines of codes, you only have to fix the code and submit to repeat instantaneously the entire analysis. And you can perfectly document what you did, leaving an audit trail for the future. This is how pros work and can document the quality of the results of their analyses.

\hypertarget{software-installation}{%
\section*{Software installation}\label{software-installation}}
\addcontentsline{toc}{section}{Software installation}

\hypertarget{r}{%
\subsection*{R}\label{r}}
\addcontentsline{toc}{subsection}{R}

To install R on a computer, go to \url{http://cran.r-project.org/}. You will find compiled versions (binairies) for your preferred operating system (Windows, MacOS, Linux).

Note : R has already been installed on the lab computers (the version may be slightly different, but this should not matter).

\hypertarget{rstudio-or-vs-code}{%
\subsection*{Rstudio or VS code}\label{rstudio-or-vs-code}}
\addcontentsline{toc}{subsection}{Rstudio or VS code}

RStudio and VS code are integrated development environment software or IDE. RStudio was develop specifically to work with R. VScode is more generela but work extremely well with R. Both are available on Windows, OS X and Linux

\begin{itemize}
\tightlist
\item
  RStudio: \url{https://www.rstudio.com/products/rstudio/download/}
\item
  VScode: \url{https://code.visualstudio.com/download}
\end{itemize}

\hypertarget{r-libraries}{%
\subsection*{R libraries}\label{r-libraries}}
\addcontentsline{toc}{subsection}{R libraries}

R is essentially unlimited in terms of functions that can be used, because is relies on functions packages that can be added as extra components to use in R.

\begin{itemize}
\tightlist
\item
  Rmarkdown
\item
  tinytex
\end{itemize}

Those 2 packages should be installed automatically with RStudio but I recommend to install them manually in case they are not. To do so, just copy-paste the text below in R terminal.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{install.packages}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"rmarkdown"}\NormalTok{, }\StringTok{"tinytex"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\hypertarget{gpower}{%
\subsection*{G*Power}\label{gpower}}
\addcontentsline{toc}{subsection}{G*Power}

G*Power est un programme gratuit, d√©velopp√© par des psychologues de l'Universit√© de Dusseldorf en Allemagne.
Le programme existe en version Mac et Windows.
Il peut cependant √™tre utilis√© sous linux via Wine.
G*Power vous permettra d'effectuer une analyse de puissance pour la majorit√© des tests que nous verrons au cours de la session sans avoir √† effectuer des calculs complexes ou farfouiller dans des tableaux ou des figures d√©crivant des distributions ou des courbes de puissance.

T√©l√©chargez le programme sur le site \url{https://www.psychologie.hhu.de/arbeitsgruppen/allgemeine-psychologie-und-arbeitspsychologie/gpower.html}

\hypertarget{general-laboratory-instructions}{%
\section*{General laboratory instructions}\label{general-laboratory-instructions}}
\addcontentsline{toc}{section}{General laboratory instructions}

\begin{itemize}
\tightlist
\item
  Bring a USB key or equivalent so you can save your work. Alternatively, email your results to yourself.
\item
  Read the lab exercise before coming to the lab. Read the R code and come with questions about the code.
\item
  During pre-labs, listen to the special instructions
\item
  Do the laboratory exercises at your own rhythm, in teams. Then, I
  recommend that you start (complete?) the lab assignment so that you can benefit from the presence of the TA or prof.
\item
  During your analyses, copy and paste results in a separate document, for example in your preferred word processing program. Annotate abundantly
\item
  Each time you shut down R, save the history of your commands (ex: labo1.1 rHistory, labo1.2.rHistory, etc). You will be able to redo the lab rapidly, get code fragments, or more easily identify errors.
\item
  Create your own ``library'' of code fragments (snippets). Annotate
  it abundantly. You will thank yourself later.
\end{itemize}

\hypertarget{notes-about-the-manual}{%
\section*{Notes about the manual}\label{notes-about-the-manual}}
\addcontentsline{toc}{section}{Notes about the manual}

You will find explanations on the theory, R code and functions, IDE best practice and exercises with R.

The manual tries to highlight some part of the text using the following boxes and icons.

\begin{rmdcode}
Exercises,
\end{rmdcode}

\begin{rmdcaution}
warnings,
\end{rmdcaution}

\begin{rmdwarning}
warnings,
\end{rmdwarning}

\begin{rmdimportant}
important points
\end{rmdimportant}

\begin{rmdnote}
notes
\end{rmdnote}

\begin{rmdtip}
and tips
\end{rmdtip}
\#\#\# Resources \{-\}

This document was developped using the excellent \href{https://bookdown.org/}{bookdown} üì¶ de \href{https://yihui.name/}{Yihui Xie}. The manual is based on the previous lab manual \emph{Findlay, Morin and Rundle, BIO4158 Laboratory manual for BIO4158}.

\hypertarget{license}{%
\subsection*{License}\label{license}}
\addcontentsline{toc}{subsection}{License}

The document is available follwoing the license \href{http://creativecommons.org/licenses/by-nc-sa/4.0/}{License Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International}.

\begin{figure}
\centering
\includegraphics{images/icons/license_cc.png}
\caption{License Creative Commons}
\end{figure}

\hypertarget{introduction-to-r}{%
\chapter{Introduction to R}\label{introduction-to-r}}

After completing this laboratory exercise, you should be able to:

\begin{itemize}
\tightlist
\item
  Open R data files
\item
  Import rectangular data sets
\item
  Export R data to text files
\item
  Verify that data were imported correctly
\item
  Examine the distribution of a variable
\item
  Examine visually and test for normality of a variable
\item
  Calculate descriptive statistics for a variable
\item
  Transform data
\end{itemize}

\hypertarget{set-intro}{%
\section{Packages and data needed for the lab}\label{set-intro}}

This labs needs the following:

\begin{itemize}
\tightlist
\item
  R packages:

  \begin{itemize}
  \tightlist
  \item
    ggplot2
  \end{itemize}
\item
  data files

  \begin{itemize}
  \tightlist
  \item
    ErablesGatineau.csv
  \item
    sturgeon.csv
  \end{itemize}
\end{itemize}

\hypertarget{importing-and-exporting-data}{%
\section{Importing and exporting data}\label{importing-and-exporting-data}}

There are multiple format to save data. The 2 most used formats with R are \texttt{.csv} and \texttt{.Rdata}.

\begin{itemize}
\tightlist
\item
  \texttt{.csv} files are used to store data in a simple format and are editable using any text editor (e.g.~Word, Writer, atom, \ldots) and spreadsheets (e.g.~MS Excel, LO Calc).
  They can be read using the function \texttt{read.csv()} and created in R with \texttt{write.csv()}.
\item
  \texttt{.Rdata} files are used to store not only data but any R object, however, those files can only be used in R. They are created using the \texttt{save()} function and read using the \texttt{load()} function.
\end{itemize}

Data for exercises and labs are provides in \texttt{.csv}.

\hypertarget{working-directory}{%
\subsection{Working directory}\label{working-directory}}

\begin{rmdwarning}
Potentially the most frequent error when starting with R is link to loading data or reading data from an external file in R.
\end{rmdwarning}

A typical error message is:

\begin{verbatim}
Error in file(file, "rt") : cannot open the connection
In addition: Warning message:
In file(file, "rt") :
  cannot open file 'ou_est_mon_fichier.csv': No such file or directory
\end{verbatim}

This type of error simply means that R cannot find the file you specified. By default, when R starts, a folder is define as the based folder for R. This is the working directory. R by default will save any files in this folder and will start looking for files in this folder. So you need to specify to R where to look for files and where to save your files. This can be done in 3 different ways:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{file.choose()}. (not recommended, because not reproducible). This function will open a dialog box allowing you to click on the file you want. This is not recommended and can be long because you will have to do it absolutely every time you use R.
\item
  specify the complete path in the function. For example \texttt{read.csv("/home/julien/Documents/cours/BIO4558/labo/data/monfichier.csv")}. This is longer to type the first time and a bit tricky to get the correct path but after you can run the line of code and it works every time without trying to remember were you saved that damned file. However, this is specific to your own computer and would not work elsewhere.
\item
  specify a working directory with \texttt{setwd()}. This simplify tells R where to look for files and where to save files. (This is automatically done when using .Rmd files). Just set the working directory to where you want and after that all path will be relative to this working directory. The big advantage is that if you keep a similar folder structure for you R project it will be compatible and reproducible across all computer and OS
\end{enumerate}

To know which folder is the workind directory simply type \texttt{getwd()}

\begin{rmdtip}
When opening Rstudio by double-clicking on a file, it will automatically set the working irectory to the folder where this file is located. This can be super handy.
\end{rmdtip}

\begin{rmdimportant}
For all labs, I strongly recommend you to make a folder where you will save all your R scripts and data and use it as your working directory in R. For better organisation I suggest to save your data in a subfolder named \texttt{data\ All\ R\ code\ for\ data\ loading\ in\ the\ manual\ is\ based\ on\ that\ structure.\ This\ is\ why\ dat\ loading\ or\ saving\ code\ look\ like}data/my\_file.xxx`. If you follow it also all code for data loading can be simply copy-pasted and should work.
\end{rmdimportant}

\hypertarget{opening-a-.rdata-file}{%
\subsection{\texorpdfstring{Opening a \texttt{.Rdata} file}{Opening a .Rdata file}}\label{opening-a-.rdata-file}}

You can double-click on the file and R/Rstudio should open. Alternatively, you can use \texttt{load()} function and specify the names (and path) of the file. For example to load the data \texttt{ErablesGatineau.Rdata} in R which is located in the folder \texttt{data} in the working directory you can use:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{load}\NormalTok{(}\StringTok{"data/ErablesGatineau.Rdata"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{open-a-.csv-file}{%
\subsection{\texorpdfstring{Open a \texttt{.csv} file}{Open a .csv file}}\label{open-a-.csv-file}}

To import data saved in a \texttt{.csv} file, you need to use the \texttt{read.csv()} function.
For example, to create a R object named \texttt{erables} which contain the data from the file \texttt{ErablesGatineau.csv}, you need to use:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{erables \textless{}{-}}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"data/ErablesGatineau.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{rmdwarning}
Beware of the coma. If you are working in adifferent language (other than english), be careful because the decimal symbol might ot be the same.
By default R use the point for the decimal sign. If the dat use the coma for the decimal then R would not be able to read the file correctly. In this case you can use \texttt{read.csv2()} or \texttt{read.data()} which should solve the problem.
\end{rmdwarning}

To verify that the data were read and loaded properly, you can list all objects in memory with the \texttt{ls()} function, or get a more detailed description with \texttt{ls.str()}:

\begin{rmdtip}
I do not recommend to use \texttt{ls.str()} since it can produce really long R ouputs when you have multiple R object loaded.
I suggest instead to use the combination of \texttt{ls()} to get the list of all R objects and then \texttt{str()} only for the objects you want to look at.
\end{rmdtip}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ls}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "erables" "params"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(erables)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 'data.frame':    100 obs. of  3 variables:
##  $ station: chr  "A" "A" "A" "A" ...
##  $ diam   : num  22.4 36.1 44.4 24.6 17.7 ...
##  $ biom   : num  732 1171 673 1552 504 ...
\end{verbatim}

R confirms that the object \texttt{erables}.
\texttt{erables} is a data.frame that contains 100 observations (lines) of 3 variables (columns) : \texttt{station} , a variable of type Factor with 2
levels, and \texttt{diam} and \texttt{biom} that are 2 numeric variables.

\hypertarget{entering-data-in-r}{%
\subsection{Entering data in R}\label{entering-data-in-r}}

R is not the ideal environment to input data. It is possible, but the syntax is heavy and makes most people upset. Use your preferred worksheet program instead. It will be more efficient and less frustrating.

\hypertarget{cleaning-up-correcting-data}{%
\subsection{Cleaning up / correcting data}\label{cleaning-up-correcting-data}}

Another operation that can be frustrating in R. Our advice: unless you want to keep track of all corrections made (so that you can go back to the original data), do not change data in R. Return to the original data file (in a worksheet or database), correct the data there, and then reimport into R. It is simple to resubmit the few lines of code to reimport data. Doing things this way will leave you with a single version of your data file that has all corrections, and the code that allows you to repeat the analysis exactly.

\hypertarget{exporting-data-from-r}{%
\subsection{Exporting data from R}\label{exporting-data-from-r}}

You have 2 options: export data in \texttt{.csv} or in \texttt{.Rdata}

To export in \texttt{.Rdata} use the function \texttt{save()} to export in \texttt{.csv} use \texttt{write.csv()}

For example, to save teh object \texttt{mydata} in a file \texttt{wonderful\_data.csv}that will be saved in your working directory you can type:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{write.csv}\NormalTok{(mydata, }\DataTypeTok{file =} \StringTok{"wonderful\_data.csv"}\NormalTok{, }\DataTypeTok{row.names =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{preliminary-examination-of-data}{%
\section{Preliminary examination of data}\label{preliminary-examination-of-data}}

The first step of data analysis is to examine the data at hand. This examination will tell you if the data were correctly imported, whether the numbers are credible, whether all data came in, etc. This initial data examination often will allow you to detect unlikely observations, possibly due to errors at the data entry stage. Finally, the initial plotting of the data will allow you to visualize the major trends that will be confirmed later by your statistical analysis.

The file \texttt{sturgeon.csv} contains data on sturgeons from the Saskatchewan River. These data were collected to examine how sturgeon size varies among sexes ( \texttt{sex} ), sites ( \texttt{location} ), and years( \texttt{year} ).

\begin{itemize}
\tightlist
\item
  Load the data from \texttt{sturgeon.csv} in a R object named \texttt{sturgeon}.
\item
  use the function \texttt{str()} to check that the data was loaded and read correctly.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sturgeon \textless{}{-}}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"data/sturgeon.csv"}\NormalTok{)}
\KeywordTok{str}\NormalTok{(sturgeon)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 'data.frame':    186 obs. of  9 variables:
##  $ fklngth : num  37 50.2 28.9 50.2 45.6 ...
##  $ totlngth: num  40.7 54.1 31.3 53.1 49.5 ...
##  $ drlngth : num  23.6 31.5 17.3 32.3 32.1 ...
##  $ rdwght  : num  15.95 NA 6.49 NA 29.92 ...
##  $ age     : int  11 24 7 23 20 23 20 7 23 19 ...
##  $ girth   : num  40.5 53.5 31 52.5 50 54.2 48 28.5 44 39 ...
##  $ sex     : chr  "MALE" "FEMALE" "MALE" "FEMALE" ...
##  $ location: chr  "THE_PAS" "THE_PAS" "THE_PAS" "THE_PAS" ...
##  $ year    : int  1978 1978 1978 1978 1978 1978 1978 1978 1978 1978 ...
\end{verbatim}

\hypertarget{summary-statistics}{%
\subsection{Summary statistics}\label{summary-statistics}}

To get summary statistics on the contents of the data frame sturgeon, type the command:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(sturgeon)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     fklngth         totlngth        drlngth          rdwght     
##  Min.   :24.96   Min.   :28.15   Min.   :14.33   Min.   : 4.73  
##  1st Qu.:41.00   1st Qu.:43.66   1st Qu.:25.00   1st Qu.:18.09  
##  Median :44.06   Median :47.32   Median :27.00   Median :23.10  
##  Mean   :44.15   Mean   :47.45   Mean   :27.29   Mean   :24.87  
##  3rd Qu.:48.00   3rd Qu.:51.97   3rd Qu.:29.72   3rd Qu.:30.27  
##  Max.   :66.85   Max.   :72.05   Max.   :41.93   Max.   :93.72  
##                  NA's   :85      NA's   :13      NA's   :4      
##       age            girth           sex              location        
##  Min.   : 7.00   Min.   :11.50   Length:186         Length:186        
##  1st Qu.:17.00   1st Qu.:40.00   Class :character   Class :character  
##  Median :20.00   Median :44.00   Mode  :character   Mode  :character  
##  Mean   :20.24   Mean   :44.33                                        
##  3rd Qu.:23.50   3rd Qu.:48.80                                        
##  Max.   :55.00   Max.   :73.70                                        
##  NA's   :11      NA's   :85                                           
##       year     
##  Min.   :1978  
##  1st Qu.:1979  
##  Median :1979  
##  Mean   :1979  
##  3rd Qu.:1980  
##  Max.   :1980  
## 
\end{verbatim}

For each variable, R lists:

\begin{itemize}
\tightlist
\item
  the minimum
\item
  the maximum
\item
  the median that is the \(50^{th}\) percentile, here the \(93^{rd}\) value of the 186 observations ordered in ascending order
\item
  values at the first (25\%) and third quartile (75\%)
\item
  the number of missing values in the column.
\end{itemize}

Note that several variables have missing values (NA). Only the variables \texttt{fklngth} (fork length), \texttt{sex} , \texttt{location} , and \texttt{year} have 186 observations.

\begin{rmdwarning}
\textbf{Beware of missing values}

Several R functions are sensitive to missing values and you will frequently have to do your analyses on data subsets without missing data, or by using optional parameters in various commands. We will get back to this, but you should always pay attention and take note of missing data when you do analyses.
\end{rmdwarning}

\hypertarget{histogram-empirical-probability-density-boxplot-and-visual-assessment-of-normality}{%
\subsection{Histogram, empirical probability density, boxplot, and visual assessment of normality}\label{histogram-empirical-probability-density-boxplot-and-visual-assessment-of-normality}}

Let's look more closely at the distribution of \texttt{fklngth}.
The command \texttt{hist()} will create a histogram. For the histogram of \texttt{fklngth} in the \texttt{sturgeon} data frame, type the command:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{hist}\NormalTok{(sturgeon}\OperatorTok{$}\NormalTok{fklngth)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{Labs_BIO4158_files/figure-latex/hist-stur-1.pdf}
\caption{\label{fig:hist-stur}Histogram of fluke length of sturgeons}
\end{figure}

The data appear to be approximately normal. This is good to know.

\begin{rmdnote}
Note that this syntax is a bit heavy as you need to prefix variable names by the data frame name \texttt{sturgeon\$}. You can lighten the syntax by making the variables directly accessible by commands by typing the command \texttt{attach()}.
However, I \textbf{strongly recommend not to use} it because it can lead to many problems hard to detect compare to the little benfit is provides
\end{rmdnote}

This histogram (Fig. \ref{fig:hist-stur}) is a very classical representation of the distribution. Histograms are not perfect however because their shape partly depends on the number of bins used, more so for small samples. One can do better, especially if you want to visually compare the observed distribution to a normal distribution. But you need to come up with a bit of extra R code based on the \texttt{ggplot2} üì¶.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\# load ggplot2 if needed}
\KeywordTok{library}\NormalTok{(ggplot2)}

\CommentTok{\#\# use "sturgeon" dataframe to make plot called mygraph}
\CommentTok{\# and define x axis as representing fklngth}
\NormalTok{mygraph \textless{}{-}}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ sturgeon, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ fklngth))}

\CommentTok{\#\# add data to the mygraph ggplot}
\NormalTok{mygraph \textless{}{-}}\StringTok{ }\NormalTok{mygraph }\OperatorTok{+}
\StringTok{  }\CommentTok{\#\# add semitransparent histogram}
\StringTok{  }\KeywordTok{geom\_histogram}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ ..density..),}
    \DataTypeTok{bins =} \DecValTok{30}\NormalTok{, }\DataTypeTok{color =} \StringTok{"black"}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.3}
\NormalTok{  ) }\OperatorTok{+}
\StringTok{  }\CommentTok{\#\#  add density smooth}
\StringTok{  }\KeywordTok{geom\_density}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\CommentTok{\#\# add observations positions or rug bars}
\StringTok{  }\KeywordTok{geom\_rug}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\CommentTok{\#\# add Gaussian curve adjusted to the data with mean and sd from fklngth}
\StringTok{  }\KeywordTok{stat\_function}\NormalTok{(}
    \DataTypeTok{fun =}\NormalTok{ dnorm,}
    \DataTypeTok{args =} \KeywordTok{list}\NormalTok{(}
      \DataTypeTok{mean =} \KeywordTok{mean}\NormalTok{(sturgeon}\OperatorTok{$}\NormalTok{fklngth),}
      \DataTypeTok{sd =} \KeywordTok{sd}\NormalTok{(sturgeon}\OperatorTok{$}\NormalTok{fklngth)}
\NormalTok{    ),}
    \DataTypeTok{color =} \StringTok{"red"}
\NormalTok{  )}

\CommentTok{\#\# display graph}
\NormalTok{mygraph}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{Labs_BIO4158_files/figure-latex/stur-g1-1.pdf}
\caption{\label{fig:stur-g1}Distribution of fluke length in sturgeon plotted with ggplot}
\end{figure}

Each observation is represented by a short vertical bar below the x- axis (rug). The red line is the normal distribution with the same mean and standard deviation as the data. The other line is the empirical distribution, smoothed from the observations.

The ggplot object you just created (\texttt{mygraph}) can be further manipulated. For example, you can plot the distribution of \texttt{fklngth} per \texttt{sex} and \texttt{year} groups simply by adding a \texttt{facet\_grid()} statement:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mygraph }\OperatorTok{+}\StringTok{ }\KeywordTok{facet\_grid}\NormalTok{(year }\OperatorTok{\textasciitilde{}}\StringTok{ }\NormalTok{sex)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Labs_BIO4158_files/figure-latex/aventure-1.pdf}

Each panel contains the data distribution for one sex that year, and the recurring red curve is the normal distribution for the entire data set. It can serve as a reference to help visually evaluate differences among panels.

Another way to visually assess normality of data is the QQ plot that is
obtained by the pair of commands \texttt{qqnorm()} and \texttt{qqline()}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{qqnorm}\NormalTok{(sturgeon}\OperatorTok{$}\NormalTok{fklngth)}
\KeywordTok{qqline}\NormalTok{(sturgeon}\OperatorTok{$}\NormalTok{fklngth)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Labs_BIO4158_files/figure-latex/stur-norm-1.pdf}
Perfectly normal data would follow the straight diagonal line. Here there are deviations in the tails of the distribution and a bit to the right of the center.
Compare this representation to the two preceding graphs.
You will probably agree that it is easier to visualize how data deviate from normality by looking at a histogram of an empirical probability density than by looking at the QQ plots.
However, QQ plots are often automatically produced by various statistical routines and you should be able to interpret them.
In addition, one can easily run a formal test of normality in R with the command \texttt{shapiro.test()} that computes a statistic (\texttt{W}) that measures how tightly data fall around the straight diagonal line of the QQ plot. If data fall perfectly on the line, then \texttt{W\ =\ 1}. If \texttt{W} is much less than 1, then data are not normal.

For the \texttt{fklngth} data:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{shapiro.test}\NormalTok{(sturgeon}\OperatorTok{$}\NormalTok{fklngth)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Shapiro-Wilk normality test
## 
## data:  sturgeon$fklngth
## W = 0.97225, p-value = 0.0009285
\end{verbatim}

W is close to 1, but far enough to indicate a statistically significant deviation from normality.

Visual examination of very large data sets is often made difficult by the superposition of data points. Boxplots are an interesting alternative.
The command \texttt{boxplot(fklngth\textasciitilde{}sex,\ notch=TRUE)} produces a boxplot of \texttt{fklngth} for each \texttt{sex} , and adds whiskers.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{boxplot}\NormalTok{(fklngth }\OperatorTok{\textasciitilde{}}\StringTok{ }\NormalTok{sex, }\DataTypeTok{data =}\NormalTok{ sturgeon, }\DataTypeTok{notch =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{Labs_BIO4158_files/figure-latex/boxplot-stur-1.pdf}
\caption{\label{fig:boxplot-stur}Boxplot of fluke length in strugeon by sex}
\end{figure}

The slightly thicker line inside the box of figure \ref{fig:boxplot-stur} indicates the median.
The width of the notch is proportional to the uncertainty around the median estimate.
One can visually assess the approximate statistical significance of differences among medians by looking at the overlap of the notches (here there is no overlap and one could tentatively conclude that the median female size is larger than the median male size).
Boxes extend from the first to third quartile (the 25\textsuperscript{th} to 75\textsuperscript{th} percentile if you prefer).
Bars (whiskers) extend above and below the boxes from the minimum to the maximum observed value or, if there are extreme values, from the smallest to the largest observed value within 1.5x the interquartile range from the median. Observations exceeding the limits of the whiskers (hence further away from the median than 1.5x the interquartile range, the range between the 25\textsuperscript{th} and 75\textsuperscript{th} percentile) are plotted as circles. These are outliers, possibly aberrant data.

\hypertarget{scatterplots}{%
\subsection{Scatterplots}\label{scatterplots}}

In addition to histograms and other univariate plots, it is often informative to examine scatter plots.
The command \texttt{plot(y\textasciitilde{}x)} produces a scatter plot of y on the vertical axis (the ordinate) vs x on the horizontal axis (abscissa).

\begin{rmdcode}
Create a scatterplot of fklngth vs age using the plot() command.
\end{rmdcode}

You should obtain:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(fklngth }\OperatorTok{\textasciitilde{}}\StringTok{ }\NormalTok{age, }\DataTypeTok{data =}\NormalTok{ sturgeon)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Labs_BIO4158_files/figure-latex/stur-biv-plot-1.pdf}

R has a function to create all pairwise scatterplots rapidly called
\texttt{pairs()} . One of \texttt{pairs()} options is the addition of a lowess trace on
each plot to that is a smoothed trend in the data.
To get the plot matrix with the lowess smooth for all variables in the
sturgeon data frame, execute the command
\texttt{pairs(sturgeon,\ panel=panel.smooth)}. Howeber given the large number of variable in \texttt{sturgeon} we can limit the plot to the first 6 columns in the data.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pairs}\NormalTok{(sturgeon[, }\DecValTok{1}\OperatorTok{:}\DecValTok{6}\NormalTok{], }\DataTypeTok{panel =}\NormalTok{ panel.smooth)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Labs_BIO4158_files/figure-latex/pairs-stur-1.pdf}

\hypertarget{creating-data-subsets}{%
\section{Creating data subsets}\label{creating-data-subsets}}

You will frequently want to do analyses on some subset of your data.
The command \texttt{subset()} is what you need to isolate cases meeting some criteria.
For example, to create a subset of the sturgeon data frame that contains only females caught in 1978, you could write:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sturgeon\_female\_}\DecValTok{1978}\NormalTok{ \textless{}{-}}\StringTok{ }\KeywordTok{subset}\NormalTok{(sturgeon, sex }\OperatorTok{==}\StringTok{ "FEMALE"} \OperatorTok{\&}\StringTok{ }\NormalTok{year }\OperatorTok{==}\StringTok{ "1978"}\NormalTok{)}
\NormalTok{sturgeon\_female\_}\DecValTok{1978}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      fklngth totlngth  drlngth rdwght age girth    sex   location year
## 2   50.19685 54.13386 31.49606     NA  24  53.5 FEMALE    THE_PAS 1978
## 4   50.19685 53.14961 32.28346     NA  23  52.5 FEMALE    THE_PAS 1978
## 6   49.60630 53.93701 31.10236  35.86  23  54.2 FEMALE    THE_PAS 1978
## 7   47.71654 51.37795 33.97638  33.88  20  48.0 FEMALE    THE_PAS 1978
## 15  48.89764 53.93701 29.92126  35.86  23  52.5 FEMALE    THE_PAS 1978
## 105 46.85039       NA 28.34646  23.90  24    NA FEMALE CUMBERLAND 1978
## 106 40.74803       NA 24.80315  17.50  18    NA FEMALE CUMBERLAND 1978
## 107 40.35433       NA 25.59055  20.90  21    NA FEMALE CUMBERLAND 1978
## 109 43.30709       NA 27.95276  24.10  19    NA FEMALE CUMBERLAND 1978
## 113 53.54331       NA 33.85827  48.90  20    NA FEMALE CUMBERLAND 1978
## 114 51.77165       NA 31.49606  35.30  26    NA FEMALE CUMBERLAND 1978
## 116 45.27559       NA 26.57480  23.70  24    NA FEMALE CUMBERLAND 1978
## 118 53.14961       NA 32.67717  45.30  25    NA FEMALE CUMBERLAND 1978
## 119 50.19685       NA 32.08661  33.90  26    NA FEMALE CUMBERLAND 1978
## 123 49.01575       NA 29.13386  37.50  22    NA FEMALE CUMBERLAND 1978
\end{verbatim}

\begin{rmdcaution}
When using criteria to select cases, be careful of the \texttt{==} syntax to mean equal to.
In this context, if you use a single \texttt{=}, you will not get what you want.
The following table lists the most common criteria to create expressions and their R syntax.
\end{rmdcaution}

\begin{longtable}[]{@{}llll@{}}
\toprule
Operateur & Explication & Operateur & Explication\tabularnewline
\midrule
\endhead
== & Equal to & != & Not equal to\tabularnewline
\textgreater{} & Larger than & \textless{} & Lower than\tabularnewline
\textgreater= & Larger than or equal to & \textless= & Lower than or equal to\tabularnewline
\& & And (vectorized) & \textbar{} & Or (vectorized)\tabularnewline
\&\& & And (control) & \textbar\textbar{} & Or (control)\tabularnewline
! & Not & &\tabularnewline
\bottomrule
\end{longtable}

\begin{rmdcode}
Using the commands \texttt{subset()} and \texttt{hist()} , create a histogram for females caught in 1979 and 1980 (hint: \texttt{sex=="FEMALE"\ \&\ (year\ =="1979"\ \textbar{}\ year=="1980")})
\end{rmdcode}

\begin{figure}
\centering
\includegraphics{Labs_BIO4158_files/figure-latex/intror-subex-1.pdf}
\caption{\label{fig:intror-subex}Distibution of fluke length of female sturgeons in 1979 and 1980}
\end{figure}

\hypertarget{data-transformation}{%
\section{Data transformation}\label{data-transformation}}

You will frequently transform raw data to better satisfy assumptions of statistical tests. R will allow you to do that easily.
The most used functions are probably:

\begin{itemize}
\tightlist
\item
  \texttt{log()}
\item
  \texttt{sqrt()}
\item
  \texttt{ifelse()}
\end{itemize}

You can use these functions directly within commands, create vector variables, or add columns in data frames.
To do a plot of the decimal log of fklngth vs age, you can simply use the \texttt{log10()} function within the plot command:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(}\KeywordTok{log10}\NormalTok{(fklngth)}\OperatorTok{\textasciitilde{}}\NormalTok{age, }\DataTypeTok{data =}\NormalTok{ sturgeon)}
\end{Highlighting}
\end{Shaded}

To create a vector variable, an orphan variable if you wish, one that is not part of a data frame, called \texttt{lfklngth} and corresponding too the decimal log of \texttt{fklngth}, simply enter:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{logfklngth \textless{}{-}}\StringTok{ }\KeywordTok{log10}\NormalTok{(sturgeon}\OperatorTok{$}\NormalTok{fklngth)}
\end{Highlighting}
\end{Shaded}

If you want this new variable to be added to a data frame, then you must prefix the variable name by the data frame name and the \texttt{\$} symbol.
For example to add the variable \texttt{lfkl} containing the decimal log of \texttt{fklngth} to the \texttt{sturgeon} data frame, enter:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sturgeon}\OperatorTok{$}\NormalTok{lfkl \textless{}{-}}\StringTok{ }\KeywordTok{log10}\NormalTok{(sturgeon}\OperatorTok{$}\NormalTok{fklngth)}
\end{Highlighting}
\end{Shaded}

\texttt{lfkl} will be added to the data frame \texttt{sturgeon} for the R session.
Do not forget to save the modified data frame if you want to keep the modified version. Or better, save you Rscript and do not forget to run the line of code again next time you need it.

For conditional transformations, you can use the function \texttt{ifelse()}.
For example, to create a new variable called dummy with a value of 1 for males and 0 for females, you can use:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sturgeon}\OperatorTok{$}\NormalTok{dummy \textless{}{-}}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(sturgeon}\OperatorTok{$}\NormalTok{sex }\OperatorTok{==}\StringTok{ "MALE"}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{exercice}{%
\section{Exercice}\label{exercice}}

The file \texttt{salmonella.csv} contains numerical values for the variable
called ratio for two environments (\texttt{milieu}: \texttt{IN\ VITRO} or \texttt{IN\ VIVO})
and for 3 strains (\texttt{souche}).
Examine the ratio variable and make a graph to visually assess normality for the wild (SAUVAGE) strain.

\begin{figure}
\centering
\includegraphics{Labs_BIO4158_files/figure-latex/intror-exer-1.pdf}
\caption{\label{fig:intror-exer}Distibution of infection ratios by the wild (SAUVAGE) strain of salmonella}
\end{figure}

\hypertarget{power-analysis-with-r-and-gpower}{%
\chapter{Power Analysis with R and G*Power}\label{power-analysis-with-r-and-gpower}}

After completing this laboratory, you should :

\begin{itemize}
\tightlist
\item
  be able to compute the power of a t-test with G*Power and R
\item
  be able to calculate the required sample size to achieve a desired power level with a t-test
\item
  be able to calculate the detectable effect size by a t-test given the sample size, the power and \(\alpha\)
\item
  understand how power changes when sample size increases, the effect size changes, or when \(\alpha\) decreases
\item
  understand how power is affected when you change from a two-tailed to a one-tailed test.
\end{itemize}

\hypertarget{the-theory}{%
\section{The theory}\label{the-theory}}

\hypertarget{what-is-power}{%
\subsection{What is power?}\label{what-is-power}}

\emph{Power is the probability of rejecting the null hypothesis when it is false}

\hypertarget{why-do-a-power-analysis}{%
\subsection{Why do a power analysis?}\label{why-do-a-power-analysis}}

\hypertarget{assess-the-strength-of-evidence}{%
\subsubsection*{Assess the strength of evidence}\label{assess-the-strength-of-evidence}}
\addcontentsline{toc}{subsubsection}{Assess the strength of evidence}

Power analysis, performed after accepting a null hypothesis, can help assess the probability of rejecting the null if it were false, and if the magnitude of the effect was equal to that observed (or to any other given magnitude). This type of \emph{a posteriori} analysis is very common.

\hypertarget{design-better-experiments}{%
\subsubsection*{Design better experiments}\label{design-better-experiments}}
\addcontentsline{toc}{subsubsection}{Design better experiments}

Power analysis, performed prior to conducting an experiment (but most often after a preliminary experiment), can be used to determine the number of observations required to detect an effect of a given magnitude with some probability (the power). This type of \emph{a priori} experiment should be more common.

\hypertarget{estimate-minimum-detectable-effect}{%
\subsubsection*{Estimate minimum detectable effect}\label{estimate-minimum-detectable-effect}}
\addcontentsline{toc}{subsubsection}{Estimate minimum detectable effect}

Sampling effort is often predetermined (when you are handed data of an experiment already completed), or extremely constrained (when logistics dictates what can be done). Whether it is \emph{a priori} or \emph{a posteriori}, power analysis can help you estimate, for a fixed sample size and a given power, what is the minimum effect size that can be detected.

\hypertarget{factors-affecting-power}{%
\subsection{Factors affecting power}\label{factors-affecting-power}}

For a given statistical test, there are 3 factors that affect power.

\hypertarget{decision-criteria}{%
\subsubsection*{Decision criteria}\label{decision-criteria}}
\addcontentsline{toc}{subsubsection}{Decision criteria}

Power is related to \(\alpha\), the probability level at which one rejects the null hypothesis. If this decision criteria is made very strict (i.e.~if critical \(\alpha\) is set to a very low value, like 0.1\% or \(p = 0.001\)), then power will be lower than if the critical \(\alpha\) was less strict.

\hypertarget{sample-size}{%
\subsubsection*{Sample size}\label{sample-size}}
\addcontentsline{toc}{subsubsection}{Sample size}

The larger the sample size, the larger the power. As sample size increases, one's ability to detect small effect sizes as being statistically significant gets better.

\hypertarget{effect-sizze}{%
\subsubsection*{Effect sizze}\label{effect-sizze}}
\addcontentsline{toc}{subsubsection}{Effect sizze}

The larger the effect size, the larger the power. For a given sample size, the ability to detect an effect as being significant is higher for large effects than for small ones. Effect size measures how false the null hypothesis is.

\hypertarget{what-is-gpower}{%
\section{What is G*Power?}\label{what-is-gpower}}

G*Power is free software developed by quantitative psychologists from the University of Dusseldorf in Germany.
It is available in MacOS and Windows versions.
It can be run under Linux using Wine or a virtual machine.

G*Power will allow you to do power analyses for the majority of statistical tests we will cover during the term without making lengthy calculations and looking up long tables and figures of power curves. It is a really useful tool that you need to master.

It is possible to perform all analysis made by G*Power in R, but it requires a bit more code, and a better understanding of the process since everything should be coded by hand. In simple cases, R code is also provided.

\begin{rmdcode}
Download the software \textbf{\href{https://www.psychologie.hhu.de/arbeitsgruppen/allgemeine-psychologie-und-arbeitspsychologie/gpower.html}{here}} and install it on your computer and your workstation (if it is not there already).
\end{rmdcode}

\hypertarget{how-to-use-gpower}{%
\section{How to use G*Power}\label{how-to-use-gpower}}

\hypertarget{general-principle}{%
\subsection{General Principle}\label{general-principle}}

Using G*Power generally involves 3 steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Choosing the appropriate test
\item
  Choosing one of the 5 types of available power analyses
\item
  Enter parameter values and press the \textbf{Calculate} button
\end{enumerate}

\hypertarget{types-of-power-analyses}{%
\subsection{Types of power analyses}\label{types-of-power-analyses}}

First, \(\alpha\) is define as the probability level at which
one rejects the null hypothesis, and \(\beta\) is \(1 - power\).

\hypertarget{a-priori}{%
\subsubsection*{A priori}\label{a-priori}}
\addcontentsline{toc}{subsubsection}{A priori}

Computes the sample size required given \(\beta\), \(\alpha\), and the effect size. This type of analysis is useful when planning experiments.

\hypertarget{compromise}{%
\subsubsection*{Compromise}\label{compromise}}
\addcontentsline{toc}{subsubsection}{Compromise}

Computes \(\alpha\) and \(\beta\) for a given \(\alpha\)/\(\beta\) ratio, sample size, and effect size. Less commonly used (I have never used it myself) although it can be useful when the \(\alpha\)/\(\beta\) ratio has meaning, for example when the cost of type I and type II errors can be quantified.

\hypertarget{criterion}{%
\subsubsection*{Criterion}\label{criterion}}
\addcontentsline{toc}{subsubsection}{Criterion}

Computes \(\alpha\) for a given \(\beta\), sample size, and effect size. In practice, I see little interest in this. Let me know if you see something I don't!

\hypertarget{post-hoc}{%
\subsubsection*{Post-hoc}\label{post-hoc}}
\addcontentsline{toc}{subsubsection}{Post-hoc}

Computes the power for a given \(\alpha\), effect size, and
sample size. Used frequently to help in the interpretation of a test
that is not statistically significant, but only if an effect size that is
biologically significant is used (and not the observed effect size). Not
relevant when the test is significant.
Sensitivity. Computes the detectable effect size for a given \(\beta\) ,\(\alpha\), and
sample size. Very useful at the planning stage of an experiment.

\hypertarget{how-to-calculate-effect-size}{%
\subsection{How to calculate effect size}\label{how-to-calculate-effect-size}}

G*Power can perform power analyses for several statistical tests.
The metric for effect size depends on the test. Note that other
software packages often use different effect size metrics and that it is
important to use the correct one for each package. G\emph{Power has an
effect size calculator for many tests that only requires you to enter
the relevant values. The following table lists the effect size metrics
used by G}Power for the various tests.

\begin{longtable}[]{@{}lcl@{}}
\toprule
\begin{minipage}[b]{0.11\columnwidth}\raggedright
Test\strut
\end{minipage} & \begin{minipage}[b]{0.35\columnwidth}\centering
Taille d'effet\strut
\end{minipage} & \begin{minipage}[b]{0.46\columnwidth}\raggedright
Formule\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.11\columnwidth}\raggedright
t-test on means\strut
\end{minipage} & \begin{minipage}[t]{0.35\columnwidth}\centering
d\strut
\end{minipage} & \begin{minipage}[t]{0.46\columnwidth}\raggedright
\(d = \frac{|\mu_1 - \mu_2|}{\sqrt{({s_1}^2 + {s_2}^2)/2}}\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.11\columnwidth}\raggedright
t-test on correlations\strut
\end{minipage} & \begin{minipage}[t]{0.35\columnwidth}\centering
r\strut
\end{minipage} & \begin{minipage}[t]{0.46\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.11\columnwidth}\raggedright
other t-tests\strut
\end{minipage} & \begin{minipage}[t]{0.35\columnwidth}\centering
f\strut
\end{minipage} & \begin{minipage}[t]{0.46\columnwidth}\raggedright
\(f = \frac{\mu_1}{\sigma}\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.11\columnwidth}\raggedright
F-test (ANOVA)\strut
\end{minipage} & \begin{minipage}[t]{0.35\columnwidth}\centering
f\strut
\end{minipage} & \begin{minipage}[t]{0.46\columnwidth}\raggedright
\(f = \frac{\frac{\sqrt{\sum_{i=1}^k (\mu_i - \mu)^2}}{k}}{\sigma}\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.11\columnwidth}\raggedright
other F-tests\strut
\end{minipage} & \begin{minipage}[t]{0.35\columnwidth}\centering
\(f^2\)\strut
\end{minipage} & \begin{minipage}[t]{0.46\columnwidth}\raggedright
\(f^2 = \frac{{R_p}^2}{1-{R_p}^2}\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.11\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.35\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.46\columnwidth}\raggedright
\({R_p}\) is the squared partial correlation coefficient\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.11\columnwidth}\raggedright
Chi-square test\strut
\end{minipage} & \begin{minipage}[t]{0.35\columnwidth}\centering
w\strut
\end{minipage} & \begin{minipage}[t]{0.46\columnwidth}\raggedright
\(w = \sqrt{ \sum_{i=1}^m \frac{(p_{0i} - p_{1i})^2 }{ p_{0i}} }\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.11\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.35\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.46\columnwidth}\raggedright
\(p_{0i}\) and \(p_{1i}\) are theporportion in category \(i\) predicted by the null, \(_0\), and alternative, \(_1\), hypothesis\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{power-analysis-for-a-t-test-on-two-independent-means}{%
\section{Power analysis for a t-test on two independent means}\label{power-analysis-for-a-t-test-on-two-independent-means}}

The objective of this lab is to learn to use G*Power and understand how the 4 parameters of power analyses (\(\alpha\), \(\beta\), sample size and effect size) are related to each other. For this, you will only use the standard t-test to compare two independent means. This is the test most used by biologists, you have all used it, and it will serve admirably for this lab. What you will learn today will be applicable to all other power analyses.

Jaynie Stephenson studied the productivity of streams in the Ottawa region. She has measured fish biomass in 36 streams, 18 on the Shield and 18 in the Ottawa Valley. She found that fish biomass was lower in streams from the valley (2.64 \(g/m^2\) , standard deviation = 3.28) than from the Shield (3.31 \(g/m^2\) , standard deviation = 2.79.).

When she tested the null hypothesis that fish biomass is the same in the two regions by a t-test, she obtained:

\begin{verbatim}
Pooled-Variance Two-Sample t-Test
t = -0.5746, df = 34, p-value = 0.5693
\end{verbatim}

She therefore accepted the null hypothesis (since p is much larger than 0.05) and concluded that fish biomass is the same in the two regions.

\hypertarget{post-hoc-analysis}{%
\subsection{Post-hoc analysis}\label{post-hoc-analysis}}

Using the observed means and standard deviations, we can use G*Power to calculate the power of the two-tailed t-test for two independent means, using the observed effect size (the difference between the two means, weighted by the standard deviations) for \(\alpha\) = 0.05.

Start G*Power.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  In *\textbf{Test family} , choose: t tests
\item
  For \textbf{Statistical test} , choose: Means: Difference between two independent means (two groups)
\item
  For \textbf{Type of power analysis} , choose: Post hoc: Compute achieved power - given \(\alpha\), sample size, and effect size
\item
  At \textbf{Input Parameters} ,
\end{enumerate}

\begin{itemize}
\tightlist
\item
  in the box \textbf{Tail(s)} , chose: Two,
\item
  check that \(\alpha\) \textbf{err prob} is equal to 0.05
\item
  Enter 18 for the \textbf{Sample size} of group 1 and of group 2
\item
  then, to calculate effect size (d), click on \textbf{Determine =\textgreater{}}
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  In the window that opens,
\end{enumerate}

\begin{itemize}
\tightlist
\item
  select \textbf{n1 = n2} , then
\item
  enter the two means (\textbf{Mean group} 1 et 2)
\item
  the two standard deviations(\textbf{SD group} 1 et 2)
\item
  click on \textbf{Calculate} and \textbf{transfer to main window}
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  After you click on the \textbf{Calculate button} in the main window, you
  should get the following:
\end{enumerate}

\begin{figure}
\includegraphics[width=1\linewidth]{images/gpower_1} \caption{Post-hoc analysis with estimated effect size}\label{fig:gpower-1}
\end{figure}

Similar analysis can be done in R. You first need to calculate the effect size \texttt{d} for a t-test comparing 2 means, and then use the \texttt{pwr.t.test()} function from the \texttt{pwr} üì¶.
The easiest is to create anew function in R to estimate the effect size \texttt{d}since we are going to reuse it multiple times during the lab.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# load package pwr}
\KeywordTok{library}\NormalTok{(pwr)}
\CommentTok{\# define d for a 2 sample t{-}test}
\NormalTok{d \textless{}{-}}\StringTok{ }\ControlFlowTok{function}\NormalTok{(u1, u2, sd1, sd2) \{}
  \KeywordTok{abs}\NormalTok{(u1 }\OperatorTok{{-}}\StringTok{ }\NormalTok{u2) }\OperatorTok{/}\StringTok{ }\KeywordTok{sqrt}\NormalTok{((sd1}\OperatorTok{\^{}}\DecValTok{2} \OperatorTok{+}\StringTok{ }\NormalTok{sd2}\OperatorTok{\^{}}\DecValTok{2}\NormalTok{) }\OperatorTok{/}\StringTok{ }\DecValTok{2}\NormalTok{)}
\NormalTok{\}}

\CommentTok{\# power analysis}
\KeywordTok{pwr.t.test}\NormalTok{(}\DataTypeTok{n =} \DecValTok{18}\NormalTok{, }\DataTypeTok{d =} \KeywordTok{d}\NormalTok{(}\DataTypeTok{u1 =} \FloatTok{2.64}\NormalTok{, }\DataTypeTok{sd1 =} \FloatTok{3.28}\NormalTok{, }\DataTypeTok{u2 =} \FloatTok{3.31}\NormalTok{, }\DataTypeTok{sd2 =} \FloatTok{2.79}\NormalTok{), }\DataTypeTok{sig.level =} \FloatTok{0.05}\NormalTok{, }\DataTypeTok{type =} \StringTok{"two.sample"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##      Two-sample t test power calculation 
## 
##               n = 18
##               d = 0.220042
##       sig.level = 0.05
##           power = 0.09833902
##     alternative = two.sided
## 
## NOTE: n is number in *each* group
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# plot similar to G*Power}
\NormalTok{x \textless{}{-}}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\OperatorTok{{-}}\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DataTypeTok{length =} \DecValTok{200}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(x, }\KeywordTok{dnorm}\NormalTok{(x), }\DataTypeTok{type =} \StringTok{"l"}\NormalTok{, }\DataTypeTok{col =} \StringTok{"red"}\NormalTok{, }\DataTypeTok{lwd =} \DecValTok{2}\NormalTok{)}
\NormalTok{qc \textless{}{-}}\StringTok{ }\KeywordTok{qt}\NormalTok{(}\FloatTok{0.025}\NormalTok{, }\DecValTok{16}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v =}\NormalTok{ qc, }\DataTypeTok{col =} \StringTok{"green"}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v =} \OperatorTok{{-}}\NormalTok{qc, }\DataTypeTok{col =} \StringTok{"green"}\NormalTok{)}
\KeywordTok{lines}\NormalTok{(x, }\KeywordTok{dnorm}\NormalTok{(x, }\DataTypeTok{mean =}\NormalTok{ (}\FloatTok{3.31} \OperatorTok{{-}}\StringTok{ }\FloatTok{2.64}\NormalTok{)), }\DataTypeTok{type =} \StringTok{"l"}\NormalTok{, }\DataTypeTok{col =} \StringTok{"blue"}\NormalTok{, }\DataTypeTok{lwd =} \DecValTok{2}\NormalTok{)}

\CommentTok{\# power corresponds to the shaded area}
\NormalTok{y \textless{}{-}}\StringTok{ }\KeywordTok{dnorm}\NormalTok{(x, }\DataTypeTok{mean =}\NormalTok{ (}\FloatTok{3.31} \OperatorTok{{-}}\StringTok{ }\FloatTok{2.64}\NormalTok{))}
\KeywordTok{polygon}\NormalTok{(}\KeywordTok{c}\NormalTok{(x[x }\OperatorTok{\textless{}=}\StringTok{ }\OperatorTok{{-}}\NormalTok{qc], }\OperatorTok{{-}}\NormalTok{qc), }\KeywordTok{c}\NormalTok{(y[x }\OperatorTok{\textless{}=}\StringTok{ }\OperatorTok{{-}}\NormalTok{qc], }\DecValTok{0}\NormalTok{), }\DataTypeTok{col =} \KeywordTok{rgb}\NormalTok{(}\DataTypeTok{red =} \DecValTok{0}\NormalTok{, }\DataTypeTok{green =} \FloatTok{0.2}\NormalTok{, }\DataTypeTok{blue =} \DecValTok{1}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.5}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{Labs_BIO4158_files/figure-latex/r-power-1.pdf}
\caption{\label{fig:r-power}Post-hoc analysis with estimated effect size in R}
\end{figure}

Let's examine the figure \ref{fig:gpower-1}.

\begin{itemize}
\tightlist
\item
  The curve on the left, in red, corresponds to the expected distri- bution of the t-statistics when \(H_0\) is true (\emph{i.e.} when the two means are equal) given the sample size (18 per region) and the observed standard deviations.
\item
  The vertical green lines correspond to the critical values of t for \(\alpha = 0.05\) and a total sample size of 36 (2x18).
\item
  The shaded pink regions correspond to the rejection zones for \(H_0\). If Jaynie had obtained a \emph{t-value} outside the interval delimited by the critical values ranging from -2.03224 to 2.03224, she would then have rejected \(H_0\) , the null hypothesis of equal means. In fact, she obtained a t-value of -0.5746 and concluded that the biomass is equal in the two regions.
\item
  The curve on the right, in blue, corresponds to the expected dis- tribution of the t-statistics if \(H_1\) is true (here \(H_1\) is that there is a difference in biomass between the two regions equal to \(3.33 - 2.64 = 0.69 g/m^2\) , given the observed standard deviations). This distribution is what we should observe if \(H_1\) was true and we repeated a large number of times the experiment using random samples of 18 streams in each of the two regions and calculated a t-statistic for each sample. On average, we would obtain a t-statistic of about 0.6.
\item
  Note that there is considerable overlap of the two distributions and that a large fraction of the surface under the right curve is within the interval where \(H_0\) is accepted between the two vertical green lines at -2.03224 and 2.03224. This proportion, shaded in blue under the distribution on the right is labeled \(\beta\) and corresponds to the risk of \emph{type II error} (accept \(H_0\) when \(H_1\) is true).
\item
  Power is simply \(1-\beta\), and is here 0.098339. Therefore, if the mean biomass differed by \(0.69 g/m^2\) between the two regions, Jaynie had only 9.8\% chance of being able to detect it as a statistically significant difference at ÔÅ°=5\% with a sample size of 18 streams in each region.
\end{itemize}

\textbf{Let's recapitulate}: The difference in biomass between regions is not statistically significant according to the t-test. It is because the difference is relatively small relative to the precision of the measurements. It is therefore not surprising that that power, i.e.~the probability of detecting a statistically significant difference, is small. Therefore, this analysis is not very informative.

\textbf{Indeed, a post hoc power analysis using the observed effect size is not useful}. It is much more informative to conduct a post hoc power analysis for an effect size that is different from the observed effect size. But what effect size to use? It is the biology of the system under study that will guide you. For example, with respect to fish biomass in streams, one could argue that a two fold change in biomass (say from 2.64 to 5.28 g/m\textsuperscript{2} ) has ecologically significant repercussions. We would therefore want to know if Jaynie had a good chance of detecting a difference as large as this before accepting her conclusion that the biomass is the same in the two regions. So, what were the odds that Jaynie could detect a difference of 2.64 g/m\textsuperscript{2} between the two regions? G*Power can tell you if you cajole it the right way.

\begin{rmdcode}
Change the mean of group 2 to 5.28, recalculate effect size, and click on Calculate to obtain figure \ref{fig:gpower-2}.
\end{rmdcode}

\begin{figure}
\includegraphics[width=1\linewidth]{images/gpower_2} \caption{Post-hoc analysis using an effect size different from the one estimated}\label{fig:gpower-2}
\end{figure}

Same analysis using R (without all the code for the interesting but not really useful plot)

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pwr.t.test}\NormalTok{(}\DataTypeTok{n =} \DecValTok{18}\NormalTok{, }\DataTypeTok{d =} \KeywordTok{d}\NormalTok{(}\DataTypeTok{u1 =} \FloatTok{2.64}\NormalTok{, }\DataTypeTok{sd1 =} \FloatTok{3.28}\NormalTok{, }\DataTypeTok{u2 =} \FloatTok{5.28}\NormalTok{, }\DataTypeTok{sd2 =} \FloatTok{2.79}\NormalTok{), }\DataTypeTok{sig.level =} \FloatTok{0.05}\NormalTok{, }\DataTypeTok{type =} \StringTok{"two.sample"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##      Two-sample t test power calculation 
## 
##               n = 18
##               d = 0.8670313
##       sig.level = 0.05
##           power = 0.7146763
##     alternative = two.sided
## 
## NOTE: n is number in *each* group
\end{verbatim}

The power is 0.71, therefore Jaynie had a reasonable chance (71\%) of detecting a doubling of biomass with 18 streams in each region.

Note that this post hoc power analysis, done for an effect size considered biologically meaningful, is much more informative than the preceeding one done with the observed effect size (which is what too many students do because it is the default of so many power calculation programs). Jaynie did not detect a difference between the two regions. There are two possibilities: 1) there is really no difference between the regions, or 2) the precision of measurements is so low (because the sample size is small and/or there is large variability within a region) that it is very unlikely to be able to detect even large differences. The second power analysis can eliminate this second possibility because Jaynie had 71\% chances of detecting a doubling of biomass.

\hypertarget{a-priori-analysis}{%
\subsection{A priori analysis}\label{a-priori-analysis}}

Suppose that a difference in biomass of \(3.31-2.64 = 0.67 g/m^2\) can be ecologically significant. The next field season should be planned so that Jaynie would have a good chance of detecting such a difference in fish biomass between regions. How many streams should Jaynie sample in each region to have 80\% of detecting such a difference (given the observed variability)?

\begin{rmdcode}
Change the type of power analysis in G*Power to \textbf{A priori: Compute sample size - given \(\alpha\) , power, and effect size}. Ensure that the values for means and standard deviations are those obtained by Jaynie. Recalculate the effect size metric and enter 0.8 for power and you will obtain (\ref{fig:gpower-3}.
\end{rmdcode}

\begin{figure}
\includegraphics[width=1\linewidth]{images/gpower_3} \caption{A priori analysis}\label{fig:gpower-3}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pwr.t.test}\NormalTok{(}\DataTypeTok{power =} \FloatTok{0.8}\NormalTok{, }\DataTypeTok{d =} \KeywordTok{d}\NormalTok{(}\DataTypeTok{u1 =} \FloatTok{2.64}\NormalTok{, }\DataTypeTok{sd1 =} \FloatTok{3.28}\NormalTok{, }\DataTypeTok{u2 =} \FloatTok{3.31}\NormalTok{, }\DataTypeTok{sd2 =} \FloatTok{2.79}\NormalTok{), }\DataTypeTok{sig.level =} \FloatTok{0.05}\NormalTok{, }\DataTypeTok{type =} \StringTok{"two.sample"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##      Two-sample t test power calculation 
## 
##               n = 325.1723
##               d = 0.220042
##       sig.level = 0.05
##           power = 0.8
##     alternative = two.sided
## 
## NOTE: n is number in *each* group
\end{verbatim}

\textbf{Ouch!} The required sample would be of 326 streams in each region! It would cost a fortune and require several field teams otherwise only a few dozen streams could be sampled over the summer and it would be very unlikely that such a small difference in biomass could be detected. Sampling fewer streams would probably be in vain and could be considered as a waste of effort and time: why do the work on several dozens of streams if the odds of success are that low?

If we recalculate for a power of 95\%, we find that 538 streams would be required from each region. Increasing power means more work!

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pwr.t.test}\NormalTok{(}\DataTypeTok{power =} \FloatTok{0.95}\NormalTok{, }\DataTypeTok{d =} \KeywordTok{d}\NormalTok{(}\DataTypeTok{u1 =} \FloatTok{2.64}\NormalTok{, }\DataTypeTok{sd1 =} \FloatTok{3.28}\NormalTok{, }\DataTypeTok{u2 =} \FloatTok{3.31}\NormalTok{, }\DataTypeTok{sd2 =} \FloatTok{2.79}\NormalTok{), }\DataTypeTok{sig.level =} \FloatTok{0.05}\NormalTok{, }\DataTypeTok{type =} \StringTok{"two.sample"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##      Two-sample t test power calculation 
## 
##               n = 537.7286
##               d = 0.220042
##       sig.level = 0.05
##           power = 0.95
##     alternative = two.sided
## 
## NOTE: n is number in *each* group
\end{verbatim}

\hypertarget{sensitivity-analysis---calculate-the-detectable-effect-size}{%
\subsection{Sensitivity analysis - Calculate the detectable effect size}\label{sensitivity-analysis---calculate-the-detectable-effect-size}}

Given the observed variability, a sampling effort of 18 streams per region, and with \(\alpha\) = 0.05, what effect size could Jaynie detect with 80\% probability \(\beta=0.2\))?

\begin{rmdcode}
Change analysis type in G*Power to \textbf{Sensitivity: Compute required effect size - given \(\alpha\) , power, and sample size} and size is 18 in each region.
\end{rmdcode}

\begin{figure}
\includegraphics[width=1\linewidth]{images/gpower_4} \caption{Analyse de sensitivit√©}\label{fig:gpower-4}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pwr.t.test}\NormalTok{(}\DataTypeTok{power =} \FloatTok{0.8}\NormalTok{, }\DataTypeTok{n =} \DecValTok{18}\NormalTok{, }\DataTypeTok{sig.level =} \FloatTok{0.05}\NormalTok{, }\DataTypeTok{type =} \StringTok{"two.sample"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##      Two-sample t test power calculation 
## 
##               n = 18
##               d = 0.9612854
##       sig.level = 0.05
##           power = 0.8
##     alternative = two.sided
## 
## NOTE: n is number in *each* group
\end{verbatim}

The detectable effect size for this sample size, \(\alpha = 0.05\) and \(\beta = 0.2\) (or power of 80\%) is 0.961296.

\begin{rmdcaution}
Attention, this effect size is the metric \texttt{d} and is dependent on sampling variability.
\end{rmdcaution}

Here, \texttt{d} is approximately equal to

\[ d = \frac{| \bar{X_1} \bar{X_2} |} {\sqrt{\frac{{s_1}^2 +{s_2}^2}{2}}}\]

To convert this d value without units into a value for the detectable difference in biomass between the two regions, you need to multiply \texttt{d} by the denominator of the equation.

\[
| \bar{X_1} - \bar{X_2} | = d * \sqrt{\frac{{s_1}^2 +{s_2}^2}{2}}
\]

In R this can be done with the following code

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pwr.t.test}\NormalTok{(}\DataTypeTok{power =} \FloatTok{0.8}\NormalTok{, }\DataTypeTok{n =} \DecValTok{18}\NormalTok{, }\DataTypeTok{sig.level =} \FloatTok{0.05}\NormalTok{, }\DataTypeTok{type =} \StringTok{"two.sample"}\NormalTok{)}\OperatorTok{$}\NormalTok{d }\OperatorTok{*}\StringTok{ }\KeywordTok{sqrt}\NormalTok{((}\FloatTok{3.28}\OperatorTok{\^{}}\DecValTok{2} \OperatorTok{+}\StringTok{ }\FloatTok{2.79}\OperatorTok{\^{}}\DecValTok{2}\NormalTok{) }\OperatorTok{/}\StringTok{ }\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2.926992
\end{verbatim}

Therefore, with 18 streams per region, \(\alpha\) = 0.05 and \(\beta\) = 0.2 (so power of 80\%), Jaynie could detect a difference of 2.93 g/m\textsuperscript{2} between regions, a bit more than a doubling of biomass.

\hypertarget{important-points-to-remember}{%
\section{Important points to remember}\label{important-points-to-remember}}

\begin{itemize}
\tightlist
\item
  Post hoc power analyses are relevant only when the null hypothesis is accepted because it is impossible to make a \emph{type II error} when rejecting \(H_0\) .
\item
  With very large samples, power is very high and minute differences can be statistically detected, even if they are not biologically significant.
\item
  When using a stricter significance criteria (\(\alpha < 0.05\)) power is reduced.
\item
  Maximizing power implies more sampling effort, unless you use a more liberal statistical criteria (\(\alpha > 0.05\))
\item
  The choice of \(\beta\) is somewhat arbitrary. \(\beta=0.2\) (power of 80\%) is considered relatively high by most.
\end{itemize}

\cleardoublepage

\hypertarget{appendix-appendix}{%
\appendix \addcontentsline{toc}{chapter}{\appendixname}}


\hypertarget{software-tools}{%
\chapter{Software Tools}\label{software-tools}}

For those who are not familiar with software packages required for using R Markdown, we give a brief introduction to the installation and maintenance of these packages.

\hypertarget{r-and-r-packages}{%
\section{R and R packages}\label{r-and-r-packages}}

R can be downloaded and installed from any CRAN (the Comprehensive R Archive Network) mirrors, e.g., \url{https://cran.rstudio.com}. Please note that there will be a few new releases of R every year, and you may want to upgrade R occasionally.

To install the \textbf{bookdown} package, you can type this in R:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"bookdown"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This installs all required R packages. You can also choose to install all optional packages as well, if you do not care too much about whether these packages will actually be used to compile your book (such as \textbf{htmlwidgets}):

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"bookdown"}\NormalTok{, }\DataTypeTok{dependencies =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

If you want to test the development version of \textbf{bookdown} on GitHub, you need to install \textbf{devtools} first:

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{if}\NormalTok{ (}\OperatorTok{!}\KeywordTok{requireNamespace}\NormalTok{(}\StringTok{"devtools"}\NormalTok{)) }\KeywordTok{install.packages}\NormalTok{(}\StringTok{"devtools"}\NormalTok{)}
\NormalTok{devtools}\OperatorTok{::}\KeywordTok{install\_github}\NormalTok{(}\StringTok{"rstudio/bookdown"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

R packages are also often constantly updated on CRAN or GitHub, so you may want to update them once in a while:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{update.packages}\NormalTok{(}\DataTypeTok{ask =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Although it is not required, the RStudio IDE can make a lot of things much easier when you work on R-related projects. The RStudio IDE can be downloaded from \url{https://www.rstudio.com}.

\hypertarget{pandoc}{%
\section{Pandoc}\label{pandoc}}

An R Markdown document (\texttt{*.Rmd}) is first compiled to Markdown (\texttt{*.md}) through the \textbf{knitr} package, and then Markdown is compiled to other output formats (such as LaTeX or HTML) through Pandoc.\index{Pandoc} This process is automated by the \textbf{rmarkdown} package. You do not need to install \textbf{knitr} or \textbf{rmarkdown} separately, because they are the required packages of \textbf{bookdown} and will be automatically installed when you install \textbf{bookdown}. However, Pandoc is not an R package, so it will not be automatically installed when you install \textbf{bookdown}. You can follow the installation instructions on the Pandoc homepage (\url{http://pandoc.org}) to install Pandoc, but if you use the RStudio IDE, you do not really need to install Pandoc separately, because RStudio includes a copy of Pandoc. The Pandoc version number can be obtained via:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rmarkdown}\OperatorTok{::}\KeywordTok{pandoc\_version}\NormalTok{()}
\CommentTok{\#\# [1] \textquotesingle{}2.9.2.1\textquotesingle{}}
\end{Highlighting}
\end{Shaded}

If you find this version too low and there are Pandoc features only in a later version, you can install the later version of Pandoc, and \textbf{rmarkdown} will call the newer version instead of its built-in version.

\hypertarget{latex}{%
\section{LaTeX}\label{latex}}

LaTeX\index{LaTeX} is required only if you want to convert your book to PDF. You may see \url{https://www.latex-project.org/get/} for more information about LaTeX and its installation, but we strongly recommend that you install the lightweight and cross-platform LaTeX distribution named \href{https://yihui.org/tinytex/}{TinyTeX} and based on TeX Live. TinyTeX can be easily installed through the R package \textbf{tinytex} (which should be automatically installed when you install \textbf{bookdown}):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tinytex}\OperatorTok{::}\KeywordTok{install\_tinytex}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

With TinyTeX, you should never see error messages like this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{! LaTeX Error: File \textasciigrave{}titling.sty\textquotesingle{} not found.}

\NormalTok{Type X to quit or \textless{}RETURN\textgreater{} to proceed,}
\NormalTok{or enter new name. (Default extension: sty)}

\NormalTok{Enter file name:}
\NormalTok{! Emergency stop.}
\NormalTok{\textless{}read *\textgreater{}}

\NormalTok{l.107 \^{}\^{}M}

\NormalTok{pandoc: Error producing PDF}
\NormalTok{Error: pandoc document conversion failed with error 43}
\NormalTok{Execution halted}
\end{Highlighting}
\end{Shaded}

The above error means you used a package that contains \texttt{titling.sty}, but it was not installed. LaTeX package names are often the same as the \texttt{*.sty} filenames, so in this case, you can try to install the \texttt{titling} package. If you use TinyTeX with R Markdown, missing LaTeX packages will be installed automatically, so you never need to worry about such problems.

LaTeX distributions and packages are also updated from time to time, and you may consider updating them especially when you run into LaTeX problems. You can find out the version of your LaTeX distribution by:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{system}\NormalTok{(}\StringTok{"pdflatex {-}{-}version"}\NormalTok{)}
\CommentTok{\#\# pdfTeX 3.14159265{-}2.6{-}1.40.21 (TeX Live 2020/Debian)}
\CommentTok{\#\# kpathsea version 6.3.2}
\CommentTok{\#\# Copyright 2020 Han The Thanh (pdfTeX) et al.}
\CommentTok{\#\# There is NO warranty.  Redistribution of this software is}
\CommentTok{\#\# covered by the terms of both the pdfTeX copyright and}
\CommentTok{\#\# the Lesser GNU General Public License.}
\CommentTok{\#\# For more information about these matters, see the file}
\CommentTok{\#\# named COPYING and the pdfTeX source.}
\CommentTok{\#\# Primary author of pdfTeX: Han The Thanh (pdfTeX) et al.}
\CommentTok{\#\# Compiled with libpng 1.6.37; using libpng 1.6.37}
\CommentTok{\#\# Compiled with zlib 1.2.11; using zlib 1.2.11}
\CommentTok{\#\# Compiled with xpdf version 4.02}
\end{Highlighting}
\end{Shaded}

To update TinyTeX, you may run:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tinytex}\OperatorTok{::}\KeywordTok{tlmgr\_update}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

From year to year, you may need to upgrade TinyTeX, too (otherwise you cannot install or update any LaTeX packages), in which case you may reinstall TinyTeX:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tinytex}\OperatorTok{::}\KeywordTok{reinstall\_tinytex}\NormalTok{()}
\end{Highlighting}
\end{Shaded}


\printindex

\end{document}
